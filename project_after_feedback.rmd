---
subtitle: "MA8701 Advanced Statistical Learning V2023"
title: "Compulsory exercise: Team SuperGreat"
author: "Nora Aasen, Elias Angelsen, Jonas Nordstrom"
date: "`r format(Sys.time(), '%d %B, %Y')`"
bibliography: [references.bib,packages.bib]
# nocite: '@*'
output: 
  # html_document
  pdf_document
---
  
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE, 
                      eval = TRUE, 
                      tidy=TRUE, 
                      message=FALSE, 
                      warning=FALSE, 
                      strip.white=TRUE, 
                      prompt=FALSE, 
                      cache=TRUE, 
                      size="scriptsize", 
                      fig.width=6, 
                      fig.height=4.5, 
                      cex = 0.7, 
                      fig.align = "center")
```


# Introduction

## Possibly elaborate some, both concerning presentation of dataset and summary of our project

In this project we have studied the [Framingham Coronary Heart Disease Dataset](https://www.kaggle.com/datasets/dileep070/heart-disease-prediction-using-logistic-regression?fbclid=IwAR1LE3P3vM1SyHBifotrNdXoKGv7szGR07labEAQo6XUqV9Pi90vtAp4mS4) which can be found on kaggle @noauthor_logistic_nodate. This dataset contains patient information for inhabitants in Framingham, Massachusetts, and is typically used to predict the chance of getting coronary heart disease (CHD) within the next 10 years. 

For this project, however, we intend to investigate how to handle missing data in combination with lasso as a method for making inference about the variables. We will do single regression imputation using the package called `mice` @R-mice, and investigate how imputation may affect the subsequent lasso regression. 

## Exploratory Analysis

We start by examining the data set. We use the packages `ggplot2` @R-ggplot2 and `tidyr` @R-tidyr to make the plots. 

```{r Loading Data, fig.width= 9, fig.height=6}
# Load and look at data
data <- read.csv("C:\\Users\\noraa\\Student\\5thYear\\MA8701\\data_analysis_proj\\Heart_disease_analysis\\framingham.csv")
data_dim = dim(data)
pos_response = sum(data$TenYearCHD==1)

library(ggplot2)
library(tidyr) # gather()

# We visualize the data
ggplot(gather(data), aes(value)) + 
    geom_histogram(bins = 16) + 
    facet_wrap(~key, scales = 'free_x')

# Code education as a factor variable instead of 1-2-3-4.
data$education = factor(data$education, labels = c("none","hs","college","post-grad"))
```

This data set contains `r data_dim[1]` observations, `r data_dim[2]-1` covariates and a binary response variable `TenYearCHD`, indicating that a natural model choice is a logistic regression model. The response variable has `r pos_response` positive observations, which equals about `r round(pos_response/data_dim[1]*100,1)`\% of the total observations. We see from the plot that most of our covariates are either binary, or numeric. However, we note that the variable education is most likely a categorical covariate. We could not find any further elaboration for which four categories the numbers represent, so based on the frequency of each value and qualified guessing, we changed it to a factor variable and defined the four categories as `r names(summary(data$education))[1:4]`.

The next thing we looked at was the number of missing data in our data set. We used the packages `ggmice` @R-ggmice, `naniar` @R-naniar, and `gridExtra` @R-gridExtra to produce the plots. 

```{r plot_missing_data, fig.cap = "My plot", fig.width=9, fig.height = 5}
# Look at the missing data
library(ggmice) # plot_pattern()
library(naniar) # gg_miss_var()
library(gridExtra) # grid.arrange()

gluc_miss = sum(is.na(data$glucose))/length(data$glucose)

plot1 <- plot_pattern(data, rotate = T)
plot2 <- gg_miss_var(data)
grid.arrange(plot1, plot2, ncol=2)
```


As we can see there are seven covariates that contains missing data: `glucose`, `education`, `BPMeds`, `totChol`, `cigsPerDay`, `heartRate`, and `BMI`, where `glucose` is by far the covariate that has the most `NA`'s, with a total of `r round(gluc_miss*100,1)`\% missing values. We cannot use the rows that contain missing values as is. An option for handling the problem would be to remove all rows that contains `NA`'s, also called \textit{complete case analysis}. In this project we do a method called \emph{single imputation}, and compare it with the complete case analysis to investigate how imputing missing data can affect the subsequent inference. 

See Figure \@ref(fig:plot_missing_data).

Before beginning the imputation we have to split the data into two disjoint sets. We call them training and test set here, but because our goal is inference they will serve more as independent sets where one is used for covariate reduction and the other is used for estimating and inference. The same split is used for both the imputation method and complete method. 

```{r train-test split}
# Split into training and test first to avoid data leakage
set.seed(8701)
tr = 7/10 # train ratio
r = dim(data)[1]
size = round(r*tr)
train = sample(1:r,size = size)

d.train = data[train,]
d.test = data[-train,]

# Make a dataset containing only the complete cases
d.complete <- data[complete.cases(data), ]
d.train.complete <- d.train[complete.cases(d.train), ]
d.test.complete <- d.test[complete.cases(d.test), ]

pos_response_c = sum(data[complete.cases(data),]$TenYearCHD==1)
```

The complete data set contains `r sum(complete.cases(data))` observations and the response variable has `r pos_response_c` positive responses, which equals about `r round(pos_response_c/sum(complete.cases(data))*100,1)`\% of the total observations. As we can see, the proportion of positive observations in the response is the same, which is a good indicator that our data is missing at random (MAR), which we will come back to.


# Missing Data: Elias kan få skrive denne delen!

## Mangler å renskrive teorien, kommentere på MAR; MCAR; or MNAR, kommentere fordelingen etter imputeringen. Evt. se mail med kommentarer for inspo til hva som bør med. Prøv å begrense mengden tekst så langt det lar seg gjøre.  


We start by recalling that there are several types of mechanisms for missing data.
Let $Z = (X,y)$ denote the full collection of covariates and responses, respectively, and we let a subscript mis/obs indicate whether we are restricting $Z$ (or $X$) to the missing or observed parts, respectively.
We may form an indicator (0-1) matrix $R$ indicating missing (0) and observed (1) covariates. 
Assume $\psi$ is short for the parameters in the distribution of $R$.

The missing data may be characterized by the conditioning in the distribution of $R$.
We define the data to be:
\begin{itemize}
  \item missing completely at random (MCAR) if $P(R | Z, \psi) = P(R | \psi)$,
  \item missing at random (MAR) if $P(R | Z, \psi) = P(R | Z_{obs}, \psi)$,
  \item missing not at random (MNAR) if $P(R | Z, \psi) = P(R | Z, \psi)$ (i.e. we don't have MCAR or MAR).
\end{itemize}


```{r investigate MAR/MCAR}
# We investigate the response variable for rows that has missing data 
x = which(colSums(is.na(data)) > 0)
M = matrix(nrow=2, ncol = length(x)+1)

# For row 1 we see how many rows that has NA for each covariate
M[1,] = c(colSums(is.na(data))[x], NA)

# For row 2 we compute the percentage of positive responses only for those rows that has NA in that covariate
for (i in 1:length(x)){
  r = is.na(data[,x[i]])
  df = data[r,]
  M[2,i] = round(sum(df$TenYearCHD)/length(df$TenYearCHD),3)
}

M[2,length(x)+1] = round(sum(data$TenYearCHD)/length(data$TenYearCHD),3)
colnames(M) = c(colnames(data)[colSums(is.na(data)) > 0],"full")
rownames(M) = c("# miss", "response freq")
as.table(M)
```

By exploring the missing pattern of for example the variable `cigsPerDay`, we see that our missing mechanism is not MCAR.
No non-smoker has failed to answer the question ''How may cigarettes do you smoke a day?'', which is a question only aimed at smokers. 
The simple explanation may be that the survey automatically fills in $0$ for `cigsPerDay` if you claim to be a non-smoker. 
In more mathematical terms, the missingness of `cigsPerDay` depends on the observed answer to ''Do you smoke?'' (found in variable `currentSmoker`), indicating that we do not work with MCAR data. Luckily, most methods are applicable if our missingness is at least MAR. 

We will assume that the missing mechanism is MAR for all our missing observations, as there is no clear reason to suspect it to be MNAR. 

To treat the missing data, we will use single imputation, as multiple imputation may cause difficulties with the resulting inference, as Rubin's rules needs to be combined with the Lasso, bootstrap and concluding inference. Multiple imputation was therefore not considered because it is beyond the scope of this project.

To perform single imputation we will use regression imputation, where we adapt our regression technique depending on the type of variable imputed.

Our data is split into training and test sets, with the test-to-training ratio being 3:7. In order to avoid data leakage in our imputation of the test set, we fit the imputation models on the training set.
The main idea is that the test set should be viewed as several independent observations. Using the test set to impute itself will use information not present at the time of training and will yield unintended correlation.

Mice uses polyreg for the factor variable and predictive mean matching (pmm) for all other.

We can use logreg for BPMeds - since it is binary -, polyreg for education and linear regression for all other columns with missing 

We use the package called mice @R-mice to imputed the data. Furthermore, since we do not want to use the test data to impute on itself we use the function `mice.reuse` from the package NADIA @R-NADIA. 

```{r Imputation with mice}
library(mice) # mice()
library(NADIA) # mice.reuse()

# Change the method for each covariate to our choice
meth = mice(data, maxit = 0)$method  
meth[which(meth == "pmm")] = "norm.predict"
meth["BPMeds"] <- "logreg"

# Make a single imputation modelusing the training data
imp_mod.train = mice(d.train, m=1, printFlag = F, method = meth)
d.train.imp = complete(imp_mod.train)

# Use the imputation model trained on the training set on the test set to avoid data leakage
# Hva skjer med heartRate når vi bruker mice.reuse? 
imp_mod.test = mice.reuse(imp_mod.train, d.test, printFlag = F)
d.test.imp = imp_mod.test$"1"

# Compare the density of imputed data vs. actual data in the training set
densityplot(imp_mod.train)

c1 <- rgb(173,216,230,max = 255, alpha = 95, names = "lt.blue")
c2 <- rgb(255,192,203, max = 255, alpha = 95, names = "lt.pink")
barplot(prop.table(table(d.train.complete$education)), col = c1, xlab = "Category", ylab = "Proportion of total samples", main = "Education: Observed and Imputed", ylim = c(0,0.6)) # Plot 1st histogram using a transparent color
barplot(prop.table(table(d.train.imp$education[is.na(d.train$education)])), col = c2, add = TRUE)
legend("topright", legend=c("Complete", "Imputed"), cex=1, pch=15,col=c(c1,c2))
```



# Model

In this section we fit the same model on both the imputed and complete training data. The two data sets are indentical apart from the rows that contained `NA`'s. Since we want to do lasso, we first standardize the data. Secondly, we use bootstrap to calculate the non-zero coefficients multiple times. Lastly, we fit a logistic regression model using the test set.


```{r complete train test, echo = T}
# Scale data for lasso

x.train.complete = scale(model.matrix(TenYearCHD ~ . -1, data = d.train.complete, family = binomial())) 
train.complete.mean = attr(x.train.complete, "scaled:center")
train.complete.sd = attr(x.train.complete, "scaled:scale")
y.train.complete = d.train.complete$TenYearCHD

x.train.imp = scale(model.matrix(TenYearCHD ~ . -1, data = d.train.imp, family = binomial()))
train.imp.mean = attr(x.train.imp, "scaled:center")
train.imp.sd = attr(x.train.imp, "scaled:scale")
y.train.imp = d.train.imp$TenYearCHD
```





## Lasso on complete data 

We continue to do the Lasso on the complete case data. This is done by first estimating $\lambda_{min}$ using cross-validation and then fitting a model with the highest $\lambda$ within one standard deviation of $\lambda_{min}$ as penalty term. For easy implementation of the lasso method, we use the package glmnet @R-glmnet. 

```{r Bootstrap Lasso, echo = T, fig.width=9, fig.height=4.5}
set.seed(8701)
library(glmnet) # implementing lasso

B = 500 # Number of boostrap datasets
B.size.complete = dim(d.train.complete)[1] # Number of samples
# Make a matrix that store the computed coefficients
B.coef.complete = matrix(NA, nrow = B, ncol = length(colnames(x.train.complete))+1) 

for (i in 1:B){
  B.data = sample(1:B.size.complete, size = B.size.complete, replace = TRUE)
  x = x.train.complete[B.data,]
  y = y.train.complete[B.data]
  cv.out = cv.glmnet(x, y, family = "binomial", alpha = 1)
  lasso_mod = glmnet(x, y, family = "binomial", alpha = 1, intercept = T, lasso = cv.out$lambda.1se)
  
  B.coef.complete[i,] <- coef(lasso_mod,s=cv.out$lambda.1se)[,1]
}

colnames(B.coef.complete) = names(coef(lasso_mod,s=cv.out$lambda.1se)[,1])
B.coef.count.complete = ifelse(B.coef.complete == 0,0,1)
coef70.complete <- names(apply(B.coef.count.complete, 2, sum)/B)[apply(B.coef.count.complete, 2, sum)/B > 0.7]

# We do not include intercept as it is always in the model and too large to fit in the scaling.
par(mfrow = c(1,2))
boxplot.matrix(B.coef.complete[,-1], ylim = c(-0.25,0.55), las = 2, main = "Boxplot of estimated coefficients")
barplot(apply(B.coef.count.complete, 2, sum)/B, las = 2, main = "Percentage of times coefficient was nonzero")
abline(h=0.7,col="red", lty=2, lwd=3)

# Fit a logistic model using the test data and chosen covariates
mod.complete <- glm(TenYearCHD ~ age + male + sysBP + glucose, data = d.test.complete, family = binomial())
```

After using lasso on `r B` data sets the variables that have nonzero coefficients in at least 70\% of the time, are `r coef70.complete`. They are also indicated on the right figure above. We use the chosen covariates to fit a logistic regression model. To avoid overfitting, we use the independent test data to estimate the coefficients. This can be thought of as single split???? 

## Something about Mette's comment on how we did inference. 

## Should we say something about the boxplot as well? 


## Lasso on imputed data

We now do the same thing, just using the imputed data instead of the complete case.

```{r Bootstrap Lasso imputed, echo = T, fig.width=9, fig.height=4.5}
B = 500 # Number of boostrap datasets
B.size.imp = dim(d.train.imp)[1] # Number of samples
# Make a matrix that store the computed coefficients
B.coef.imp = matrix(NA, nrow = B, ncol = length(colnames(x.train.imp))+1) 

for (i in 1:B){
  B.data = sample(1:B.size.imp, size = B.size.imp, replace = TRUE)
  x = x.train.imp[B.data,]
  y = y.train.imp[B.data]
  cv.out = cv.glmnet(x, y, family = "binomial", alpha = 1)
  lasso_mod = glmnet(x, y, family = "binomial", alpha = 1, intercept = T, lasso = cv.out$lambda.1se)
  
  B.coef.imp[i,] <- coef(lasso_mod,s=cv.out$lambda.1se)[,1]
}

colnames(B.coef.imp) = names(coef(lasso_mod,s=cv.out$lambda.1se)[,1])
B.coef.count.imp = ifelse(B.coef.imp == 0,0,1)
coef70.imp <- names(apply(B.coef.count.imp, 2, sum)/B)[apply(B.coef.count.imp, 2, sum)/B > 0.7]

# We do not include intercept as it is always in the model and too large to fit in the scaling.
par(mfrow = c(1,2))
boxplot.matrix(B.coef.imp[,-1], ylim = c(-0.25,0.55), las = 2, main = "Boxplot of estimated coefficients")
barplot(apply(B.coef.count.imp, 2, sum)/B, las = 2, main = "Percentage of times coefficient was nonzero")
abline(h=0.7,col="red", lty=2, lwd=3)

# Fit a logistic model using the test data and chosen covariates
mod.imp <- glm(TenYearCHD ~ age + male + sysBP + glucose + cigsPerDay, data = d.test.imp, family = binomial())
```

The Lasso on the imputed data chooses `r coef70.imp`as the non-zero covariates, here also indicated in the right plot above. The performance of this model is quite similar to that of the complete case, but we note in particular that `cigsPerDay`, `BPMeds` and `glucose` are non-zero more often when using the imputed data. This could perhaps be a sign of correlation in the training data caused by the imputation. As before, we use the chosen covariates to fit a logistic regression model. To avoid overfitting, we use the "independent" imputed test data to estimate the coefficients. However, we question whether this might be a poor choice, as the training data were used to build the imputation model that were used to predict the missing values in the test set. 


# Inference

To access the two models we compute the AIC and look at the confidence intervals from the model. 


```{r complete case comparison}
data.frame("Complete"=round(summary(mod.complete)$aic,2),
      "Imputed" = round(summary(mod.imp)$aic,2))

summary(mod.complete)
summary(mod.imp)

confint(mod.complete)
confint(mod.imp)
```
We see that the AIC evaluates the model fitted on imputed data to be worse. This could be penalty for adding an  extra coefficient in our model, so for curiosity we check also a model fitted using the imputed test set, but with the same covariates as chosen by the complete data.  


```{r alternative model, eval = T, echo = T}
# Check the AIC when using the same covariates? 
mod.imp2 = glm(TenYearCHD ~ age + male + sysBP + glucose, data = d.test.imp, family = binomial())
summary(mod.imp2)
confint(mod.imp2)
data.frame("Complete"=round(summary(mod.complete)$aic,2),
      "Imputed" = round(summary(mod.imp)$aic,2),
      "Alternative imputed" = round(summary(mod.imp2)$aic,2))

CI.mat = matrix(NA, nrow = 3, ncol = 6) 
CI.mat[1,] = c(round(confint(mod.complete)[,1]-confint(mod.complete)[,2],2),NA)
CI.mat[2,] = c(round(confint(mod.imp)[,1]-confint(mod.imp)[,2],2))
CI.mat[3,] = c(round(confint(mod.imp2)[,1]-confint(mod.imp2)[,2],2),NA)
colnames(CI.mat) = names(coef(mod.imp))
rownames(CI.mat) = c("Complete", "Imputed", "Alternative Imputed")
abs(CI.mat)

```


## Finish this train of thought
We did not use ROC-AUC to evaluate the model as both train and test data has been used to fit the final model. Hence, a third, independent data would be needed in order to run ROC-AUC calculations...

# Discussion


When we compare the chosen coefficients from the complete case data compared to the imputed data, we see that they correspond. Thus, in a data-rich situation imputation may only introduce unnecessary variance, without having an immediate effect on the quality of the model or inference.

Another interesting thing we discovered, was that even though we obtain good results with the imputed data, the data quality seems to go down, if only barely. 
The sensitivity and specificity of the model went down slightly when using the imputed data, but as the Lasso selects the same variables, it does not matter a lot for our purposes.
The decrease in data quality might have been more visible if the percentage of imputed values were higher, or if we were in a data-poor situation.

Use the boxplot (sd not in 0 something ) instead of cutoff percentage for times they were non-zero...? 

# References 