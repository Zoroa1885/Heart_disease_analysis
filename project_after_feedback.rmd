---
subtitle: "MA8701 Advanced Statistical Learning V2023"
title: "Compulsory exercise: Team SuperGreat"
author: "Nora Aasen, Elias Angelsen, Jonas Nordstrom"
date: "`r format(Sys.time(), '%d %B, %Y')`"
bibliography: [references.bib,packages.bib]
# nocite: '@*'
output: 
  # html_document
  pdf_document
---
  
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE, 
                      eval = TRUE, 
                      tidy=TRUE, 
                      message=FALSE, 
                      warning=FALSE, 
                      strip.white=TRUE, 
                      prompt=FALSE, 
                      cache=TRUE, 
                      size="scriptsize", 
                      fig.width=6, 
                      fig.height=4.5, 
                      cex = 0.7, 
                      fig.align = "center")
```


# Introduction

## Possibly elaborate some, both concerning presentation of dataset and summary of our project

In this project we have studied the [Framingham Coronary Heart Disease Dataset](https://www.kaggle.com/datasets/dileep070/heart-disease-prediction-using-logistic-regression?fbclid=IwAR1LE3P3vM1SyHBifotrNdXoKGv7szGR07labEAQo6XUqV9Pi90vtAp4mS4) which can be found on kaggle @noauthor_logistic_nodate. This dataset contains patient information for inhabitants in Framingham, Massachusetts, and is typically used to predict the chance of getting coronary heart disease (CHD) within the next 10 years. 

For this project, however, we intend to investigate how to handle missing data in combination with lasso as a method for making inference about the variables. We will do single regression imputation using the mice package @R-mice, and investigate how imputation may affect the subsequent lasso regression. 

## Exploratory Analysis

We start by examining the data set. We use the packages ggplot2 @R-ggplot2 and tidyr @R-tidyr to make the plots. 

```{r Loading Data, fig.width= 9, fig.height=6}
# Load and look at data
# data <- read.csv("C:\\Heart_disease_analysis\\framingham.csv")
data <- read.csv("C:\\Users\\noraa\\Student\\5thYear\\MA8701\\data_analysis_proj\\Heart_disease_analysis\\framingham.csv")
data_dim = dim(data)
pos_response = sum(data$TenYearCHD==1)

library(ggplot2)
library(tidyr) # gather()

# We visualize the data
ggplot(gather(data), aes(value)) + 
    geom_histogram(bins = 16) + 
    facet_wrap(~key, scales = 'free_x')

# Code education as a factor variable instead of 1-2-3-4.
data$education = factor(data$education, labels = c("none","hs","college","post-grad"))
```

This data set contains `r data_dim[1]` observations, `r data_dim[2]-1` covariates and a binary response variable `TenYearCHD`. We will try to fit a logistic regression model. The response variable has `r pos_response` observations that are 1, which equals about `r round(pos_response/data_dim[1]*100,1)`\% of the total observations. Most of our covariates are either binary, or numeric. However, we note that the variable education is most likely a categorical covariate. We could not find any further elaboration for which four categories the numbers represent, so based on the frequency of each value and qualified guessing, we changed it to a factor variable and defined the four categories as `r names(summary(data$education))[1:4]`.

The next thing we looked at was the number of missing data in our data set. We used the packages ggmice @R-ggmice, naniar @R-naniar, and @R-gridExtra to produce the plots. 

```{r plot missing data, fig.width=9, fig.height = 5}
# Look at the missing data
library(ggmice) # plot_pattern()
library(naniar) # gg_miss_var()
library(gridExtra) # grid.arrange()

plot1 <- plot_pattern(data, rotate = T)
plot2 <- gg_miss_var(data)
grid.arrange(plot1, plot2, ncol=2)
```


As we can see there are seven covariates that has missing data: `glucose`, `education`, `BPMeds`, `totChol`, `cigsPerDay`, `heartRate`, and `BMI`. We cannot use the rows that contain missing values as is. The easiest solution is to remove all rows that contains `NA`'s. This is the \textit{complete case} solution.
We split the data into training and test sets, and make a complete training and test set with the same split that will be used for reference. 

## Consdier changing the name of the test set to something else? Since we really dont use it for testing....

```{r train-test split}
# Split into training and test first to avoid data leakage
set.seed(8701)
tr = 7/10 # train ratio
r = dim(data)[1]
size = round(r*tr)
train = sample(1:r,size = size)

d.train = data[train,]
d.test = data[-train,]

# Make a dataset containing only the complete cases
d.complete <- data[complete.cases(data), ]
d.train.complete <- d.train[complete.cases(d.train), ]
d.test.complete <- d.test[complete.cases(d.test), ]

pos_response_c = sum(data[complete.cases(data),]$TenYearCHD==1)
```

The complete data set contains `r sum(complete.cases(data))` observations and the response variable has `r pos_response_c` observations that are 1, which equals about `r round(pos_response_c/sum(complete.cases(data))*100,1)`\% of the total observations. As we can see, the proportion of positive observations in the response is the same, which is a good indicator that our data is missing at random (MAR), as we will discuss later.


# Missing Data


We start by recalling that there are several types of mechanisms for missing data.
Let $Z = (X,y)$ denote the full collection of covariates and responses, respectively, and we let a subscript mis/obs indicate whether we are restricting $Z$ (or $X$) to the missing or observed parts, respectively.
We may form an indicator (0-1) matrix $R$ indicating missing (0) and observed (1) covariates. 
Assume $\psi$ is short for the parameters in the distribution of $R$.

The missing data may be characterized by the conditioning in the distribution of $R$.
We define the data to be:
\begin{itemize}
  \item missing completely at random (MCAR) if $P(R | Z, \psi) = P(R | \psi)$,
  \item missing at random (MAR) if $P(R | Z, \psi) = P(R | Z_{obs}, \psi)$,
  \item missing not at random (MNAR) if $P(R | Z, \psi) = P(R | Z, \psi)$ (i.e. we don't have MCAR or MAR).
\end{itemize}


```{r investigate MAR/MCAR}
x = which(colSums(is.na(data)) > 0)

M = matrix(nrow=2, ncol = length(x)+1)
M[1,] = c(colSums(is.na(data))[x], NA)

for (i in 1:length(x)){
  r = is.na(data[,x[i]])
  df = data[r,]
  M[2,i] = round(sum(df$TenYearCHD)/length(df$TenYearCHD),3)
}

M[2,length(x)+1] = round(sum(data$TenYearCHD)/length(data$TenYearCHD),3)


colnames(M) = c(colnames(data)[colSums(is.na(data)) > 0],"full")
rownames(M) = c("# miss", "response freq")
as.table(M)
```

By exploring the missing pattern of for example the variable `cigsPerDay`, we see that our missing mechanism is not MCAR.
No non-smoker has failed to answer the question ''How may cigarettes do you smoke a day?'', which is a question only aimed at smokers. 
The simple explanation may be that the survey automatically fills in $0$ for `cigsPerDay` if you claim to be a non-smoker. 
In more mathematical terms, the missingness of `cigsPerDay` depends on the observed answer to ''Do you smoke?'' (found in variable `currentSmoker`), indicating that we do not work with MCAR data. Luckily, most methods are applicable if our missingness is at least MAR. 

We will assume that the missing mechanism is MAR for all our missing observations, as there is no clear reason to suspect it to be MNAR. 

To treat the missing data, we will use single imputation, as multiple imputation may cause difficulties with the resulting inference, as Rubin's rules needs to be combined with the Lasso, bootstrap and concluding inference. Multiple imputation was therefore not considered because it is beyond the scope of this project.

To perform single imputation we will use regression imputation, where we adapt our regression technique depending on the type of variable imputed. For continuous variables, we use a linear regression model, for binary variables, we use logistic regression, and for the variable ''education'', which is a four-class variable, we have used kNN for multiclass imputation. 
This is implemented manually, but this could have been done using the `MICE` package and the function `mice`.

To avoid encountering observations with more than one missing value, and hence problems with regressing, we remove all rows containing more than one `NA`. 

Our data is split into training and test sets, with the test-to-training ratio being 3:7. In order to avoid data leakage in our imputation of the test set, we fit the imputation models on the training set.
The main idea is that the test set should be viewed as several independent observations. Using the test set to impute itself will use information not present at the time of training and will yield unintended correlation.

Mice uses polyreg for the factor variable and predictive mean matching (pmm) for all other.

We can use logreg for BPMeds - since it is binary -, polyreg for education and linear regression for all other columns with missing 

We use the package called mice @R-mice to imputed the data. Furthermore, since we do not want to use the test data to impute on itself we use the function `mice.reuse` from the package NADIA @R-NADIA. 

```{r Imputation with mice}
library(mice) # mice()
library(NADIA) # mice.reuse()

meth = mice(data, maxit = 0)$method  
meth[which(meth == "pmm")] = "norm.predict"
meth["BPMeds"] <- "logreg"

imp_mod.train = mice(d.train, m=1, printFlag = F, method = meth)
d.train.imp = complete(imp_mod.train)

# Use the imputation model trained on the training set on the test set, to ensure no data leakage.
imp_mod.test = mice.reuse(imp_mod.train, d.test, printFlag = F)
d.test.imp = imp_mod.test$"1"

densityplot(imp_mod.train)
```



# Model

In the model section we will consider the two data sets; the complete case and imputed case. Both data sets were dvided into a train and test set. 
Since we want to do Lasso, we must standardize the data.
The problem with this is data leakage. If we want to standardize the test data, we should standardize it using the mean and the standard deviation of the training data. Most importantly, using the test data to scale the test data will introduce correlation between the independent observations of the test set.
Since the scaling information from the test set is "not available" to us at the time of training, we cannot expect the coefficients in the Lasso to be appropriately scaled compared to the test data.
We solve this by scaling the training data, and then using the attributes of the training data to scale the test data accordingly.


```{r complete train test, echo = T}
# Scale data for lasso

x.train.complete = scale(model.matrix(TenYearCHD ~ . -1, data = d.train.complete, family = binomial())) 
train.complete.mean = attr(x.train.complete, "scaled:center")
train.complete.sd = attr(x.train.complete, "scaled:scale")
y.train.complete = d.train.complete$TenYearCHD

x.train.imp = scale(model.matrix(TenYearCHD ~ . -1, data = d.train.imp, family = binomial()))
train.imp.mean = attr(x.train.imp, "scaled:center")
train.imp.sd = attr(x.train.imp, "scaled:scale")
y.train.imp = d.train.imp$TenYearCHD

# Same for test data, but with training set attributes to avoid data leak: When do we use this? 
x.test.complete = scale(model.matrix(TenYearCHD ~ . -1, data = d.test.complete, family = binomial()), center = train.complete.mean, scale = train.complete.sd)
x.test.imp = scale(model.matrix(TenYearCHD ~ . -1, data = d.test.imp, family = binomial()), center = train.imp.mean, scale = train.imp.sd)

```

Given the binary response it is natural to consider fitting a logistic regression model to our data. Although we intend to use lasso, it is nice to start by fitting a regular logistic regression model on the complete case data to get an indication of which covariates that are most present, and for later comparison. 
We obtain the regression coefficients, a confusion matrix, a ROC-curve and the ROC-AUC. 

```{r Logistic Model, fig.width= 4, fig.height=4}
# Is it interesting to have a logistic regression model for reference? 
mod0 <- glm(TenYearCHD ~ ., data = d.train.complete, family = binomial())
summary(mod0)
```


## Should we comment on whether or not we shoud do Group Lasso since Edu is included?



## Lasso on Complete Case 

We continue to do the Lasso on the complete case data. To do this, we use cross-validation to find $\lambda_{min}$ and use the highest $\lambda$ with deviance within one standard deviation of $\lambda_{min}$. We cross-validate for $\lambda$ and plot the shrinkage and binomial deviance.

To obtain a better understanding of these coefficients, we bootstrap from the training data to fit Lasso models and store their coefficients.

For easy implementation of the lasso method, we use the package glmnet @R-glmnet. 

```{r Bootstrap Lasso, echo = T, fig.width=9, fig.height=4.5}
set.seed(8701)
library(glmnet) # implementing lasso

B = 500 # Number of boostrap datasets
B.size.complete = dim(d.train.complete)[1] # Number of samples
# Make a matrix that store the computed coefficients
B.coef.complete = matrix(NA, nrow = B, ncol = length(colnames(x.train.complete))+1) 

for (i in 1:B){
  B.data = sample(1:B.size.complete, size = B.size.complete, replace = TRUE)
  x = x.train.complete[B.data,]
  y = y.train.complete[B.data]
  cv.out = cv.glmnet(x, y, family = "binomial", alpha = 1)
  lasso_mod = glmnet(x, y, family = "binomial", alpha = 1, intercept = T, lasso = cv.out$lambda.1se)
  
  B.coef.complete[i,] <- coef(lasso_mod,s=cv.out$lambda.1se)[,1]
}

colnames(B.coef.complete) = names(coef(lasso_mod,s=cv.out$lambda.1se)[,1])
B.coef.count.complete = ifelse(B.coef.complete == 0,0,1)
coef70.complete <- names(apply(B.coef.count.complete, 2, sum)/B)[apply(B.coef.count.complete, 2, sum)/B > 0.7]

# We do not include intercept as it is always in the model and too large to fit in the scaling.
par(mfrow = c(1,2))
boxplot.matrix(B.coef.complete[,-1], ylim = c(-0.25,0.55), las = 2, main = "Boxplot of estimated coefficients")
barplot(apply(B.coef.count.complete, 2, sum)/B, las = 2, main = "Percentage of times coefficient was nonzero")
abline(h=0.7,col="red", lty=2, lwd=3)

# Fit a logistic model using the test data and chosen covariates
mod.complete <- glm(TenYearCHD ~ age + male + sysBP + glucose, data = d.test.complete, family = binomial())
```

We ran the bootstrap constructing `r B` data sets. The variables that have nonzero coefficients in the Lasso models at least 70\% of the time, are `r coef70.complete`.
This is indeed similar to the ones we picked out earlier. 



## Lasso on Imputed Data

We now do the same thing, just using the imputed data instead of the complete case. 
Even though the imputed data includes more samples, the data quality is going down when we impute. 

To get hands-on experience with the `MICE` package, we also construct an imputed data set using `mice` and `mice.reuse`, for comparison. 
`mice` can be used on the entire training data, without needing to remove those samples with two or more covariates missing. 
The imputation model from the training set is emplyed to impute the test set as well, to avoid data leakage. 


First we try the Lasso on the imputed dataset where we imputed with our manual technique.
We show the shrinkage and the binomial deviance over $\lambda$, which is a part of the cross-validation
```{r Bootstrap Lasso imputed, echo = T, fig.width=9, fig.height=4.5}
B = 500 # Number of boostrap datasets
B.size.imp = dim(d.train.imp)[1] # Number of samples
# Make a matrix that store the computed coefficients
B.coef.imp = matrix(NA, nrow = B, ncol = length(colnames(x.train.imp))+1) 

for (i in 1:B){
  B.data = sample(1:B.size.imp, size = B.size.imp, replace = TRUE)
  x = x.train.imp[B.data,]
  y = y.train.imp[B.data]
  cv.out = cv.glmnet(x, y, family = "binomial", alpha = 1)
  lasso_mod = glmnet(x, y, family = "binomial", alpha = 1, intercept = T, lasso = cv.out$lambda.1se)
  
  B.coef.imp[i,] <- coef(lasso_mod,s=cv.out$lambda.1se)[,1]
}

colnames(B.coef.imp) = names(coef(lasso_mod,s=cv.out$lambda.1se)[,1])
B.coef.count.imp = ifelse(B.coef.imp == 0,0,1)
coef70.imp <- names(apply(B.coef.count.imp, 2, sum)/B)[apply(B.coef.count.imp, 2, sum)/B > 0.7]

# We do not include intercept as it is always in the model and too large to fit in the scaling.
par(mfrow = c(1,2))
boxplot.matrix(B.coef.imp[,-1], ylim = c(-0.25,0.55), las = 2, main = "Boxplot of estimated coefficients")
barplot(apply(B.coef.count.imp, 2, sum)/B, las = 2, main = "Percentage of times coefficient was nonzero")
abline(h=0.7,col="red", lty=2, lwd=3)

# Fit a logistic model using the test data and chosen covariates
## Should we use d.test.complete instead? 
mod.imp <- glm(TenYearCHD ~ age + male + sysBP + glucose + cigsPerDay, data = d.test.imp, family = binomial())
```

The performance of this model is quite similar to that of the complete case, which is to be expected.
The Lasso on the imputed data chooses `r coef70.imp`as the non-zero covariates. Notice in particular that `cigsPerDay` and `glucose` are non-zero more often when using the imputed data. This could perhaps be a sign of correlation in the training data. 


Although the difference is not huge, it may resemble that the data quality in the imputed data is slightly lower, although for selecting covariates, we obtained the same answer.
In our data rich situation, this is neither clear enough to be rendered true, nor actually a problem, but for data poor situations, this is something to keep in mind.

We try to do the same thing, using the `MICE`-imputed data set, first plotting the shrinkage and binomial deviance over $\lambda$, and then give the confusion matrix, ROC and ROC-AUC.

The predictive performance has not changed a lot, in the eyes of AUC. The performance of this model is quite similar to that of the complete case, which is to be expected.

We suspect that the sensitivity and specificity is even lower for the `MICE`-imputed data, as it also imputes those with more than one missing value.



# Inference

In order to do inference we simply fit a logistic regression model using the `glm` function in `R`, and extract the inference from there. However, we will keep the coefficients chosen by the lasso-bootstrapping procedure in the earlier models, and now use the test data to fit a logistic model to avoid overfitting. Note that we have only used the test data to predict and observe different measures, such as ROC-AUC, sensitivity/specificity, and so on. 
The test data is therefore suitable for inference, as it has not been perturbed in the procedure of fitting the models.


```{r complete case comparison}
table("Complete"=round(summary(mod.complete)$aic,2),
      "Imputed" = round(summary(mod.imp)$aic,2))

summary(mod.complete)
summary(mod.imp)

confint(mod.complete)
confint(mod.imp)
```


```{r alternative model, eval = F, echo = F}
mod.imp2 = glm(TenYearCHD ~ age + male + sysBP + glucose + cigsPerDay, data = d.test.complete, family = binomial())
summary(mod.imp2)
confint(mod.imp2)
```


# Discussion

When we compare the chosen coefficients from the complete case data compared to the imputed data, we see that they correspond. Thus, in a data-rich situation imputation may only introduce unnecessary variance, without having an immediate effect on the quality of the model or inference.

Another interesting thing we discovered, was that even though we obtain good results with the imputed data, the data quality seems to go down, if only barely. 
The sensitivity and specificity of the model went down slightly when using the imputed data, but as the Lasso selects the same variables, it does not matter a lot for our purposes.
The decrease in data quality might have been more visible if the percentage of imputed values were higher, or if we were in a data-poor situation.


# References 