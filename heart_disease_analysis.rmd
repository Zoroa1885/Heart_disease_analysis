---
subtitle: "MA8701 Advanced Statistical Learning V2023"
title: "Compulsory exercise: Team Supergreat"
author: "Nora Aasen, Elias Angelsen, Jonas Nordstr?m"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document
  # pdf_document
---
  
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = FALSE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",
                      fig.width=6, fig.height=4.5,fig.align = "center")
```


```{r Loading Packages,eval=TRUE,echo=FALSE}
# Load libraries
library(naniar)
library(mice)
library(glmnet)
library(tidyr)
library(dplyr)
library(caret)
library(ggcorrplot)
library(ggplot2)
library(e1071) #If we use KNN for imputation of education (multiclass)
```

# Introduction

In this project we have studied the [Framingham Coronary Heart Disease Dataset](https://www.kaggle.com/datasets/dileep070/heart-disease-prediction-using-logistic-regression?fbclid=IwAR1LE3P3vM1SyHBifotrNdXoKGv7szGR07labEAQo6XUqV9Pi90vtAp4mS4). This dataset contains patient information for inhabitants in Framingham, Massachusetts, and is typically used to predict the chance of getting coronary heart disease (CHD) within the next 10 years. For this project, however, we intend to use lasso to find the most important risk factors.


We start by examining the dataset. 


## Exploratory Analysis
```{r Loading Data}
# Load and look at data
# data <- framingham # Just if manual import
data <- read.csv("framingham.csv")
data_dim = dim(data)
pos_response = sum(data$TenYearCHD==1)
str(data)


ggplot(gather(data), aes(value)) + 
    geom_histogram(bins = 16) + 
    facet_wrap(~key, scales = 'free_x')

# Code education as a factor variable instead
data$education = factor(data$education, labels = c("none","hs","college","post-grad"))

```

This data set contains `r data_dim[1]` observations, `r data_dim[2]-1` covariates and a binary response variable `TenYearCHD`, thus we will try to fit a logistic regression model. The response variable has `r pos_response` observations that are 1, which equals about `r round(pos_response/data_dim[1]*100,1)`\% of the total observations. Most of our covariates are either binary, or numeric. However, we notice that the variable education is most likely a categorical covariate. We could not find any further elaboration for which four categories the numbers represent, so based on the frequency of each value and naive guessing we changed it to a factor variable and defined the four categories as `r names(summary(data$education))[1:4]`.

The next thing we looked at was the number of missing data in our data set.

```{r plot missing data}
# Look at the missing data
md_mat = md.pattern(data, rotate.names = T, plot = T) # Can we make these plots prettier?
gg_miss_var(data)

```


As we can see there are six covariates that has missing data: `glucose`, `education`, `BPMeds`, `totChol`, `cigsPerDay`, and `BMI`. We cannot use the rows that contain missing values as is. The easiest solution is to remove all rows that contains `NA`'s. This is the \textit{complete case} solution.  

```{r complete case}
# Make a dataset containing only the complete cases
df_complete <- data[complete.cases(data), ]
data_dim_c = dim(df_complete)
pos_response_c = sum(df_complete$TenYearCHD==1)
```

The complete data set contains `r data_dim_c[1]` observations and the response variable has `r pos_response` observations that are 1, which equals about `r round(pos_response_c/data_dim_c[1]*100,1)`\% of the total observations. As we can see, the proportion of positive observations in the response is the same, which is a good indicator that our data is missing at random (MAR), and it is therefore possible to do imputation.

From the exploratory analysis we see that there are many missing data points. However, the complete case data set is also quite large. Our main focus for this project will therefore be to compare the results from doing lasso on the complete case with the results from doing lasso on an imputed data set. 


```{r exp analysis, eval = F}
# Is it of interest to consider a correlation plot in this case? 

# Make a correlation plot 
cor_mat = cor(subset(df_complete, select = -education)) # Cannot compute correlation with factor variables
ggcorrplot(cor_mat, hc.order = TRUE, type = "lower",
   outline.col = "white",
   colors = c("blue", "white", "red"))
```

 
# Missing Data

To treat the missing data, we will use single imputation, as multiple imputation may cause difficulties with the resulting inference, as Rubins rules need to be combined with the Lasso, bootstrap and concluding inference. 

The single imputation technique we will use is simply regression imputation, where we adapt our regression technique depending on the type of variable imputed. For continuous variables, we simply use a linear regression model. For binary variables, we use logistic regression to classify their values, and for the variable "education", which is a four-class variable, we have utilized kNN for multiclass imputation. 

To avoid encountering observations with more than one missing value, and hence problems with regressing, we remove all samples with more than one NA. 

Our data is split into training and test sets, of sizes approximately $80%$ and $20%$ of the original dataset, respectively.
In order to avoid data leakage in our imputation if the test set, we fit the imputation models on the training set.
Note that we do not include the response (TenYearCHD) in the regression, and in order to avoid too much correlation between the imputed samples, we always base the regression models on the complete case data, instead of letting the imputed values for variable $n$ regress to impute variable $n+1$.  

First, we are removing all samples with more than one covariate missing.

```{r Clean Missing Data}

# Setting seed expressing our love for MA8701
set.seed(8701)

# Throwing out the samples with two or more NAs from the data
miss <- c()
for(i in 1:nrow(data)) {
  if(sum(is.na(data[i,])) > 1){
    miss <- append(miss,i)
    }  
}

data_m <- data[-miss,]

# length(miss) # This number should be 61 if done correctly (61 came from manually counting in the md.pattern-plot below)

# Exploring data_m

md.pattern(data_m)
gg_miss_var(data_m)


```


We further split the data into a training and test set, following the training-test-ratio that we have previously set.

``` {r Missing data split}

# Assigning which values should go where by randomly sampling ~20% of the indices. 
dim_data_m = dim(data_m)[1]
size_split_m = dim_data_m-round(dim_data_m/5)
spl_dat_m = sample(1:dim_data_m, size = size_split_m)

# Making training and test data
train_data_m = data_m[spl_dat_m,]
test_data_m = data_m[-spl_dat_m,]

# Making training and test data for covariates and response separately.

X_dat_m = data_m[,-dim(data_m)[2]] # Remove the column with response?
x.train_dat_m = X_dat_m[spl_dat_m,]
x.test_dat_m = X_dat_m[-spl_dat_m,]
y.train_dat_m = data_m$TenYearCHD[spl_dat_m]
y.test_dat_m = data_m$TenYearCHD[-spl_dat_m]


```

Using the above data sets, we make regression models for each missing variable and predict the new values that are to be imputed.



``` {r Missing data predictions}

# We fit linear models on the training set for glucose, cigsPerDay,

fit_for_simp_glucose_m <- lm(glucose ~., data = x.train_dat_m) # Linear model for glucose
fit_for_simp_cigs_m <- lm(cigsPerDay ~., data = x.train_dat_m) # Linear model for cigsPerDay


```

```{r MD preds old}
#---------------------------------------------------------------------------------------------------------------
#------------------------------ Predicting and imputing new glucose data ---------------------------------------
#---------------------------------------------------------------------------------------------------------------


# Checking if this is good. It is not necessarily good, as R^2 is low, but since R^2 is positive, we are probably doing better than mean imputation. 
# summary(fit_for_simp_glucose_m)

## --- Prediction --- 

# Which should be predicted? # Pick out those variables with missing glucose values that we want to impute. 
train_data_to_pred_gluc_m = ic(x.train_dat_m)
train_data_to_pred_gluc_m = train_data_to_pred_gluc_m[ici(train_data_to_pred_gluc_m$glucose),] 

test_data_to_pred_gluc_m = ic(x.test_dat_m)
test_data_to_pred_gluc_m = test_data_to_pred_gluc_m[ici(test_data_to_pred_gluc_m$glucose),]


# Predict the variables with missing glucose values that we want to impute. 
pred_glucose_train_m <- predict(fit_for_simp_glucose_m, newdata = train_data_to_pred_gluc_m) 
pred_glucose_test_m <- predict(fit_for_simp_glucose_m, newdata = test_data_to_pred_gluc_m)


## --- Visuals ---

# Lets see what we predicted, using base plot. 

plot(x.train_dat_m$glucose)
points(seq(1,length(x.train_dat_m[,1]),length.out = length(pred_glucose_train_m)),pred_glucose_train_m, col = "orange")
abline(h = mean(x.train_dat_m$glucose), col = "red")

# Can also do this by making histogram objects

hg_pred_gluc = hist(pred_glucose_train_m, plot = FALSE)
hg_old_gluc = hist(x.train_dat_m$glucose, plot = FALSE)

# Make transparent colors
c1 <- rgb(173,216,230,max = 255, alpha = 80, names = "lt.blue")
c2 <- rgb(255,192,203, max = 255, alpha = 80, names = "lt.pink")

# Plot together
plot(hg_old_gluc, col = c1) # Plot 1st histogram using a transparent color
plot(hg_pred_gluc, , col = c2, add = TRUE)


## Add predictions into original data by changing those with incomplete glucose


x.train_dat_m[ici(x.train_dat_m$glucose),]$glucose <- pred_glucose_train_m
x.test_dat_m[ici(x.test_dat_m$glucose),]$glucose <- pred_glucose_test_m

# Run these to see that we eliminated the missing variables.

md.pattern(x.train_dat_m)
gg_miss_var(x.train_dat_m)

md.pattern(x.test_dat_m)
gg_miss_var(x.test_dat_m)


# We continue to do this with the other cont. variables {Cigs per day, BMI, TotChol, HR (only for training data)}, based on the original complete case.

#---------------------------------------------------------------------------------------------------------------
#-------------------------- Predicting and imputing new data for Cigs Per Day ----------------------------------
#---------------------------------------------------------------------------------------------------------------


# Checking if this is good. It is not necessarily good, as R^2 is low, but since R^2 is positive, we are probably doing better than mean imputation. 
# summary(fit_for_simp_cigs_m)

## --- Prediction --- 

# Which should be predicted? # Pick out those variables with missing glucose values that we want to impute. 
train_data_to_pred_cigs_m = ic(x.train_dat_m)
train_data_to_pred_cigs_m = train_data_to_pred_cigs_m[ici(train_data_to_pred_cigs_m$cigsPerDay),]

test_data_to_pred_cigs_m = ic(x.test_dat_m)
test_data_to_pred_cigs_m = test_data_to_pred_cigs_m[ici(test_data_to_pred_cigs_m$cigsPerDay),]


# Predict the variables with missing glucose values that we want to impute.
pred_cigs_train_m <- predict(fit_for_simp_cigs_m, newdata = train_data_to_pred_cigs_m) 
pred_cigs_test_m <- predict(fit_for_simp_cigs_m, newdata = test_data_to_pred_cigs_m)


## --- Visuals ---

## Lets see what we predicted

plot(x.train_dat_m$cigs)
plot(pred_cigs_train_m)

# Make histogram objects

hg_pred_cigs = hist(pred_cigs_train_m, plot = FALSE)
hg_old_cigs = hist(x.train_dat_m$cigsPerDay, plot = FALSE)

# Plot together (basically useless because of few samples, but can probably fix that).
plot(hg_old_cigs, col = c1) # Plot 1st histogram using a transparent color
plot(hg_pred_cigs, , col = c2, add = TRUE)


## Add predictions into original data by changing those with incomplete cigsPerDay


x.train_dat_m[ici(x.train_dat_m$cigsPerDay),]$cigsPerDay <- pred_cigs_train_m
x.test_dat_m[ici(x.test_dat_m$cigsPerDay),]$cigsPerDay <- pred_cigs_test_m

md.pattern(x.train_dat_m)
gg_miss_var(x.train_dat_m)

md.pattern(x.test_dat_m)
gg_miss_var(x.test_dat_m)

# We continue to do this with the other continuous variables {BMI, TotChol, HR (only for training data)}


#---------------------------------------------------------------------------------------------------------------
#-------------------------- Predicting and imputing new data for BMI -------------------------------------------
#---------------------------------------------------------------------------------------------------------------



# We fit a linear model on the training set.

fit_for_simp_BMI_m <- lm(BMI ~., data = x.train_dat_m) # Q: Should the response be included? Open question :( We think no. Argue why later!

# Checking if this is good. It is not necessarily good, as R^2 is low, but since R^2 is positive, we are probably doing better than mean imputation.
# summary(fit_for_simp_BMI_m)

## --- Prediction --- 

# Which should be predicted?
train_data_to_pred_BMI_m = ic(x.train_dat_m)
train_data_to_pred_BMI_m = train_data_to_pred_BMI_m[ici(train_data_to_pred_BMI_m$BMI),]

test_data_to_pred_BMI_m = ic(x.test_dat_m)
test_data_to_pred_BMI_m = test_data_to_pred_BMI_m[ici(test_data_to_pred_BMI_m$BMI),]


# Predict them
pred_BMI_train_m <- predict(fit_for_simp_BMI_m, newdata = train_data_to_pred_BMI_m) 
pred_BMI_test_m <- predict(fit_for_simp_BMI_m, newdata = test_data_to_pred_BMI_m)



## --- Visuals ---

## Lets see what we predicted

plot(x.train_dat_m$BMI)
plot(pred_BMI_train_m)

# Make histogram objects

hg_pred_BMI = hist(pred_BMI_train_m, plot = FALSE)
hg_old_BMI = hist(x.train_dat_m$BMI, plot = FALSE)

# Plot together (basically useless because of few samples, but can probably fix that).
plot(hg_old_BMI, col = c1) # Plot 1st histogram using a transparent color
plot(hg_pred_BMI, col = c2, add = TRUE)


## Add predictions into original data by changing those with incomplete BMI


x.train_dat_m[ici(x.train_dat_m$BMI),]$BMI <- pred_BMI_train_m
x.test_dat_m[ici(x.test_dat_m$BMI),]$BMI <- pred_BMI_test_m

md.pattern(x.train_dat_m)
gg_miss_var(x.train_dat_m)

md.pattern(x.test_dat_m)
gg_miss_var(x.test_dat_m)

# We continue to do this with the other continuous variables {TotChol, HR (only for training data)}


#---------------------------------------------------------------------------------------------------------------
#-------------------------- Predicting and imputing new data for TotChol ---------------------------------------
#---------------------------------------------------------------------------------------------------------------



# We fit a linear model on the training set.

fit_for_simp_totChol_m <- lm(totChol ~., data = x.train_dat_m) # Q: Should the response be included? Open question :( We think no. Argue why later!

# Checking if this is good. It is not necessarily good, as R^2 is low, but since R^2 is positive, we are probably doing better than mean imputation.
# summary(fit_for_simp_totChol_m)

## --- Prediction --- 

# Which should be predicted?
train_data_to_pred_totChol_m = ic(x.train_dat_m)
train_data_to_pred_totChol_m = train_data_to_pred_totChol_m[ici(train_data_to_pred_totChol_m$totChol),]

test_data_to_pred_totChol_m = ic(x.test_dat_m)
test_data_to_pred_totChol_m = test_data_to_pred_totChol_m[ici(test_data_to_pred_totChol_m$totChol),]


# Predict them
pred_totChol_train_m <- predict(fit_for_simp_totChol_m, newdata = train_data_to_pred_totChol_m) 
pred_totChol_test_m <- predict(fit_for_simp_totChol_m, newdata = test_data_to_pred_totChol_m)


## --- Visuals ---

## Lets see what we predicted

plot(x.train_dat_m$totChol)
plot(pred_totChol_train_m)

# Make histogram objects

hg_pred_totChol = hist(pred_totChol_train_m, plot = FALSE)
hg_old_totChol = hist(x.train_dat_m$totChol, plot = FALSE)

# Plot together (basically useless because of few samples, but can probably fix that).
plot(hg_old_totChol, col = c1) # Plot 1st histogram using a transparent color
plot(hg_pred_totChol, col = c2, add = TRUE)


## Add predictions into original data by changing those with incomplete TotChol


x.train_dat_m[ici(x.train_dat_m$totChol),]$totChol <- pred_totChol_train_m
x.test_dat_m[ici(x.test_dat_m$totChol),]$totChol <- pred_totChol_test_m

md.pattern(x.train_dat_m)
gg_miss_var(x.train_dat_m)

md.pattern(x.test_dat_m)
gg_miss_var(x.test_dat_m)

# We continue to do this with the other continuous variables {HR (only for training data)}

#---------------------------------------------------------------------------------------------------------------
#-------------------------- Predicting and imputing new data for HR (training data) ----------------------------
#---------------------------------------------------------------------------------------------------------------



# We fit a linear model on the training set.

fit_for_simp_heartRate_m <- lm(heartRate ~., data = x.train_dat_m) 

# Checking if this is good. It is not necessarily good, as R^2 is low, but since R^2 is positive, we are probably doing better than mean imputation.
# summary(fit_for_simp_heartRate_m)

## --- Prediction --- 

# Which should be predicted?
train_data_to_pred_heartRate_m = ic(x.train_dat_m)
train_data_to_pred_heartRate_m = train_data_to_pred_heartRate_m[ici(train_data_to_pred_heartRate_m$heartRate),]


# Predict
pred_heartRate_train_m <- predict(fit_for_simp_heartRate_m, newdata = train_data_to_pred_heartRate_m) 

## Add predictions into original data by changing those with incomplete HR


x.train_dat_m[ici(x.train_dat_m$heartRate),]$heartRate <- pred_heartRate_train_m

md.pattern(x.train_dat_m)
gg_miss_var(x.train_dat_m)

md.pattern(x.test_dat_m)
gg_miss_var(x.test_dat_m)

# We continue to do this with the binary variable BP, using logistic regression for classification.



#---------------------------------------------------------------------------------------------------------------
#-------------------------- Predicting and imputing new data for BPMeds (Binary) -------------------------------
#---------------------------------------------------------------------------------------------------------------



# We fit a logistic model on the training set.

fit_for_simp_BPMeds_m <- glm(BPMeds ~., data = x.train_dat_m, family = "binomial")

# Checking if this is good. It is not necessarily good, as R^2 is low, but since R^2 is positive, we are probably doing better than mean imputation.
# summary(fit_for_simp_BPMeds_m)

## --- Prediction --- 

# Which should be predicted?
train_data_to_pred_BPMeds_m = ic(x.train_dat_m)
train_data_to_pred_BPMeds_m = train_data_to_pred_BPMeds_m[ici(train_data_to_pred_BPMeds_m$BPMeds),]

test_data_to_pred_BPMeds_m = ic(x.test_dat_m)
test_data_to_pred_BPMeds_m = test_data_to_pred_BPMeds_m[ici(test_data_to_pred_BPMeds_m$BPMeds),]


# Predict them (first numerically in (0,1), then classify to 0 or 1 depending on value)
pred_BPMeds_train_m_probs <- predict(fit_for_simp_BPMeds_m, newdata = train_data_to_pred_BPMeds_m, type = "response") 
pred_BPMeds_test_m_probs <- predict(fit_for_simp_BPMeds_m, newdata = test_data_to_pred_BPMeds_m, type = "response")

pred_BPMeds_train_m <- ifelse(pred_BPMeds_train_m_probs >= 0.5, 1, 0)
pred_BPMeds_test_m <- ifelse(pred_BPMeds_test_m_probs >= 0.5, 1, 0)


## --- Visuals ---

## Lets see what we predicted

plot(x.train_dat_m$BPMeds)
plot(pred_BPMeds_train_m) # Seems like a reasonable (~ 1/41) was classified as 1, compared to the training data. 

# Make histogram objects

hg_pred_BPMeds = hist(pred_BPMeds_train_m, plot = FALSE)
hg_old_BPMeds = hist(x.train_dat_m$BPMeds, plot = FALSE)

# Plot together (basically useless because of few samples, but can probably fix that).
plot(hg_old_BPMeds, col = c1) # Plot 1st histogram using a transparent color
plot(hg_pred_BPMeds, col = c2, add = TRUE)


## Add predictions into original data by changing those with incomplete BPMeds


x.train_dat_m[ici(x.train_dat_m$BPMeds),]$BPMeds <- pred_BPMeds_train_m
x.test_dat_m[ici(x.test_dat_m$BPMeds),]$BPMeds <- pred_BPMeds_test_m

md.pattern(x.train_dat_m)
gg_miss_var(x.train_dat_m)

md.pattern(x.test_dat_m)
gg_miss_var(x.test_dat_m)

# We continue to do this with the last variable, education, which is a four-class variable. We use KNN.



#---------------------------------------------------------------------------------------------------------------
#-------------------------- Predicting and imputing new data for Education (Multiclass) ------------------------
#---------------------------------------------------------------------------------------------------------------


# We fit a KNN model for classification on the training set.

fit_for_simp_edu_m <- gknn(education ~., data = x.train_dat_m)

## --- Prediction --- 

# Which should be predicted?
train_data_to_pred_edu_m = ic(x.train_dat_m)
train_data_to_pred_edu_m = train_data_to_pred_edu_m[ici(train_data_to_pred_edu_m$education),]

test_data_to_pred_edu_m = ic(x.test_dat_m)
test_data_to_pred_edu_m = test_data_to_pred_edu_m[ici(test_data_to_pred_edu_m$education),]


# Predict them
pred_edu_train_m <- predict(fit_for_simp_edu_m, newdata = train_data_to_pred_edu_m, type = "class") 
pred_edu_test_m <- predict(fit_for_simp_edu_m, newdata = test_data_to_pred_edu_m, type = "class")

## --- Visuals ---

## Lets see what we predicted

plot(x.train_dat_m$education)
plot(pred_edu_train_m) # Seems like a reasonable (~ 1/41) was classified as 1, compared to the training data. 

## Add predictions into original data by changing those with incomplete education.


x.train_dat_m[ici(x.train_dat_m$education),]$education <- pred_edu_train_m
x.test_dat_m[ici(x.test_dat_m$education),]$education <- pred_edu_test_m

md.pattern(x.train_dat_m)
gg_miss_var(x.train_dat_m)

md.pattern(x.test_dat_m)
gg_miss_var(x.test_dat_m)



```

There are several steps in this procedure that could be improved upon. First of all, we could have used more flexible models for imputation than linear imputation, and we could have fit several models on the training set and done an evaluation procedure within the training set (e.g. cross validated optimization of ROC-AUC) to pick the models imputing each variable. 

As mentioned, we did not include the response (TenYearCHD) in the regressions. It is not clear to us why it would be a better choice to include it. More reading and testing would be needed to figure out the optimal solution, if there is any canonical choice.



# Model

In the model section we will consider two data sets: the complete case and imputed case. Both data sets are further divided into a train and test set. 

```{r complete train test}
# Split the complete data into training and test
tr = 7/10 # train ratio
r = dim(df_complete)[1]
size = round(r*tr)
train = sample(1:r,size = size)
X = model.matrix(TenYearCHD ~  ., data = df_complete)[,-1] # Remove the response
x.train_c = scale(X[train,])
x.test_c = scale(X[-train,])
y.train_c = df_complete$TenYearCHD[train]
y.test_c = df_complete$TenYearCHD[-train]


```


Given the binary response it is natural to consider fitting a logistic regression model to our data. Although we intend to use lasso, it is nice to start by fitting a regular logistic regression model to get an indication of which covariates that are most present, and for later comparison. 
```{r Logistic Model}
# Fit a logistic model
mod0 <- glm(TenYearCHD ~ ., data = df_complete[train,], family = binomial())
set.seed(294)
summary(mod0)
```

The logistic regression model chooses `r names(coefficients(mod0))[c(1,2,3,5,8,13,14,18)]` as the significant covariates. 

## Lasso on Complete Case 

Next we wish to do.
```{r Lasso}

# Use cross-validation to find lambda
cv.out = cv.glmnet(x.train_c, y.train_c, family = "binomial", intercept = F, alpha = 1)
plot(cv.out$glmnet.fit, "lambda", label=F)
plot(cv.out)
cv.out$lambda.min
cv.out$lambda.1se
lasso_mod = glmnet(x.train_c, y.train_c, family = "binomial", intercept = F, alpha = 1,  lasso = cv.out$lambda.1se)

lasso_coef <- coef(lasso_mod,s=cv.out$lambda.1se)
lasso_coef[,1]
``` 


```{r Bootstrap Lasso}
B = 500

B_coef = matrix(NA, nrow = B, ncol = length(lasso_coef[,1]))
for (i in 1:B){
  data_b = sample(1:size, size = size, replace = TRUE)
  x = x.train_c[data_b,]
  y = y.train_c[data_b]
  cv.out = cv.glmnet(x, y, family = "binomial", alpha = 1)
  lasso_mod = glmnet(x, y, family = "binomial", alpha = 1, lasso = cv.out$lambda.1se)

  B_coef[i,] <- coef(lasso_mod,s=cv.out$lambda.1se)[,1]
}

colnames(B_coef) = names(coef(lasso_mod,s=cv.out$lambda.1se)[,1])
boxplot.matrix(B_coef[,-1], ylim = c(-0.25,0.55))

B_coef_count = ifelse(B_coef == 0,0,1)
apply(B_coef_count, 2, sum)
barplot(apply(B_coef_count, 2, sum)/B)
```

## Lasso on Imputed Data


# Inference

In order to do inference we simply fit a logistic regression model using the `glm` function in `R`, and extract the inference from there. However, we will keep the coefficients chosen by the lasso-bootstrapping iteration in last section, and now use the test data to fit a logistic model to avoid overfitting. 

```{r complete case comparison}
mod <- glm(TenYearCHD ~ 1 + age + male + sysBP, data = df_complete[-train,], family = binomial())

confint(mod)



confint(mod0)

```


# Discussion

Plan for the project: 
\begin{itemize}
\item Start by fitting lasso in bootstrap using complete data
\item Then singular imputation in training set and use same in test set
\item Fit a lasso within bootstrap
\item New method: still fitting lasso but this time do single imputation within the bootstrap
\item Compare the three different methods
\item Fit logistic regression and do inference under glm assumptions
\end{itemize}


New plan of 10.03?
Plan for the project: 
\begin{itemize}
\item Start by fitting lasso with bootstrap using complete data
\item Do single imputation (reg. with different models) in training set and use same in test set
\item Fit a lasso within bootstrap
\item Compare the two different methods
\item Fit logistic regression and do inference under glm assumptions
\end{itemize}