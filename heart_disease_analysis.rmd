---
subtitle: "MA8701 Advanced Statistical Learning V2023"
title: "Compulsory exercise: Team Supergreat"
author: "Nora Aasen, Elias Angelsen, Jonas Nordstr?m"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document
  # pdf_document
---
  
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",
                      fig.width=4, fig.height=3,fig.align = "center")
```


# Setup

```{r Loading Packages,eval=TRUE,echo=FALSE}
# Load libraries
library(naniar)
library(mice)
library(glmnet)
library(dplyr)
library(caret)
library(ggcorrplot)
library(ggplot2)
```

Plan for the project: 
\begin{itemize}
\item Start by fitting lasso in bootstrap using complete data
\item Then singular imputation in training set and use same in test set
\item Fit a lasso within bootstrap
\item New method: still fitting lasso but this time do single imputation within the bootstrap
\item Compare the three different methods
\item Fit logistic regression and do inference under glm assumptions
\end{itemize}

## Exploratory Analysis
```{r Loading Data}
# Load and look at data
# data <- framingham # Just if manual import
data <- read.csv("framingham.csv")
dim(data)
sum(data$TenYearCHD==1)
head(data)
str(data)

# Code education as a factor variable instead
data$education = factor(data$education, labels = c("none","hs","college","post-grad"))

# Look at the missing data
md.pattern(data)
gg_miss_var(data)

# Make a dataset containing only the complete cases
df_complete <- data[complete.cases(data), ]
dim(df_complete)
sum(df_complete$TenYearCHD==1)
str(df_complete)
# View(df_complete)
# Should we code the binary responses to be categorical instead of integers? 


# Split data into training and test
r = dim(df_complete)[1]
size = r-round(r/5)
spl = sample(1:r,size = size)
X = model.matrix(TenYearCHD ~  ., data = df_complete)[,-1]
x.train = X[spl,]
x.test = X[-spl,]
y.train = df_complete$TenYearCHD[spl]
y.test = df_complete$TenYearCHD[-spl]
```

```{r exp analysis}
# Make a correlation plot 
cor_mat = cor(df_complete)
ggcorrplot(cor_mat, hc.order = TRUE, type = "lower",
   outline.col = "white",
   colors = c("blue", "white", "red"))
```
## Missing Data

To treat the missing data, we will use single imputation, as multiple imputation may cause difficulties with the resulting inference (as Rubins rules are not clear on how to treat Lasso/Ridge/elNet or something? Recall what Mette said )

The single imputation technique we will use is simply to 

```{r Missing Data}
# Todo: 
# Set seed
# Split into training and test data
# Train imputation model explicitly on training data
# Predict values with the same model on the test data and use this as new test.

# Setting seed expressing our love for MA8701
set.seed(8701)



#----------------------------------------------------------------
### -------- Trying to just split, predict and go for it --------
#----------------------------------------------------------------

# Split into training and test data

#Which should go where?
dim_data = dim(data)[1]
size_split = dim_data-round(dim_data/5)
spl_dat = sample(1:dim_data, size = size_split)

train_data = data[spl_dat,]
test_data = data[-spl_dat,]

# In CC, we did something like this, but I think modmat does not handle NA's, yielding out of bounds subscripts.
              # X_dat = model.matrix(TenYearCHD ~  ., data = data)[,-1]           # Removes the intercept? Where did this come from?

# This below here is a fixed version of the code, as we may want to predict using the covariates only, not the response.
X_dat = data[,-dim(data)[2]] # Remove the column with response?
x.train_dat = X_dat[spl_dat,]
x.test_dat = X_dat[-spl_dat,]
y.train_dat = data$TenYearCHD[spl_dat]
y.test_dat = data$TenYearCHD[-spl_dat]



#We fit a linear model on the training set.

# QUESTION: Should the response (TenYearCHD) be included here?

fit_for_simp_glucose <- lm(glucose ~., data = x.train_dat)

# Checking if this is good. It is not. Adjusted R^2 is totally balls.
summary(fit_for_simp_glucose)

# Prediction 


### ----------PRoblem though. We have 61 samples with more than one covariate missing. How do we make many such models? ---------
        # ^ Can we just yeet them and work with 61 less samples? 
          # We get indices that dont match up, ripp.

#Predicting new glucose data.
pred_glucose_train <- predict(fit_for_simp_glucose, newdata = ic(x.train_dat)) 
pred_glucose_test <- predict(fit_for_simp_glucose, newdata = ic(x.test_dat)) #ic = incomplete cases   #ici = incomplete case indicator


#--------------------------------------------------------------------------------------------------------
# ---------------------- Removing all samples that have more than one covariate missing -----------------
# ---------------------- and trying again, hopefully with better results --------------------------------
#--------------------------------------------------------------------------------------------------------

# Throwing out the samples with two or more NAs from the data
miss <- c()
for(i in 1:nrow(data)) {
  if(sum(is.na(data[i,])) > 1){
    miss <- append(miss,i)
    }  
}

data2 <- data[-miss,]

length(miss) # This number should be 61 if done correctly (61 came from manually counting)

# Exploring data2

md.pattern(data2)
gg_miss_var(data2)



# Split into training and test data

#Which should go where?
dim_data2 = dim(data2)[1]
size_split2 = dim_data2-round(dim_data2/5)
spl_dat2 = sample(1:dim_data2, size = size_split2)

# Making training and test data
train_data2 = data2[spl_dat2,]
test_data2 = data2[-spl_dat2,]

# Making training and test data for covariates and response separately.

X_dat2 = data2[,-dim(data2)[2]] # Remove the column with response?
x.train_dat2 = X_dat2[spl_dat2,]
x.test_dat2 = X_dat2[-spl_dat2,]
y.train_dat2 = data2$TenYearCHD[spl_dat2]
y.test_dat2 = data2$TenYearCHD[-spl_dat2]


## Predicting new glucose data.

# We fit a linear model on the training set.

fit_for_simp_glucose_2 <- lm(glucose ~., data = x.train_dat2) # Should we have included the response? Think not.

# Checking if this is good. It is not. Adjusted R^2 is totally balls.
summary(fit_for_simp_glucose_2)

# Prediction 

# Which should be predicted?
train_data_to_pred_gluc_2 = ic(x.train_dat2)
train_data_to_pred_gluc_2 = train_data_to_pred_gluc_2[ici(train_data_to_pred_gluc_2$glucose),]

test_data_to_pred_gluc_2 = ic(x.test_dat2)
test_data_to_pred_gluc_2 = test_data_to_pred_gluc_2[ici(test_data_to_pred_gluc_2$glucose),]


# Predict them
pred_glucose_train_2 <- predict(fit_for_simp_glucose_2, newdata = train_data_to_pred_gluc_2) 
pred_glucose_test_2 <- predict(fit_for_simp_glucose2, newdata = test_data_to_pred_gluc_2)



## Lets see what we predicted

plot(train_data$glucose)
plot(pred_glucose_train_2)

# Make histogram objects

hg_pred = hist(pred_glucose_train_2, plot = FALSE)
hg_old = hist(train_data$glucose, plot = FALSE)


# Make transparent colors
c1 <- rgb(173,216,230,max = 255, alpha = 80, names = "lt.blue")
c2 <- rgb(255,192,203, max = 255, alpha = 80, names = "lt.pink")

# Plot together
plot(hg_old, col = c1) # Plot 1st histogram using a transparent color
plot(hg_pred, , col = c2, add = TRUE)


## Add predictions into original data by changing those with incomplete glucose


x.train_dat2[ici(x.train_dat2$glucose),]$glucose <- pred_glucose_train_2
x.test_dat2[ici(x.test_dat2$glucose),]$glucose <- pred_glucose_test_2

md.pattern(x.train_dat2)
gg_miss_var(x.train_dat2)

md.pattern(x.test_dat2)
gg_miss_var(x.test_dat2)



# Can do this again for the other variables needed. Is this ok? 






```



# Model

## Naive Approach
```{r Logistic Model}
# Fit a logistic model
mod0 <- glm(TenYearCHD ~ ., data = df_complete, family = binomial())

summary(mod0)
```


## Lasso
```{r Lasso}
# Må vi fjerne intercept før lasso? 

# Use cross-validation to find lambda
cv.out = cv.glmnet(x.train, y.train, family = "binomial", alpha = 1)
plot(cv.out$glmnet.fit, "lambda", label=FALSE)
plot(cv.out)
cv.out$lambda.min
cv.out$lambda.1se
lasso_mod = glmnet(x.train, y.train, family = "binomial", alpha = 1, lasso = cv.out$lambda.1se)

lasso_coef <- coef(lasso_mod,s=cv.out$lambda.1se)
lasso_coef[,1]
``` 




```{r Bootstrap Lasso}
B = 1000

B_coef = matrix(NA, nrow = B, ncol = 16)
for (i in 1:B){
  print(i)
  spl_b = sample(1:size, size = size, replace = TRUE)
  x = x.train[spl_b,]
  y = y.train[spl_b]
  cv.out = cv.glmnet(x, y, family = "binomial", alpha = 1)
  lasso_mod = glmnet(x.train, y.train, family = "binomial", alpha = 1, lasso = cv.out$lambda.1se)

  B_coef[i,] <- coef(lasso_mod,s=cv.out$lambda.1se)[,1]
}

colnames(B_coef) = names(coef(lasso_mod,s=cv.out$lambda.1se)[,1])
B_coef = ifelse(B_coef == 0,0,1)
apply(B_coef, 2, sum)

```

## Ridge 
```{r Ridge Model}
response <- "TenYearCHD"
features <- names(df_complete)[!names(df_complete) %in% c("TenYearCHD")]

df_complete["education"] <- factor(df_complete$education, labels = c("None", "HighSchool", "Collage", "Grad"))

X <- model.matrix(~.-1, df_complete[features])
y <- df_complete[response]

train_frac = 0.8
train.ind <- sample(c(TRUE, FALSE), nrow(df_complete),
                    replace=TRUE, prob=c(train_frac, 1-train_frac))

X_train <- X[train.ind,]
X_test <- X[!train.ind,]

y_train <- y[train.ind,]
y_test <- y[!train.ind,]

ridge_cv <- cv.glmnet(X_train, y_train, family = "binomial", alpha = 0)

plot(ridge_cv)
lambda_min = ridge_cv$lambda.min
lambda_1se = ridge_cv$lambda.1se
cat("Lambda min:", lambda_min, "\n")
cat("Lambda 1se:", lambda_1se)


print(ridge_cv)
coef(ridge_cv, s = "lambda.1se")
```
Make a fit with the best lambda and measuring error
```{r}
ridge_fit = glmnet(X_train, y_train, lambda = lambda_min, alpha = 0, family = "binomial")
ridge_pred = predict(ridge_fit, X_test, type = "class")

ridge_pred_class = factor(round(ridge_pred))
y_test <- factor(y_test)

conf_mat <- confusionMatrix(ridge_pred_class, y_test)
conf_mat

mse_ridge = mean((ridge_pred - y_test)^2)
mse_ridge
```

