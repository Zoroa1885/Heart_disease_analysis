---
subtitle: "MA8701 Advanced Statistical Learning V2023"
title: "Compulsory exercise: Team Supergreat"
author: "Nora Aasen, Elias Angelsen, Jonas Nordstrom"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  # html_document
  pdf_document
---
  
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = FALSE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",
                      fig.width=6, fig.height=4.5,fig.align = "center")
```


```{r Loading Packages,eval=TRUE,echo=FALSE}
# Load libraries
library(naniar)
library(mice)
library(glmnet)
library(tidyr)
library(dplyr)
library(caret)
library(ggcorrplot)
library(ggplot2)
library(e1071)
library(pROC)
library(caret)
library(NADIA)
```

# Introduction

In this project we have studied the [Framingham Coronary Heart Disease Dataset](https://www.kaggle.com/datasets/dileep070/heart-disease-prediction-using-logistic-regression?fbclid=IwAR1LE3P3vM1SyHBifotrNdXoKGv7szGR07labEAQo6XUqV9Pi90vtAp4mS4). This dataset contains patient information for inhabitants in Framingham, Massachusetts, and is typically used to predict the chance of getting coronary heart disease (CHD) within the next 10 years. For this project, however, we intend to use lasso to find the most important risk factors.

Our main ------------TODO---------

We start by examining the dataset. 


## Exploratory Analysis
```{r Loading Data}
# Load and look at data
# data <- framingham # Just if manual import
data <- read.csv("framingham.csv")
data_dim = dim(data)
pos_response = sum(data$TenYearCHD==1)
str(data)

# We visualize the data
ggplot(gather(data), aes(value)) + 
    geom_histogram(bins = 16) + 
    facet_wrap(~key, scales = 'free_x')

# Code education as a factor variable instead of 1-2-3-4.
data$education = factor(data$education, labels = c("none","hs","college","post-grad"))

```

This data set contains `r data_dim[1]` observations, `r data_dim[2]-1` covariates and a binary response variable `TenYearCHD`. We will try to fit a logistic regression model. The response variable has `r pos_response` observations that are 1, which equals about `r round(pos_response/data_dim[1]*100,1)`\% of the total observations. Most of our covariates are either binary, or numeric. However, we notice that the variable education is most likely a categorical covariate. We could not find any further elaboration for which four categories the numbers represent, so based on the frequency of each value and qualified guessing, we changed it to a factor variable and defined the four categories as `r names(summary(data$education))[1:4]`.

The next thing we looked at was the number of missing data in our data set.

```{r plot missing data}
# Look at the missing data
md_mat = md.pattern(data, rotate.names = T, plot = T)
gg_miss_var(data)
```

As we can see there are six covariates that has missing data: `glucose`, `education`, `BPMeds`, `totChol`, `cigsPerDay`, and `BMI`. We cannot use the rows that contain missing values as is. The easiest solution is to remove all rows that contains `NA`'s. This is the \textit{complete case} solution.
We split the data into training and test sets, as well as copying the complete case data set for later solutions. 

```{r train-test split}
# Split into training and test first
set.seed(8701)
tr = 7/10 # train ratio
r = dim(data)[1]
size = round(r*tr)
train = sample(1:r,size = size)

d.train = data[train,]
d.test = data[-train,]
```

```{r complete case}
# Make a dataset containing only the complete cases
d.complete <- data[complete.cases(data), ]
d.train.complete <- d.train[complete.cases(d.train), ]
d.test.complete <- d.test[complete.cases(d.test), ]

pos_response_c = sum(data[complete.cases(data),]$TenYearCHD==1)
```

The complete data set contains `r sum(complete.cases(data))` observations and the response variable has `r pos_response_c` observations that are 1, which equals about `r round(pos_response_c/sum(complete.cases(data))*100,1)`\% of the total observations. As we can see, the proportion of positive observations in the response is the same, which is a good indicator that our data is missing at random (MAR), as we will discuss later.

To end our exploratory analysis, we make a correlation plot of the complete data set, but note that we have removed the multi-class variable 'education'. 

```{r exp analysis, eval = F}
# Is it of interest to consider a correlation plot in this case? 

# Make a correlation plot 
cor_mat = cor(subset(d.complete, select = -education)) # Cannot compute correlation with factor variables
ggcorrplot(cor_mat, hc.order = TRUE, type = "lower",
   outline.col = "white",
   colors = c("blue", "white", "red"))
```

The above tells us that some variables are highly correlated. These are 'currentSmoker' and 'cigsPerDay', 'sysBP' and 'diaBP', 'prevalentHyp and 'diaBP'/'sysBP', and 'diabetes' and 'glucose'.
Initially, we therefore expect at most one of 'currentSmoker' and 'cigsPerDay', one of 'sysBP' and 'diaBP', and one of 'diabetes' and 'glucose' to be picked out by the Lasso.
The case for 'prevalentHyp' is slightly more complex. It's influence towards the response may be absorbed into 'sysBP' and/or 'diaBP', in the eyes of the Lasso. 
It could also dominate both and push both of these out of the model, or it could be included in the model, but with much lower significance than the other included variables.
We don't know yet. 

From the exploratory analysis we know that there are many missing data points. However, the complete case data set is quite large, enabling the use of purely complete cases. Our main focus for this project will partially be to compare the results from doing lasso on the complete case with the results from doing lasso on an imputed data set(s).


# Missing Data


In our data set, we have missing values, and we are additionally going to handle these missing values in a slightly more refined manner than just considering the complete case.

Recall that there are several types of mechanisms for missing data.
Let $Z = (X,y)$ denote the full collection of covariates and responses, respectively, and we let a subscript mis/obs indicate whether we are restricting $Z$ (or $X$) to the missing or observed parts, respectively.
We may form an indicator (0-1) matrix $R$ indicating missing (0) and observed (1) covariates. 
Assume $\psi$ is short for the parameters in the distribution of $R$.

The missing data may be characterized by the conditioning in the distribution of $R$.
We define the data to be:
\begin{itemize}
  \item missing completely at random (MCAR) if $P(R | Z, \psi) = P(R | \psi)$,
  \item missing at random (MAR) if $P(R | Z, \psi) = P(R | Z_{obs}, \psi)$,
  \item missing not at random (MNAR) if $P(R | Z, \psi) = P(R | Z, \psi)$ (i.e. we don't have MCAR or MAR).
\end{itemize}

By for example exploring the missing pattern of the variable 'cigsPerDay', we obtain a clear indication that our missing mechanism is not MCAR.
No non-smoker has failed to answer the question "how may cigarettes do you smoke a day?", which is a question only aimed at smokers. 
The simple explanation may be that the survey they answered automatically fills in $0$ for 'cigsPerDay' if you claim to be a non-smoker. 
In more mathematical terms, the missingness of 'cigsPerDay' depends on the observed answer to "do you smoke?" (found in variable 'currentSmoker'), indicating that we do not work with MCAR data.
Luckily, most methods are applicable if our missingness is at least MAR. 

We will assume that the missing mechanism is MAR, as there is no clear reason to suspect it to be MNAR. 
An example of a pressing obstruction to being MAR can again be found in 'cigsPerDay'. In the real world, if smokers have failed to report 'cigsPerDay', it may for example be because they smoke so much that they are ashamed to answer the question (and skips it), but we will simply assume that such a thing is not happening, as we trust people to answer truthfully (often, at least) if they volunteer for medical studies. 

To treat the missing data, we will use single imputation, as multiple imputation may cause difficulties with the resulting inference, as Rubins rules needs to be combined with the Lasso, bootstrap and concluding inference. Multiple imputation has therefore not been the focus in this project.

The single imputation technique we will use is simply regression imputation, where we adapt our regression technique depending on the type of variable imputed. For continuous variables, we simply use a linear regression model. For binary variables, we use logistic regression to classify their values, and for the variable "education", which is a four-class variable, we have utilized kNN for multiclass imputation. 
This is implemented manually, but this could have been done using the 'MICE' package and the function 'mice'.

To avoid encountering observations with more than one missing value, and hence problems with regressing, we remove all samples with more than one NA. 

Our data is split into training and test sets, with the test-to-training ratio being 'r tr' of the original dataset.
In order to avoid data leakage in our imputation of the test set, we fit the imputation models on the training set.
The main idea is that the test set should be viewed as several independent observations. Using the test set to impute itself will use information not present at the time of training and will yield unintended correlation.
This is again done manually, but could also have been done using 'mice.reuse', as we will do later.

Note that we do not include the response (TenYearCHD) in the regression, and in order to avoid too much correlation between the imputed samples, we always base the regression models on the complete case data, instead of letting the imputed values for variable $n$ regress to impute variable $n+1$.  

First, we remove all samples with more than one covariate missing, as this is only $61$ samples. 

```{r Remove rows with more than 1 NA}

# Setting seed expressing our love for MA8701
set.seed(8701)

# Throwing out the samples with two or more NAs from the data
miss <- c()
for(i in 1:nrow(d.train)) {
  if(sum(is.na(d.train[i,])) > 1){
    miss <- append(miss,i)
    }  
}

d.train.missing <- d.train[-miss,]


miss <- c()
for(i in 1:nrow(d.test)) {
  if(sum(is.na(d.test[i,])) > 1){
    miss <- append(miss,i)
    }  
}

d.test.missing <- d.test[-miss,]

# Check if we have removed all rows with double NA

# md.pattern(d.train.missing, rotate.names = T)
# md.pattern(d.test.missing, rotate.names = T)


```


We further split the data into further training and test sets with and without the response 'TenYearCHD', following the training-test-ratio that we have previously set.

``` {r Missing data split}
# Making training and test data for covariates and response separately.
# X.train.miss = subset(d.train.missing, select = -TenYearCHD)
# y.train.miss = d.train$TenYearCHD

# X.test.miss = subset(d.test.missing, select = -TenYearCHD)
# y.test.miss = d.test$TenYearCHD

x.train_dat_m = subset(d.train.missing, select = -TenYearCHD)
y.train.miss = d.train.missing$TenYearCHD

x.test_dat_m = subset(d.test.missing, select = -TenYearCHD)
y.test.miss = d.test.missing$TenYearCHD
```

Using these data sets, we make regression models for each missing variable based on the data we have. 
These models automatically neglects NA's. For 'glucose', 'cigsPerDay', 'BMI', 'totChol' and 'heartRate', we fit linear models, while for the binary variable 'BPMeds', a logistic model is fit. The multi-class variable 'education' will be imputed using a kNN-model. 

``` {r Missing data regression models}

# We fit linear models on the training set for glucose, cigsPerDay, BMI, totChol, heartRate.
fit_for_simp_glucose_m <- lm(glucose ~., data = x.train_dat_m)    # Linear model for glucose
fit_for_simp_cigs_m <- lm(cigsPerDay ~., data = x.train_dat_m)    # Linear model for cigsPerDay
fit_for_simp_BMI_m <- lm(BMI ~., data = x.train_dat_m)            # Linear model for BMI
fit_for_simp_totChol_m <- lm(totChol ~., data = x.train_dat_m)    # Linear model for totChol
fit_for_simp_heartRate_m <- lm(heartRate ~., data = x.train_dat_m)# Linear model for heartRate 

# We fit a logistic model on the training set for BPMeds, being a binary variable.
fit_for_simp_BPMeds_m <- glm(BPMeds ~., data = x.train_dat_m, family = "binomial") #Logistic model for BPMeds

# We fit a KNN model on the training set for education, being a multi-class variable.
fit_for_simp_edu_m <- gknn(education ~., data = x.train_dat_m) # kNN model for education

```

To predict the missing values for each variable, we pick out those samples with missing values of each variable from the training and test set.
Note that we are imputing for both the training and test set, even though the sampled (therefore "random") split into training and test set may have sorted all NA's of a variable (e.g. heartRate) into a single set (i.e. into either training or test set).
This is not a problem, as we are only picking out the incomplete cases. Therefore, predicting and filling in the missing values in the complete part is a vacuous procedure, not yielding any problems.

``` {r Missing data which-to-predict}

# Pick out those variables with missing glucose from the training and test set.
train_data_to_pred_gluc_m = ic(x.train_dat_m)
train_data_to_pred_gluc_m = train_data_to_pred_gluc_m[ici(train_data_to_pred_gluc_m$glucose),] 
test_data_to_pred_gluc_m = ic(x.test_dat_m)
test_data_to_pred_gluc_m = test_data_to_pred_gluc_m[ici(test_data_to_pred_gluc_m$glucose),]

# Pick out those variables with missing cigsPerDay from the training and test set.
train_data_to_pred_cigs_m = ic(x.train_dat_m)
train_data_to_pred_cigs_m = train_data_to_pred_cigs_m[ici(train_data_to_pred_cigs_m$cigsPerDay),]
test_data_to_pred_cigs_m = ic(x.test_dat_m)
test_data_to_pred_cigs_m = test_data_to_pred_cigs_m[ici(test_data_to_pred_cigs_m$cigsPerDay),]

# Pick out those variables with missing BMI from the training and test set.
train_data_to_pred_BMI_m = ic(x.train_dat_m)
train_data_to_pred_BMI_m = train_data_to_pred_BMI_m[ici(train_data_to_pred_BMI_m$BMI),]
test_data_to_pred_BMI_m = ic(x.test_dat_m)
test_data_to_pred_BMI_m = test_data_to_pred_BMI_m[ici(test_data_to_pred_BMI_m$BMI),]

# Pick out those variables with missing totChol from the training and test set.
train_data_to_pred_totChol_m = ic(x.train_dat_m)
train_data_to_pred_totChol_m = train_data_to_pred_totChol_m[ici(train_data_to_pred_totChol_m$totChol),]
test_data_to_pred_totChol_m = ic(x.test_dat_m)
test_data_to_pred_totChol_m = test_data_to_pred_totChol_m[ici(test_data_to_pred_totChol_m$totChol),]

# Pick out those variables with missing heartRate from the training and test set.
train_data_to_pred_heartRate_m = ic(x.train_dat_m)
train_data_to_pred_heartRate_m = train_data_to_pred_heartRate_m[ici(train_data_to_pred_heartRate_m$heartRate),]
test_data_to_pred_heartRate_m = ic(x.test_dat_m)
test_data_to_pred_heartRate_m = test_data_to_pred_heartRate_m[ici(test_data_to_pred_heartRate_m$heartRate),]

# Pick out those variables with missing BPMeds from the training and test set.
train_data_to_pred_BPMeds_m = ic(x.train_dat_m)
train_data_to_pred_BPMeds_m = train_data_to_pred_BPMeds_m[ici(train_data_to_pred_BPMeds_m$BPMeds),]
test_data_to_pred_BPMeds_m = ic(x.test_dat_m)
test_data_to_pred_BPMeds_m = test_data_to_pred_BPMeds_m[ici(test_data_to_pred_BPMeds_m$BPMeds),]

# Pick out those variables with missing education from the training and test set.
train_data_to_pred_edu_m = ic(x.train_dat_m)
train_data_to_pred_edu_m = train_data_to_pred_edu_m[ici(train_data_to_pred_edu_m$education),]
test_data_to_pred_edu_m = ic(x.test_dat_m)
test_data_to_pred_edu_m = test_data_to_pred_edu_m[ici(test_data_to_pred_edu_m$education),]

```

We then predict the missing values for imputation. For the logistic model fit to predict 'BPMeds', we assign a prediction class $1$ or $0$ depending on whether the predicted value is above or below $0.5$.

``` {r Missing Data predictions}
set.seed(8701)

# Predicting the values to impute for glucose on the training and test data.
pred_glucose_train_m <- predict(fit_for_simp_glucose_m, newdata = train_data_to_pred_gluc_m) 
pred_glucose_test_m <- predict(fit_for_simp_glucose_m, newdata = test_data_to_pred_gluc_m)

# Predicting the values to impute for cigsPerDay on the training and test data.
pred_cigs_train_m <- predict(fit_for_simp_cigs_m, newdata = train_data_to_pred_cigs_m) 
pred_cigs_test_m <- predict(fit_for_simp_cigs_m, newdata = test_data_to_pred_cigs_m)

# Predicting the values to impute for BMI on the training and test data.
pred_BMI_train_m <- predict(fit_for_simp_BMI_m, newdata = train_data_to_pred_BMI_m) 
pred_BMI_test_m <- predict(fit_for_simp_BMI_m, newdata = test_data_to_pred_BMI_m)

# Predicting the values to impute for totChol on the training and test data.
pred_totChol_train_m <- predict(fit_for_simp_totChol_m, newdata = train_data_to_pred_totChol_m) 
pred_totChol_test_m <- predict(fit_for_simp_totChol_m, newdata = test_data_to_pred_totChol_m)

# Predicting the values to impute for heartRate on the training and test data.
pred_heartRate_train_m <- predict(fit_for_simp_heartRate_m, newdata = train_data_to_pred_heartRate_m) 
pred_heartRate_test_m <- predict(fit_for_simp_heartRate_m, newdata = test_data_to_pred_heartRate_m)

# Predicting the classes to impute for BPMeds on the training and test data by first predicting numerical values in (0,1) and then classifying these to either 0 or 1. 
pred_BPMeds_train_m_probs <- predict(fit_for_simp_BPMeds_m, newdata = train_data_to_pred_BPMeds_m, type = "response") 
pred_BPMeds_test_m_probs <- predict(fit_for_simp_BPMeds_m, newdata = test_data_to_pred_BPMeds_m, type = "response")
pred_BPMeds_train_m <- ifelse(pred_BPMeds_train_m_probs >= 0.5, 1, 0)
pred_BPMeds_test_m <- ifelse(pred_BPMeds_test_m_probs >= 0.5, 1, 0)

# Predicting the classes to impute for education on the training and test data.
pred_edu_train_m <- predict(fit_for_simp_edu_m, newdata = train_data_to_pred_edu_m, type = "class") 
pred_edu_test_m <- predict(fit_for_simp_edu_m, newdata = test_data_to_pred_edu_m, type = "class")

```

To see how our predictions compare to the complete case, we plot (in different ways) the predicted values/classes.
For illustrating different types of plots useful for this, we plot glucose twice, as both a transparent histogram and a point plot. 
This is only for illustrational purposes and therefore only done for the training data with its imputed values. 

```{r Missing data train-pred-plots}

# ---- Make transparent colors for plotting ----
c1 <- rgb(173,216,230,max = 255, alpha = 80, names = "lt.blue")
c2 <- rgb(255,192,203, max = 255, alpha = 80, names = "lt.pink")

# ---- Make histogram objects for glucose ----

# Histogram objects for glucose
hg_pred_gluc = hist(pred_glucose_train_m, plot = FALSE)
hg_old_gluc = hist(x.train_dat_m$glucose, plot = FALSE)


# ---- Make point plots over indexes for all except education ----

par(mfrow = c(2,3))

# Point plot for glucose
plot(x.train_dat_m$glucose, ylab = "Value of Glucose", main = "Glucose")
points(seq(1,length(x.train_dat_m[,1]),length.out = length(pred_glucose_train_m)),pred_glucose_train_m, col = "orange", pch = 16)
abline(h = mean(x.train_dat_m$glucose), col = "red")

# Point plot for cigsperday
plot(x.train_dat_m$cigs, ylab = "Value of cigsPerDay", main = "cigsPerDay")
points(seq(1,length(x.train_dat_m[,1]),length.out = length(pred_cigs_train_m)),pred_cigs_train_m, col = "orange", pch = 16)
abline(h = mean(x.train_dat_m$cigsPerDay), col = "red")

# Point plot for BMI
plot(x.train_dat_m$BMI, ylab = "Value of BMI", main = "BMI")
points(seq(1,length(x.train_dat_m[,1]),length.out = length(pred_BMI_train_m)),pred_BMI_train_m, col = "orange", pch = 16)
abline(h = mean(x.train_dat_m$BMI), col = "red")

# Point plot for totChol
plot(x.train_dat_m$totChol, ylab = "Value of totChol", main = "totChol")
points(seq(1,length(x.train_dat_m[,1]),length.out = length(pred_totChol_train_m)),pred_totChol_train_m, col = "orange", pch = 16)
abline(h = mean(x.train_dat_m$totChol), col = "red")

# Point plot for heartRate
plot(x.train_dat_m$heartRate, ylab = "Value of heartRate", main = "heartRate")
points(seq(1,length(x.train_dat_m[,1]),length.out = length(pred_heartRate_train_m)),pred_heartRate_train_m, col = "orange", pch = 16)
abline(h = mean(x.train_dat_m$heartRate), col = "red")

# Point plot for BPMeds
plot(x.train_dat_m$BPMeds, ylab = "Class of BPMeds", main = "BPMeds")
points(seq(1,length(x.train_dat_m[,1]),length.out = length(pred_BPMeds_train_m)),pred_BPMeds_train_m, col = "orange", pch = 16)
abline(h = mean(x.train_dat_m$BPMeds), col = "red")

# ---- Make histogram plots for glucose and education ----

par(mfrow = c(1,1))

# Histogram plots for glucose
plot(hg_old_gluc, col = c1, xlab = "Value of Glucose", main = "Glucose") # Plot 1st histogram using a transparent color
plot(hg_pred_gluc, , col = c2, add = TRUE)
legend("topright", legend=c("Observed - blue", "Predictions - pink"), cex=0.6)

par(mfrow = c(1,2))

# Histograms for education
plot(x.train_dat_m$education, main = "Education - Observed", ylab = "Frequency")
plot(pred_edu_train_m, main = "Education - Predicted", ylab = "Frequency")

```
For most of the linear predictors, we are imputing using linear models. As we can see in these plots, we are close to the mean, but we are including more variance than what mean imputation would be able to. The binary variable 'BPMeds' is often classified to zero, which is expected, as the variable explains whether or not the patient was taking blood pressure medicine at the time of the survey. The 'education' variable is predicted using kNN, and we predict surprisingly many with higher education.
This can imply that the kNN-model we fit is not optimal for predicting 'education'.

Lastly, we update our data with the newly imputed values.

``` {r Missing Data imputation}

# ---- Making new data sets based on the old ones ----
x.train_imp = x.train_dat_m
x.test_imp = x.test_dat_m

# ---- We add the predictions to this data set. ----

# Adding glucose
x.train_imp[ici(x.train_imp$glucose),]$glucose <- pred_glucose_train_m
x.test_imp[ici(x.test_imp$glucose),]$glucose <- pred_glucose_test_m

# Adding cigsPerDay
x.train_imp[ici(x.train_imp$cigsPerDay),]$cigsPerDay <- pred_cigs_train_m
x.test_imp[ici(x.test_imp$cigsPerDay),]$cigsPerDay <- pred_cigs_test_m

# Adding BMI
x.train_imp[ici(x.train_imp$BMI),]$BMI <- pred_BMI_train_m
x.test_imp[ici(x.test_imp$BMI),]$BMI <- pred_BMI_test_m

# Adding totChol
x.train_imp[ici(x.train_imp$totChol),]$totChol <- pred_totChol_train_m
x.test_imp[ici(x.test_imp$totChol),]$totChol <- pred_totChol_test_m

# Adding heartRate
x.train_imp[ici(x.train_imp$heartRate),]$heartRate <- pred_heartRate_train_m
x.test_imp[ici(x.test_imp$heartRate),]$heartRate <- pred_heartRate_test_m

# Adding BPMeds
x.train_imp[ici(x.train_imp$BPMeds),]$BPMeds <- pred_BPMeds_train_m
x.test_imp[ici(x.test_imp$BPMeds),]$BPMeds <- pred_BPMeds_test_m

# Adding education
x.train_imp[ici(x.train_imp$education),]$education <- pred_edu_train_m
x.test_imp[ici(x.test_imp$education),]$education <- pred_edu_test_m


# Construct final imputed data sets

d.train.imp = x.train_imp
d.test.imp = x.test_imp

d.train.imp["TenYearCHD"] = y.train.miss
d.test.imp["TenYearCHD"] = y.test.miss


```

We have obtained completely imputed data sets. 
To see that our procedure has worked, we could consider the missing data patterns of the newly constructed data sets.

``` {r check missing data fixed, eval = F}

# For training data
md1 = md.pattern(x.train_imp)
gg_miss_var(x.train_imp)

# For test data
md2 = md.pattern(x.test_imp)
gg_miss_var(x.test_imp)


```

We have managed to impute the missing values, but there are several steps in this procedure that could be improved. 

First of all, we could have used more flexible models for imputation than linear imputation, and we could have fit several different models on the training set and done evaluation procedures within the training set (e.g. cross validated optimization of ROC-AUC) to pick the models before fixing a model to impute each variable. 

Some variables, such as 'cigsPerDay', are in some sense discrete, although we have treated them as continuous (as it is possible to smoke e.g. $2,73$ cigarettes per day). These could have been rounded off to integers, but we didn't see why this would be necessary. 
We did not include the response ('TenYearCHD') in the regressions. 
It is not clear to us why it would be a better choice to include it, as we don't want the imputed data to be overfitted towards the response.
More reading and testing would be needed to figure out the optimal solution, if there is any canonical choice, and it would be interesting to see how our results changed if the response were included.

For less code, the 'MICE' package could have been used, referring to the function 'mice' for imputation of the training set and 'mice.reuse' to reuse the imputation models on the test set. 


# Model

In the model section we will consider the two data sets; the complete case and imputed case. Both data sets are further divided into a train and test set. 
Since we want to do Lasso, we must standardize the data.
The problem with this is data leakage. If we want to standardize the test data, we should standardize it using the mean and the standard deviation of the training data. Most importantly, using the test data to scale the test data will introduce correlation between the independent observations of the test set.
Since the scaling information from the test set is "not available" to us at the time of training, we cannot expect the coefficients in the Lasso to be appropriately scaled compared to the test data.
We solve this by scaling the training data, and then using the attributes of the training data to scale the test data accordingly.


```{r complete train test}
# Make the training data ready for lasso by scaling.

set.seed(8701)

# Scale complete data for training

x.train.complete = scale(model.matrix(TenYearCHD ~ . -1, data = d.train.complete, family = binomial())) 
train.complete.mean = attr(x.train.complete, "scaled:center")
train.complete.sd = attr(x.train.complete, "scaled:scale")

y.train.complete = d.train.complete$TenYearCHD

# Imputed data training set scaling

x.train.imp = scale(model.matrix(TenYearCHD ~ . -1, data = d.train.imp, family = binomial()))
train.imp.mean = attr(x.train.imp, "scaled:center")
train.imp.sd = attr(x.train.imp, "scaled:scale")
y.train.imp = d.train.imp$TenYearCHD

# Same for imputed test data, but depending on training set attributes.
x.test.imp = scale(model.matrix(TenYearCHD ~ . -1, data = d.test.imp, family = binomial()), center = train.imp.mean, scale = train.imp.sd)

```

Given the binary response it is natural to consider fitting a logistic regression model to our data. Although we intend to use lasso, it is nice to start by fitting a regular logistic regression model on the complete case data to get an indication of which covariates that are most present, and for later comparison. 
We obtain a confusion matrix, a ROC-curve and the ROC-AUC.

```{r Logistic Model}
# Fit a logistic model
set.seed(8701)
mod0 <- glm(TenYearCHD ~ ., data = d.train.complete, family = binomial())
summary(mod0)

mod0_preds <- predict(mod0, newdata = d.test.complete, type = "response")

sum(round(mod0_preds) == 1)

predicted_value <- factor(round(mod0_preds))
expected_value <- factor(d.test.complete$TenYearCHD)

conf.mat.cc = confusionMatrix(data=predicted_value, reference = expected_value)$table
conf.mat.cc

roc_obj_cc <- roc(d.test.complete$TenYearCHD, mod0_preds, levels = c(0,1), direction = "<")
plot(roc_obj_cc)
auc(roc_obj_cc)
```

The logistic regression model chooses `r names(which(summary(mod0)$coefficients[,4] < 0.01))` as the significant covariates, where the p-value cutoff is 0.01. It classifies very few positives correctly, which is very problematic if the model would be used to predict hearth disease. 



## Lasso on Complete Case 

We continue to do the Lasso on the complete case data. To do this, we use cross-validation to find $\lambda_{min}$ and use the highest $\lambda$ with deviance within one standard deviation of $\lambda_{min}$.

```{r Lasso CV}
set.seed(8701)

# Use cross-validation to find lambda
cv.out = cv.glmnet(x.train.complete, y.train.complete, family = "binomial",intercept = F, standardize=TRUE, alpha = 1)
plot(cv.out$glmnet.fit, "lambda", label=F)
plot(cv.out)
cv.out$lambda.min
cv.out$lambda.1se

# Fit the Lasso model

lasso_mod_cc = glmnet(x.train.complete, y.train.complete, family = "binomial",intercept = F, standardize=TRUE, alpha = 1,  lasso = cv.out$lambda.1se)

lasso_coef_cc <- coef(lasso_mod_cc,s=cv.out$lambda.1se)
# lasso_coef_cc[,1]

# We predict to find the confusion matrix and the ROC-curve and the ROC-AUC.

x.test.complete = scale(model.matrix(TenYearCHD ~ . -1, data = d.test.complete, family = binomial()), center = train.complete.mean, scale = train.complete.sd)
lasso_preds_cc <- predict(lasso_mod_cc, newx = x.test.complete, type = "response",  s = cv.out$lambda.1se)

sum(round(lasso_preds_cc) == 1)

predicted_value_cc <- factor(round(lasso_preds_cc))
expected_value_cc <- factor(d.test.complete$TenYearCHD)

conf_mat_cc = confusionMatrix(data=predicted_value_cc, reference = expected_value_cc)$table
conf_mat_cc

roc_obj_lasso_cc <- roc(d.test.complete$TenYearCHD, lasso_preds_cc, levels = c(0,1), direction = "<")
plot(roc_obj_lasso_cc)
auc(roc_obj_lasso_cc)

sens_cc = sensitivity(conf_mat_cc)
spes_cc = specificity(conf_mat_cc)

```

The Lasso on the complete case data choosess `r names(which(lasso_coef_cc[,1] > 0))` as the significant covariates. 
Initially, this is the same lasso model is much better at classifying positives than the full logistic model and has only slightly worse AUC, at 'r auc(roc_obj_lasso_cc)' for the Lasso versus 'r auc(roc_obj_cc)' for the logistic model. We see that the Lasso includes 'prevalentHyp', while the logistic model finds it almost significant, with p-value 'r summary(mod0)$coefficients[11,4]'.
This agrees with the reasoning we made earlier.

To obtain a better understanding of these coefficients, we bootstrap from the training data to fit Lasso models and store their coefficients.

```{r Bootstrap Lasso, eval = F}
set.seed(8701)

B = 50
boot_size_cc = dim(d.train.complete)[1]

B_coef_cc = matrix(NA, nrow = B, ncol = length(lasso_coef_cc[,1]))
for (i in 1:B){
  data_b = sample(1:boot_size_cc, size = boot_size_cc, replace = TRUE)
  x = x.train.complete[data_b,]
  y = y.train.complete[data_b]
  cv.out = cv.glmnet(x, y, family = "binomial", alpha = 1)
  lasso_mod = glmnet(x, y, family = "binomial", alpha = 1, intercept = F, lasso = cv.out$lambda.1se)

  B_coef_cc[i,] <- coef(lasso_mod,s=cv.out$lambda.1se)[,1]
}

colnames(B_coef_cc) = names(coef(lasso_mod,s=cv.out$lambda.1se)[,1])
boxplot.matrix(B_coef_cc[,-1], ylim = c(-0.25,0.55))

B_coef_count_cc = ifelse(B_coef_cc == 0,0,1)
apply(B_coef_count_cc, 2, sum)
barplot(apply(B_coef_count_cc, 2, sum)/B, las = 2)
```
```{r, eval = F}

names_lasso_cc_70 <- names(apply(B_coef_count_cc, 2, sum)/B)[apply(B_coef_count_cc, 2, sum)/B > 0.7]
names_lasso_cc_50 <- names(apply(B_coef_count_cc, 2, sum)/B)[apply(B_coef_count_cc, 2, sum)/B > 0.5]
names_lasso_cc_70
names_lasso_cc_50
```
The variables that have nonzero coefficients in the Lasso models at least 70\% of the time, are 'r names_lasso_cc_70'.
Similarly, by those to those who are included at least $50$\% of the time, we obtain 'r names_lasso_cc_50'.
This is indeed similar to the ones we picked out earlier. 

## Lasso on Imputed Data

We now do the same thing, just using the imputed data instead of the complete case. 
Even though the imputed data includes more samples, the data quality is going down when we impute. 
We expect similar results, but we must wait and see.

To get hands-on experience with the 'MICE' package, we also construct an imputed data set using 'mice' and 'mice.reuse', for comparison. 
'mice' can be used on the entire training data, without needing to remove those samples with two or more covariates missing. 
The imputation model from the training set is emplyed to impute the test set as well, to avoid data leakage. 

```{r Imputation with mice, eval = F}
set.seed(8701)

# Important that we don't use the response as predictor when imputing
y.train <- d.train$TenYearCHD
y.test <- d.test$TenYearCHD

d.train["TenYearCHD"] <- NULL
d.test["TenYearCHD"] <- NULL

imp_train = mice(d.train, m=1)
d.train.imp.mice = complete(imp_train)

# Use the imputation model trained on the training set on the test set, to ensure no data leakage.
imp_test = mice.reuse(imp_train, d.test, printFlag = F)
d.test.imp.mice = imp_test$"1"

# We scale the model matrix as before.

x.train.imp.mice = scale(model.matrix(~ . -1, data = d.train.imp.mice, family = binomial()))
train.imp.mean.mice = attr(x.train.imp.mice, "scaled:center")
train.imp.sd.mice = attr(x.train.imp.mice, "scaled:scale")
y.train.imp.mice = d.train.imp$TenYearCHD

# Same for test data, but depending on attributes from the training data in order to avoid data leakage.
x.test.imp.mice = scale(model.matrix(~ . -1, data = d.test.imp.mice, family = binomial()), center = train.imp.mean.mice, scale = train.imp.sd.mice)

```

First we try the Lasso on the imputed dataset where we imputed with our manual technique.

```{r Manually Imputed Lasso model}
# We need to standardize the matrix so that we can drop intercepts.
cv.out = cv.glmnet(x.train.imp, y.train.imp, family = "binomial", intercept = F, standardize=TRUE, alpha = 1)
plot(cv.out$glmnet.fit, "lambda", label=F)
plot(cv.out)
cv.out$lambda.min
cv.out$lambda.1se

lasso_mod_imp = glmnet(x.train.imp, y.train.imp, family = "binomial", alpha = 1, intercept = F, standardize=TRUE,  lasso = cv.out$lambda.1se)

lasso_coef_imp <- coef(lasso_mod,s=cv.out$lambda.1se)
lasso_coef_imp[,1]

lasso_preds_imp <- predict(lasso_mod_imp, newx = x.test.imp, type = "response",  s = cv.out$lambda.1se)

sum(round(lasso_preds_imp) == 1)

predicted_value_imp <- factor(round(lasso_preds_imp))
expected_value_imp <- factor(d.test.imp$TenYearCHD)

conf_mat_imp = confusionMatrix(data=predicted_value_imp, reference = expected_value_imp)$table
cont_mat_imp

roc_obj_imp <- roc(d.test.imp$TenYearCHD, lasso_preds, levels = c(0,1), direction = "<")
plot(roc_obj_imp)
auc(roc_obj_imp)

sens_imp = sensitivity(conf_mat_imp)
spes_imp = specificity(conf_mat_imp)

```

The performance of this model is quite similar to that of the complete case, which is to be expected.
The Lasso on the manually imputed data chooses `r names(which(lasso_coef_imp[,1] > 0))` as the significant covariates. 

Recall that the sensitivity can be measured by the true positive rate (i.e. the number of true positives over all positives) and the specificity can be measured by the true negative rate (i.e. true negative over all negative).
Comparing the Lasso on the complete case data with the imputed data, we note that we have sensitivity 'r sens_cc' in the complete case and sensitivity 'r sens_imp' in the imputed case.
The specificity of the complete case Lasso is 'r spes_cc', while for the imputed case it is 'r spes_imp'.

Although the difference is not huge, it may resemble that the data quality in the imputed data is slightly lower, although for selecting covariates, we obtained the same answer.
In our data rich situation, this is neither clear enough, nor a problem, but for data poor situations, this is something to keep in mind.

We try to do the same thing, using the 'MICE'-imputed data set. 

```{r Mice Imputed Lasso model, eval = F}
set.seed(8701)
# Cross-validation
cv.out = cv.glmnet(x.train.imp.mice, y.train, family = "binomial", intercept = F, standardize=TRUE, alpha = 1)
plot(cv.out$glmnet.fit, "lambda", label=F)
plot(cv.out)
cv.out$lambda.min
cv.out$lambda.1se


lasso_mod_mice = glmnet(x.train.imp.mice, y.train, family = "binomial", alpha = 1, intercept = F, standardize=TRUE, lasso = cv.out$lambda.1se)

lasso_coef_mice <- coef(lasso_mod_mice,s=cv.out$lambda.1se)
#lasso_coef_mice[,1]

lasso_preds_mice <- predict(lasso_mod_mice, newx = x.test.imp.mice, type = "response", s = cv.out$lambda.1se)

predicted_value_mice <- factor(round(lasso_preds_mice))
expected_value_mice <- factor(y.test)

conf_mat_mice = confusionMatrix(data=predicted_value_mice, reference = expected_value_mice)$table
conf_mat_mice

roc_obj_mice <- roc(y.test, lasso_preds_mice, levels = c(0,1), direction = "<")
plot(roc_obj_mice)
auc(roc_obj_mice)

sens_mice = sensitivity(conf_mat_mice)
spes_mice = specificity(conf_mat_mice)
```

The predictive performance has not changed a lot, in the eyes of AUC. 

The performance of this model is quite similar to that of the complete case, which is to be expected.
The Lasso on the MICE-imputed data chooses `r names(which(lasso_coef_mice[,1] > 0))` as the significant covariates. 

We suspect that the sensitivity and specificity is even lower for the MICE-imputed data, as it also imputes those with more than one missing value.

Comparing the Lasso on the complete case data with the imputed data, we note that we have sensitivity 'r sens_imp' in the manually imputed case and 'r sens_mice'.
The specificity of the manually imputed case Lasso is 'r spes_imp', while for the mice case it is 'r spes_mice'.

Although the difference is not huge, it may look like our suspicions were wrong. 
The 'mice' function performs quite good imputation, but it is interesting that our manual imputation procedure manages almost as well in this case.


We



### TO BE YEETED? ---------------- START ----------------

This may indicate that the base imputation methods in 'mice' actually overfits to the training data.
Our manually imputed data set had a nuanced (although simple) take on which methods to use, such as linear models, log.reg. and kNN.

We check with methods 'mice' is using, in hope of finding the problem and improving the imputation procedure. 

### Specifying imputation methods

```{r Check what mice already uses, eval = F}
init = mice(data, maxit = 0)
meth = init$method
meth <- meth[-16] #Remove response
meth
```

It turns out that 'mice' uses predictive mean matching (pmm) for all variables, except 'education', which has multiple classes.
For 'education' it uses multiple logistic regression.
We first check which type each variable is. 

```{r Check data type, eval = F}
types = sapply(data, class)
types
```
Then we count the number of unique values each variable has, including NA's as its own level.

```{r Check number of unique, eval = F}
n_unique = sapply(data, function(x) length(unique(x)))
n_unique
```
From this, we may understand, that in the eyes of 'mice', 'BPMeds' actually has three levels, when it in reality has two. 
Therefore, we specify the methods we want mice to use before imputing again. 
Since 'BPMeds' is binary, we use logistic regression to classify, we continue to use 'polyreg' for 'education' (to compare with our manual imputation, where we used kNN), and we fit linear imputation models for the other variables.

```{r Specify new imputation methods, eval = F}
meth["BPMeds"] <- "logreg"
meth["totChol"] <- "norm.predict"
meth["BMI"] <- "norm.predict"
meth["heartRate"] <- "norm.predict"
meth["glucose"] <- "norm.predict"
meth["cigsPerDay"] <- "norm.predict"
meth
```

We try again to impute using 'mice', but with our newly specified imputation methods. 


```{r new mice imputation, eval = F}

set.seed(8701)
imp_train.mice.new = mice(d.train, m=1, method = meth)
d.train.imp.mice.new = complete(imp_train.mice.new)

imp_test.mice.new = mice.reuse(imp_train.mice.new, d.test)
d.test.imp.mice.new = imp_test.mice.new$"1"

x.train.imp.mice.new <- model.matrix(~.-1, data = d.train.imp.mice.new)
x.test.imp.mice.new <- model.matrix(~.-1, data = d.test.imp.mice.new)

# We scale the model matrix as before.

x.train.imp.mice.new= scale(model.matrix(~ . -1, data = d.train.imp.mice.new, family = binomial()))
train.imp.mean.mice.new = attr(x.train.imp.mice.new, "scaled:center")
train.imp.sd.mice.new = attr(x.train.imp.mice.new, "scaled:scale")
y.train.imp.mice.new = d.train.imp$TenYearCHD

# Same for test data, but depending on train imp
x.test.imp.mice.new = scale(model.matrix(~ . -1, data = d.test.imp.mice.new, family = binomial()), center = train.imp.mean.mice.new, scale = train.imp.sd.mice.new)

##### FIX DATA LEAKAGE COMMENT ON WHY WE SCALE TEST WEIRDLHY

```

The newly imputed data set found by specified methods in 'mice' is then used to fit a Lasso. 

```{r Lasso Imputed with new methods for mice, eval = F}
set.seed(8701)

# Use cross-validation to find lambda
cv.out = cv.glmnet(x.train.imp.mice.new, y.train, family = "binomial", intercept = F, standardize=TRUE, alpha = 1)
plot(cv.out$glmnet.fit, "lambda", label=F)
plot(cv.out)
cv.out$lambda.min
cv.out$lambda.1se



lasso_mod_imp_s = glmnet(x.train.imp.mice.new, y.train, family = "binomial", intercept = F, standardize=TRUE, alpha = 1,  lasso = cv.out$lambda.1se)

lasso_imp_coef <- coef(lasso_mod_imp_s, s=cv.out$lambda.1se)
lasso_imp_coef[,1]




lasso_imp_preds <- predict(lasso_mod_imp_s, newx = x.test.imp.mice.new, type = "response",  s = cv.out$lambda.1se)

predicted_value <- factor(round(lasso_imp_preds))
expected_value <- factor(y.test)

confusionMatrix(data=predicted_value, reference = expected_value)$table

roc_obj_lasso <- roc(y.test, lasso_imp_preds)
auc(roc_obj_lasso)
```


Surprisingly enough, we obtain a slightly better AUC, but without changing the number of true positives significantly. 
This is still terrible, as we would at least want to have a low amount of false negatives. 
Using different imputation methods seems to have some effect on the prediction, but the difference might be due to random effects, and it is not clear to us why 'mice' yields so bad results. 
We strongly suspect that 'mice' introduces extra correlation by running chained imputations.
That is, where our manual approach used the same complete data to predict each imputation value, 'mice' probably imputes one covariate, and then uses the newly imputed data set to impute further covariates. 
Since the Lasso is not that good at handling high correlation, this chained imputation by 'mice' may cause most of the problems.
### TO BE YEETED? ---------------- end ----------------


### Bootstrapping

To obtain improved estimates for the coefficients in the Lasso, we use bootstrapping. 
The data set used will be the MICE-imputed data set.


```{r Bootstrap Lasso with imputed data, eval = F}
set.seed(8701)
B = 50
size = dim(x.train.imp.mice)[1]

B_coef_mice = matrix(NA, nrow = B, ncol = length(lasso_coef[,1]))
for (i in 1:B){
  boot_ind = sample(1:size, size = size, replace = TRUE)
  
  x = x.train.imp.mice[boot_ind,]
  y = y.train[boot_ind]
  
  cv.out = cv.glmnet(x, y, family = "binomial", intercept = F, alpha = 1)
  lasso_mod = glmnet(x, y, family = "binomial", intercept = F, alpha = 1, lasso = cv.out$lambda.1se)

  B_coef_mice[i,] <- coef(lasso_mod,s=cv.out$lambda.1se)[,1]
}

colnames(B_coef_mice) = names(coef(lasso_mod,s=cv.out$lambda.1se)[,1])
boxplot.matrix(B_coef_mice[,-1], ylim = c(-0.4,0.5), las = 2)
#, ylim = c(-0.02,0.02)

B_coef_count_mice = ifelse(B_coef_mice == 0,0,1)
barplot(apply(B_coef_count_mice, 2, sum)/B, las = 2)
```

```{r, eval = F}

names_lasso_mice_70 <- names(apply(B_coef_count_mice, 2, sum)/B)[apply(B_coef_count_mice, 2, sum)/B > 0.7]
names_lasso_mice_50 <- names(apply(B_coef_count_mice, 2, sum)/B)[apply(B_coef_count_mice, 2, sum)/B > 0.5]
names_lasso_mice_70
names_lasso_mice_50
```
The variables that have nonzero coefficients in the Lasso models at least 70\% of the time, are 'r names_lasso_mice_70'.
Similarly, by those to those who are included at least $50$\% of the time, we obtain 'r names_lasso_mice_50'.
This is indeed similar to the ones we picked out earlier. 

# Inference

In order to do inference we simply fit a logistic regression model using the `glm` function in `R`, and extract the inference from there. However, we will keep the coefficients chosen by the lasso-bootstrapping iterations in the earlier models, and now use the test data to fit a logistic model to avoid overfitting. Note that we have only used the test data to predict and observe different measures, such as ROC-AUC, sensitivity/specificity, and so on. 
The test data is therefore suitable for inference, as it has not been perturbed in the procedure of fitting the models.

We start out with the complete case models and fit a logistic model with the most important variables, namely 'r names_lasso_cc_70'.

```{r complete case comparison, eval = F}
# Fit the model on the complete case training data.

mod <- glm(TenYearCHD ~ age + male + sysBP + glucose, data = d.complete[-train,], family = binomial())

# Confidence intervals.

CI_mod = confint(mod)

CI_mod0 = confint(mod0)

# Look at the coefficients

summary(mod)$coefficients

# Compare confidence intervals
CI_mod0
CI_mod

```
We can observe that all the coefficients in the new model are significant!
Comparing confidence intervals for the naive model and the new model with Lasso-selected variables, we see that the confidence intervals are shifted more towards zero, and some of them has even become slightly smaller. For example, the confidence interval of 'sysBP' went from 'r CI_mod0[14,2]-CI_mod0[14,1]' to 'r CI_mod[4,2]-CI_mod[4,1]' after the subset selection.

```{r, echo = F, eval = F}

CI_mod[4,2]-CI_mod[4,1]
CI_mod0[14,2]-CI_mod0[14,1]

CI_mod0[14,2]-CI_mod0[14,1] - (CI_mod[4,2]-CI_mod[4,1])

```

Similarly, we may fit a logistic model on the imputed data.
We include the variables 'r names_lasso_mice_70' that was nonzero more than 70\% of the times in the bootstrap.

```{r Inference for imputed data, eval = F}
# Add the response values that we took out for the mice data.

d.test.imp.mice["TenYearCHD"] <- y.test

# Fit the model.
mod <- glm(TenYearCHD ~ age + male + sysBP + cigsPerDay + glucose, data = d.test.imp.mice, family = binomial())

# Consider confidence interval and coefficients
CI_mice = confint(mod)
CI_mice

summary(mod)$coefficients
```
Again, we can observe that all the coefficients in the new model are significant!

We take a brief look at the complete case model with Lasso-selected variables to the model on the imputed data with Lasso-selected variables.

```{r comparison, echo = F}
CI_mod
CI_mice
```
We can see, for example by considering 'sysBP', see that the width of the interval has gone further down by working on the imputed data, as the confidence interval width for 'sysBP' was 'r CI_mod[3,2] - CI_mod[3,1]' for the complete case model and 'r CI_mice[3,2] - CI_mice[3,1]'.
This may simply be because we use more data to fit the model, but it may also be a nonsensical question, as we are in essence fitting two different models. 
The inclusion of 'cigsPerDay' in the model on imputed data is probably a key reason why we see such a difference. 
More or less, we obtain the same results, as it is the same covariates that come back time and time again.

# Discussion

When we set out, our goal was to explore imputation techniques and learn about 'MICE', single imputation, Lasso and bootstrapping with Lasso. 
We first fit a logistic regression model on the complete case training set. 

