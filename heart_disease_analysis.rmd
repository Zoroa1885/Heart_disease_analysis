---
subtitle: "MA8701 Advanced Statistical Learning V2023"
title: "Compulsory exercise: Team Supergreat"
author: "Nora Aasen, Elias Angelsen, Jonas Nordstrom"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  # html_document
  pdf_document
---
  
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = FALSE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",
                      fig.width=6, fig.height=4.5,fig.align = "center")
```


```{r Loading Packages,eval=TRUE,echo=FALSE}
# Load libraries
library(naniar)
library(mice)
library(glmnet)
library(tidyr)
library(dplyr)
library(caret)
library(ggcorrplot)
library(ggplot2)
library(e1071) #If we use KNN for imputation of education (multiclass)
library(pROC)
library(caret)
library(NADIA)
```

# Introduction

In this project we have studied the [Framingham Coronary Heart Disease Dataset](https://www.kaggle.com/datasets/dileep070/heart-disease-prediction-using-logistic-regression?fbclid=IwAR1LE3P3vM1SyHBifotrNdXoKGv7szGR07labEAQo6XUqV9Pi90vtAp4mS4). This dataset contains patient information for inhabitants in Framingham, Massachusetts, and is typically used to predict the chance of getting coronary heart disease (CHD) within the next 10 years. For this project, however, we intend to use lasso to find the most important risk factors.


We start by examining the dataset. 


## Exploratory Analysis
```{r Loading Data}
# Load and look at data
# data <- framingham # Just if manual import
data <- read.csv("framingham.csv")
data_dim = dim(data)
pos_response = sum(data$TenYearCHD==1)
str(data)


ggplot(gather(data), aes(value)) + 
    geom_histogram(bins = 16) + 
    facet_wrap(~key, scales = 'free_x')

# Code education as a factor variable instead
data$education = factor(data$education, labels = c("none","hs","college","post-grad"))

```

This data set contains `r data_dim[1]` observations, `r data_dim[2]-1` covariates and a binary response variable `TenYearCHD`, thus we will try to fit a logistic regression model. The response variable has `r pos_response` observations that are 1, which equals about `r round(pos_response/data_dim[1]*100,1)`\% of the total observations. Most of our covariates are either binary, or numeric. However, we notice that the variable education is most likely a categorical covariate. We could not find any further elaboration for which four categories the numbers represent, so based on the frequency of each value and qualified guessing, we changed it to a factor variable and defined the four categories as `r names(summary(data$education))[1:4]`.

The next thing we looked at was the number of missing data in our data set.

```{r plot missing data}
# Look at the missing data
md_mat = md.pattern(data, rotate.names = T, plot = T)
gg_miss_var(data)
```

As we can see there are six covariates that has missing data: `glucose`, `education`, `BPMeds`, `totChol`, `cigsPerDay`, and `BMI`. We cannot use the rows that contain missing values as is. The easiest solution is to remove all rows that contains `NA`'s. This is the \textit{complete case} solution.  

```{r train-test split}
# Split into training and test first
set.seed(8701)
tr = 7/10 # train ratio
r = dim(data)[1]
size = round(r*tr)
train = sample(1:r,size = size)

d.train = data[train,]
d.test = data[-train,]
```

```{r complete case}
# Make a dataset containing only the complete cases
d.complete <- data[complete.cases(data), ]
d.train.complete <- d.train[complete.cases(d.train), ]
d.test.complete <- d.test[complete.cases(d.test), ]

pos_response_c = sum(data[complete.cases(data),]$TenYearCHD==1)
```

The complete data set contains `r sum(complete.cases(data))` observations and the response variable has `r pos_response_c` observations that are 1, which equals about `r round(pos_response_c/sum(complete.cases(data))*100,1)`\% of the total observations. As we can see, the proportion of positive observations in the response is the same, which is a good indicator that our data is missing at random (MAR), and it is therefore possible to do imputation.

From the exploratory analysis we see that there are many missing data points. However, the complete case data set is also quite large. Our main focus for this project will therefore be to compare the results from doing lasso on the complete case with the results from doing lasso on an imputed data set.


```{r exp analysis, eval = F}
# Is it of interest to consider a correlation plot in this case? 

# Make a correlation plot 
cor_mat = cor(subset(d.complete, select = -education)) # Cannot compute correlation with factor variables
ggcorrplot(cor_mat, hc.order = TRUE, type = "lower",
   outline.col = "white",
   colors = c("blue", "white", "red"))
```


# Missing Data

In our data set, we have missing values, and we are additionally going to handle these missing values in a slightly more refined manner than just considering the complete case.

There are several types of mechanisms for missing data.
Let $Z = (X,y)$ denote the full collection of covariates and responses, respectively, and we let a subscript mis/obs indicate whether we are restricting $Z$ (or $X$) to the missing or observed parts, respectively.
We may form an indicator (0-1) matrix $R$ indicating missing (0) covariates and observed (1) covariates. 
Assume $\psi$ is short for the parameters in the distribution of $R$.

The missing data may be characterized by the conditioning in the distribution of $R$.
We define the data to be:
\begin{itemize}
  \item missing completely at random (MCAR) if $P(R | Z, \psi) = P(R | \psi)$,
  \item missing at random (MAR) if $P(R | Z, \psi) = P(R | Z_{obs}, \psi)$,
  \item missing not at random (MNAR) if $P(R | Z, \psi) = P(R | Z, \psi)$ (we don't have MCAR or MAR).
\end{itemize}

By exploring for example the missing pattern of the variable 'cigsPerDay', we obtain a clear indication that our missing mechanism is not MCAR.
No non-smoker has failed to answer the question "how may cigarettes do you smoke a day?", which is a question only aimed at smokers. 
The simple explanation may be that the survey they answered automatically fills in $0$ for 'cigsPerDay' if you claim to be a non-smoker. 
In more mathematical terms, 'cigsPerDay' depends on the observed answer to "do you smoke?" (found in variable 'currentSmoker'), indicating that we do not work with MCAR data.
Luckily, most methods are applicable if our missingness is at least MAR. 

We will assume that the missing mechanism is MAR, as there is no clear reason to suspect it to be MNAR. 
A pressing obstruction to being MAR can again be found in 'cigsPerDay'. In the real world, if smokers have failed to report 'cigsPerDay', it may for example be because they smoke so much that they are ashamed to answer the question (and skips it), but we will simply assume that such a thing is not happening, as we trust people to answer truthfully (often, at least) if they volunteer for medical studies. 

To treat the missing data, we will use single imputation, as multiple imputation may cause difficulties with the resulting inference, as Rubins rules need to be combined with the Lasso, bootstrap and concluding inference. 

The single imputation technique we will use is simply regression imputation, where we adapt our regression technique depending on the type of variable imputed. For continuous variables, we simply use a linear regression model. For binary variables, we use logistic regression to classify their values, and for the variable "education", which is a four-class variable, we have utilized kNN for multiclass imputation. 
This is implemented manually, but this could have been done using the 'MICE' package and the function 'mice'.

To avoid encountering observations with more than one missing value, and hence problems with regressing, we remove all samples with more than one NA. 

Our data is split into training and test sets, with the test-to-training ratio being 'r tr' of the original dataset.
In order to avoid data leakage in our imputation of the test set, we fit the imputation models on the training set.
The main idea is that the test set should be viewed as several independent observations. Using the test set to impute itself will use information not present at the time of training and will yield unintended correlation.
This is again done manually, but could also have been done using 'mice.reuse'.

Note that we do not include the response (TenYearCHD) in the regression, and in order to avoid too much correlation between the imputed samples, we always base the regression models on the complete case data, instead of letting the imputed values for variable $n$ regress to impute variable $n+1$.  

First, we remove all samples with more than one covariate missing, as this is only $61$ samples. 

```{r Remove rows with more than 1 NA}

# Setting seed expressing our love for MA8701
set.seed(8701)

# Throwing out the samples with two or more NAs from the data
miss <- c()
for(i in 1:nrow(d.train)) {
  if(sum(is.na(d.train[i,])) > 1){
    miss <- append(miss,i)
    }  
}

d.train.missing <- d.train[-miss,]


miss <- c()
for(i in 1:nrow(d.test)) {
  if(sum(is.na(d.test[i,])) > 1){
    miss <- append(miss,i)
    }  
}

d.test.missing <- d.test[-miss,]

# Check if we have removed all rows with double NA

# md.pattern(d.train.missing, rotate.names = T)
# md.pattern(d.test.missing, rotate.names = T)


```


We further split the data into further training and test sets with and without the response 'TenYearCHD', following the training-test-ratio that we have previously set.

``` {r Missing data split}
# Making training and test data for covariates and response separately.
# X.train.miss = subset(d.train.missing, select = -TenYearCHD)
# y.train.miss = d.train$TenYearCHD

# X.test.miss = subset(d.test.missing, select = -TenYearCHD)
# y.test.miss = d.test$TenYearCHD

x.train_dat_m = subset(d.train.missing, select = -TenYearCHD)
y.train.miss = d.train.missing$TenYearCHD

x.test_dat_m = subset(d.test.missing, select = -TenYearCHD)
y.test.miss = d.test.missing$TenYearCHD
```

Using these data sets, we make regression models for each missing variable based on the data we have. 
These models automatically neglects NA's. For 'glucose', 'cigsPerDay', 'BMI', 'totChol' and 'heartRate', we fit linear models, while for the binary variable 'BPMeds', a logistic model is fit. The multi-class variable 'education' will be imputed using a kNN-model. 

``` {r Missing data regression models}

# We fit linear models on the training set for glucose, cigsPerDay, BMI, totChol, heartRate.
fit_for_simp_glucose_m <- lm(glucose ~., data = x.train_dat_m)    # Linear model for glucose
fit_for_simp_cigs_m <- lm(cigsPerDay ~., data = x.train_dat_m)    # Linear model for cigsPerDay
fit_for_simp_BMI_m <- lm(BMI ~., data = x.train_dat_m)            # Linear model for BMI
fit_for_simp_totChol_m <- lm(totChol ~., data = x.train_dat_m)    # Linear model for totChol
fit_for_simp_heartRate_m <- lm(heartRate ~., data = x.train_dat_m)# Linear model for heartRate 

# We fit a logistic model on the training set for BPMeds, being a binary variable.
fit_for_simp_BPMeds_m <- glm(BPMeds ~., data = x.train_dat_m, family = "binomial") #Logistic model for BPMeds

# We fit a KNN model on the training set for education, being a multi-class variable.
fit_for_simp_edu_m <- gknn(education ~., data = x.train_dat_m) # kNN model for education

```

To predict the missing values for each variable, we pick out those samples with missing values of each variable from the training and test set.
Note that we are imputing for both the training and test set, even though the sampled (therefore "random") split into training and test set may have sorted all NA's of a variable (e.g. heartRate) into a single set (i.e. into either training or test set).
This is not a problem, as we are only picking out the incomplete cases. Therefore, predicting and filling in the missing values in the complete part is a vacuous procedure, not yielding any problems.

``` {r Missing data which-to-predict}

# Pick out those variables with missing glucose from the training and test set.
train_data_to_pred_gluc_m = ic(x.train_dat_m)
train_data_to_pred_gluc_m = train_data_to_pred_gluc_m[ici(train_data_to_pred_gluc_m$glucose),] 
test_data_to_pred_gluc_m = ic(x.test_dat_m)
test_data_to_pred_gluc_m = test_data_to_pred_gluc_m[ici(test_data_to_pred_gluc_m$glucose),]

# Pick out those variables with missing cigsPerDay from the training and test set.
train_data_to_pred_cigs_m = ic(x.train_dat_m)
train_data_to_pred_cigs_m = train_data_to_pred_cigs_m[ici(train_data_to_pred_cigs_m$cigsPerDay),]
test_data_to_pred_cigs_m = ic(x.test_dat_m)
test_data_to_pred_cigs_m = test_data_to_pred_cigs_m[ici(test_data_to_pred_cigs_m$cigsPerDay),]

# Pick out those variables with missing BMI from the training and test set.
train_data_to_pred_BMI_m = ic(x.train_dat_m)
train_data_to_pred_BMI_m = train_data_to_pred_BMI_m[ici(train_data_to_pred_BMI_m$BMI),]
test_data_to_pred_BMI_m = ic(x.test_dat_m)
test_data_to_pred_BMI_m = test_data_to_pred_BMI_m[ici(test_data_to_pred_BMI_m$BMI),]

# Pick out those variables with missing totChol from the training and test set.
train_data_to_pred_totChol_m = ic(x.train_dat_m)
train_data_to_pred_totChol_m = train_data_to_pred_totChol_m[ici(train_data_to_pred_totChol_m$totChol),]
test_data_to_pred_totChol_m = ic(x.test_dat_m)
test_data_to_pred_totChol_m = test_data_to_pred_totChol_m[ici(test_data_to_pred_totChol_m$totChol),]

# Pick out those variables with missing heartRate from the training and test set.
train_data_to_pred_heartRate_m = ic(x.train_dat_m)
train_data_to_pred_heartRate_m = train_data_to_pred_heartRate_m[ici(train_data_to_pred_heartRate_m$heartRate),]
test_data_to_pred_heartRate_m = ic(x.test_dat_m)
test_data_to_pred_heartRate_m = test_data_to_pred_heartRate_m[ici(test_data_to_pred_heartRate_m$heartRate),]

# Pick out those variables with missing BPMeds from the training and test set.
train_data_to_pred_BPMeds_m = ic(x.train_dat_m)
train_data_to_pred_BPMeds_m = train_data_to_pred_BPMeds_m[ici(train_data_to_pred_BPMeds_m$BPMeds),]
test_data_to_pred_BPMeds_m = ic(x.test_dat_m)
test_data_to_pred_BPMeds_m = test_data_to_pred_BPMeds_m[ici(test_data_to_pred_BPMeds_m$BPMeds),]

# Pick out those variables with missing education from the training and test set.
train_data_to_pred_edu_m = ic(x.train_dat_m)
train_data_to_pred_edu_m = train_data_to_pred_edu_m[ici(train_data_to_pred_edu_m$education),]
test_data_to_pred_edu_m = ic(x.test_dat_m)
test_data_to_pred_edu_m = test_data_to_pred_edu_m[ici(test_data_to_pred_edu_m$education),]

```

We then predict the missing values for imputation. For the logistic model fit to predict 'BPMeds', we assign a prediction class $1$ or $0$ depending on whether the predicted value is above or below $0.5$.

``` {r Missing Data predictions}

# Predicting the values to impute for glucose on the training and test data.
pred_glucose_train_m <- predict(fit_for_simp_glucose_m, newdata = train_data_to_pred_gluc_m) 
pred_glucose_test_m <- predict(fit_for_simp_glucose_m, newdata = test_data_to_pred_gluc_m)

# Predicting the values to impute for cigsPerDay on the training and test data.
pred_cigs_train_m <- predict(fit_for_simp_cigs_m, newdata = train_data_to_pred_cigs_m) 
pred_cigs_test_m <- predict(fit_for_simp_cigs_m, newdata = test_data_to_pred_cigs_m)

# Predicting the values to impute for BMI on the training and test data.
pred_BMI_train_m <- predict(fit_for_simp_BMI_m, newdata = train_data_to_pred_BMI_m) 
pred_BMI_test_m <- predict(fit_for_simp_BMI_m, newdata = test_data_to_pred_BMI_m)

# Predicting the values to impute for totChol on the training and test data.
pred_totChol_train_m <- predict(fit_for_simp_totChol_m, newdata = train_data_to_pred_totChol_m) 
pred_totChol_test_m <- predict(fit_for_simp_totChol_m, newdata = test_data_to_pred_totChol_m)

# Predicting the values to impute for heartRate on the training and test data.
pred_heartRate_train_m <- predict(fit_for_simp_heartRate_m, newdata = train_data_to_pred_heartRate_m) 
pred_heartRate_test_m <- predict(fit_for_simp_heartRate_m, newdata = test_data_to_pred_heartRate_m)

# Predicting the classes to impute for BPMeds on the training and test data by first predicting numerical values in (0,1) and then classifying these to either 0 or 1. 
pred_BPMeds_train_m_probs <- predict(fit_for_simp_BPMeds_m, newdata = train_data_to_pred_BPMeds_m, type = "response") 
pred_BPMeds_test_m_probs <- predict(fit_for_simp_BPMeds_m, newdata = test_data_to_pred_BPMeds_m, type = "response")
pred_BPMeds_train_m <- ifelse(pred_BPMeds_train_m_probs >= 0.5, 1, 0)
pred_BPMeds_test_m <- ifelse(pred_BPMeds_test_m_probs >= 0.5, 1, 0)

# Predicting the classes to impute for education on the training and test data.
pred_edu_train_m <- predict(fit_for_simp_edu_m, newdata = train_data_to_pred_edu_m, type = "class") 
pred_edu_test_m <- predict(fit_for_simp_edu_m, newdata = test_data_to_pred_edu_m, type = "class")

```

To see how our predictions compare to the complete case, we plot (in different ways) the predicted values/classes.
For illustrating different types of plots useful for this, we plot glucose twice, as both a transparent histogram and a point plot. 
This is only for illustrational purposes and therefore only done for the training data with its imputed values. 

```{r Missing data train-pred-plots}

# ---- Make transparent colors for plotting ----
c1 <- rgb(173,216,230,max = 255, alpha = 80, names = "lt.blue")
c2 <- rgb(255,192,203, max = 255, alpha = 80, names = "lt.pink")

# ---- Make histogram objects for glucose ----

# Histogram objects for glucose
hg_pred_gluc = hist(pred_glucose_train_m, plot = FALSE)
hg_old_gluc = hist(x.train_dat_m$glucose, plot = FALSE)


# ---- Make point plots over indexes for all except education ----

par(mfrow = c(2,3))

# Point plot for glucose
plot(x.train_dat_m$glucose, ylab = "Value of Glucose", main = "Glucose")
points(seq(1,length(x.train_dat_m[,1]),length.out = length(pred_glucose_train_m)),pred_glucose_train_m, col = "orange", pch = 16)
abline(h = mean(x.train_dat_m$glucose), col = "red")

# Point plot for cigsperday
plot(x.train_dat_m$cigs, ylab = "Value of cigsPerDay", main = "cigsPerDay")
points(seq(1,length(x.train_dat_m[,1]),length.out = length(pred_cigs_train_m)),pred_cigs_train_m, col = "orange", pch = 16)
abline(h = mean(x.train_dat_m$cigsPerDay), col = "red")

# Point plot for BMI
plot(x.train_dat_m$BMI, ylab = "Value of BMI", main = "BMI")
points(seq(1,length(x.train_dat_m[,1]),length.out = length(pred_BMI_train_m)),pred_BMI_train_m, col = "orange", pch = 16)
abline(h = mean(x.train_dat_m$BMI), col = "red")

# Point plot for totChol
plot(x.train_dat_m$totChol, ylab = "Value of totChol", main = "totChol")
points(seq(1,length(x.train_dat_m[,1]),length.out = length(pred_totChol_train_m)),pred_totChol_train_m, col = "orange", pch = 16)
abline(h = mean(x.train_dat_m$totChol), col = "red")

# Point plot for heartRate
plot(x.train_dat_m$heartRate, ylab = "Value of heartRate", main = "heartRate")
points(seq(1,length(x.train_dat_m[,1]),length.out = length(pred_heartRate_train_m)),pred_heartRate_train_m, col = "orange", pch = 16)
abline(h = mean(x.train_dat_m$heartRate), col = "red")

# Point plot for BPMeds
plot(x.train_dat_m$BPMeds, ylab = "Class of BPMeds", main = "BPMeds")
points(seq(1,length(x.train_dat_m[,1]),length.out = length(pred_BPMeds_train_m)),pred_BPMeds_train_m, col = "orange", pch = 16)
abline(h = mean(x.train_dat_m$BPMeds), col = "red")

# ---- Make histogram plots for glucose and education ----

par(mfrow = c(1,1))

# Histogram plots for glucose
plot(hg_old_gluc, col = c1, xlab = "Value of Glucose", main = "Glucose") # Plot 1st histogram using a transparent color
plot(hg_pred_gluc, , col = c2, add = TRUE)
legend("topright", legend=c("Observed - blue", "Predictions - pink"), cex=0.6)

par(mfrow = c(1,2))

# Histograms for education
plot(x.train_dat_m$education, main = "Education - Observed", ylab = "Frequency")
plot(pred_edu_train_m, main = "Education - Predicted", ylab = "Frequency")

```

Lastly, we update our data with the newly imputed values.

``` {r Missing Data imputation}

# ---- Making new data sets based on the old ones ----
x.train_imp = x.train_dat_m
x.test_imp = x.test_dat_m

# ---- We add the predictions to this data set. ----

# Adding glucose
x.train_imp[ici(x.train_imp$glucose),]$glucose <- pred_glucose_train_m
x.test_imp[ici(x.test_imp$glucose),]$glucose <- pred_glucose_test_m

# Adding cigsPerDay
x.train_imp[ici(x.train_imp$cigsPerDay),]$cigsPerDay <- pred_cigs_train_m
x.test_imp[ici(x.test_imp$cigsPerDay),]$cigsPerDay <- pred_cigs_test_m

# Adding BMI
x.train_imp[ici(x.train_imp$BMI),]$BMI <- pred_BMI_train_m
x.test_imp[ici(x.test_imp$BMI),]$BMI <- pred_BMI_test_m

# Adding totChol
x.train_imp[ici(x.train_imp$totChol),]$totChol <- pred_totChol_train_m
x.test_imp[ici(x.test_imp$totChol),]$totChol <- pred_totChol_test_m

# Adding heartRate
x.train_imp[ici(x.train_imp$heartRate),]$heartRate <- pred_heartRate_train_m
x.test_imp[ici(x.test_imp$heartRate),]$heartRate <- pred_heartRate_test_m

# Adding BPMeds
x.train_imp[ici(x.train_imp$BPMeds),]$BPMeds <- pred_BPMeds_train_m
x.test_imp[ici(x.test_imp$BPMeds),]$BPMeds <- pred_BPMeds_test_m

# Adding education
x.train_imp[ici(x.train_imp$education),]$education <- pred_edu_train_m
x.test_imp[ici(x.test_imp$education),]$education <- pred_edu_test_m

```

We should by now have obtained completely imputed data sets. 
To see that our procedure has worked, we could consider the missing data patterns of the newly constructed data sets.

``` {r check missing data fixed, eval = F}

# For training data
md1 = md.pattern(x.train_imp)
gg_miss_var(x.train_imp)

# For test data
md2 = md.pattern(x.test_imp)
gg_miss_var(x.test_imp)

d.train.imp = x.train_imp
d.test.imp = x.test_imp

d.train.imp["TenYearCHD"] = y.train.miss
d.test.imp["TenYearCHD"] = y.test.miss


```

We have managed to impute the missing values, but there are several steps in this procedure that could be improved. 

First of all, we could have used more flexible models for imputation than linear imputation, and we could have fit several different models on the training set and done evaluation procedures within the training set (e.g. cross validated optimization of ROC-AUC) to pick the models before fixing a model to impute each variable. 

Some variables, such as 'cigsPerDay', are in some sense discrete, although we have treated them as continuous (as it is possible to smoke e.g. $2,73$ cigarettes per day). These could have been rounded off to integers, but we didn't see why this would be necessary. 
We did not include the response ('TenYearCHD') in the regressions. 
It is not clear to us why it would be a better choice to include it, as we don't want the imputed data to be overfitted towards the response.
More reading and testing would be needed to figure out the optimal solution, if there is any canonical choice, and it would be interesting to see how our results changed if the response were included.

For less code, the 'MICE' package could have been used, referring to the function 'mice' for imputation of the training set and 'mice.reuse' to reuse the imputation models on the test set. 


# Model

In the model section we will consider the two data sets; the complete case and imputed case. Both data sets are further divided into a train and test set. 

```{r complete train test}
# Make the training data ready for lasso

set.seed(8701)

# Complete data

x.train.complete = scale(model.matrix(TenYearCHD ~ . -1, data = d.train.complete, family = binomial())) 
train.complete.mean = attr(x.train.complete, "scaled:center")
train.complete.sd = attr(x.train.complete, "scaled:scale")

y.train.complete = d.train.complete$TenYearCHD

# Imputed data
x.train.imp = scale(model.matrix(TenYearCHD ~ . -1, data = d.train.imp, family = binomial()))
train.imp.mean = attr(x.train.imp, "scaled:center")
train.imp.sd = attr(x.train.imp, "scaled:scale")
y.train.imp = d.train.imp$TenYearCHD

# Same for test data, but depending on train imp
x.test.imp = scale(model.matrix(TenYearCHD ~ . -1, data = d.test.imp, family = binomial()), center = train.imp.mean, scale = train.imp.sd)


################## !!! Viktig problem. Det funker ikke å bruke scale() fordi når vi senere skal predikere på test settet må vi standardisere dette også og da trenger vi mean og sd fra test settet, ikke train. Information leak! ##########################
```


Given the binary response it is natural to consider fitting a logistic regression model to our data. Although we intend to use lasso, it is nice to start by fitting a regular logistic regression model to get an indication of which covariates that are most present, and for later comparison. 

```{r Logistic Model}
# Fit a logistic model
mod0 <- glm(TenYearCHD ~ ., data = d.train.complete, family = binomial())
summary(mod0)

mod0_preds <- predict(mod0, newdata = d.test.complete, type = "response")

sum(round(mod0_preds) == 1)

predicted_value <- factor(round(mod0_preds))
expected_value <- factor(d.test.complete$TenYearCHD)

confusionMatrix(data=predicted_value, reference = expected_value)$table

roc_obj <- roc(d.test.complete$TenYearCHD, mod0_preds, levels = c(0,1), direction = "<")
plot(roc_obj)
auc(roc_obj)
```

The logistic regression model chooses `r names(coefficients(mod0))[c(1,2,3,5,8,13,14,18)]` as the significant covariates. It classifies very few positives correctly, which is very problematic if the model would be used to predict hearth disease. 



## Lasso on Complete Case 

Next we wish to do.
```{r Lasso CV}

# Use cross-validation to find lambda
cv.out = cv.glmnet(x.train.complete, y.train.complete, family = "binomial",intercept = F, standardize=TRUE, alpha = 1)
plot(cv.out$glmnet.fit, "lambda", label=F)
plot(cv.out)
cv.out$lambda.min
cv.out$lambda.1se

lasso_mod = glmnet(x.train.complete, y.train.complete, family = "binomial",intercept = F, standardize=TRUE, alpha = 1,  lasso = cv.out$lambda.1se)

lasso_coef <- coef(lasso_mod,s=cv.out$lambda.1se)
lasso_coef[,1]

x.test.complete = scale(model.matrix(TenYearCHD ~ . -1, data = d.test.complete, family = binomial()), center = train.complete.mean, scale = train.complete.sd)
lasso_preds <- predict(lasso_mod, newx = x.test.complete, type = "response",  s = cv.out$lambda.1se)

sum(round(lasso_preds) == 1)

predicted_value <- factor(round(lasso_preds))
expected_value <- factor(d.test.complete$TenYearCHD)

confusionMatrix(data=predicted_value, reference = expected_value)$table

roc_obj <- roc(d.test.complete$TenYearCHD, lasso_preds, levels = c(0,1), direction = "<")
plot(roc_obj)
auc(roc_obj)
```
The lasso model is much better at classifying positives than the full logistic model and has only slightly worse auc. 

Bootstrapping
```{r Bootstrap Lasso, eval = F}
################################## IGNORER FORELØPIG ##########################################
B = 500
boot_size_c = dim(d.train.complete)[1]

B_coef = matrix(NA, nrow = B, ncol = length(lasso_coef[,1]))
for (i in 1:B){
  data_b = sample(1:boot_size_c, size = boot_size_c, replace = TRUE)
  x = x.train.complete[data_b,]
  y = y.train.complete[data_b]
  cv.out = cv.glmnet(x, y, family = "binomial", alpha = 1)
  lasso_mod = glmnet(x, y, family = "binomial", alpha = 1, lasso = cv.out$lambda.1se)

  B_coef[i,] <- coef(lasso_mod,s=cv.out$lambda.1se)[,1]
}

colnames(B_coef) = names(coef(lasso_mod,s=cv.out$lambda.1se)[,1])
boxplot.matrix(B_coef[,-1], ylim = c(-0.25,0.55))

B_coef_count = ifelse(B_coef == 0,0,1)
apply(B_coef_count, 2, sum)
barplot(apply(B_coef_count, 2, sum)/B, las = 2)
#############################################################################################
```


## Lasso on Imputed Data

We do the same thing, just using the imputed data instead of the complete case. 
To spice things up, we also construct a imputed data set using 'mice' and 'mice.reuse', for comparison. 

```{r Imputation with mice, eval = F}
set.seed(8701)

# Important that we don't use the response as predictor when imputing
y.train <- d.train$TenYearCHD
y.test <- d.test$TenYearCHD

d.train["TenYearCHD"] <- NULL
d.test["TenYearCHD"] <- NULL

imp_train = mice(d.train, m=1)
d.train.imp.mice = complete(imp_train)

# Use the imputation model trained on the traning set on the test set, to ensure no data leakage.
imp_test = mice.reuse(imp_train, d.test)
d.test.imp.mice = imp_test$"1"

# We scale the model matrix as before.

x.train.imp.mice = scale(model.matrix(~ . -1, data = d.train.imp.mice, family = binomial()))
train.imp.mean.mice = attr(x.train.imp.mice, "scaled:center")
train.imp.sd.mice = attr(x.train.imp.mice, "scaled:scale")
y.train.imp.mice = d.train.imp$TenYearCHD

# Same for test data, but depending on train imp
x.test.imp.mice = scale(model.matrix(~ . -1, data = d.test.imp.mice, family = binomial()), center = train.imp.mean.mice, scale = train.imp.sd.mice)

##### FIX DATA LEAKAGE COMMENT ON WHY WE SCALE TEST WEIRDLHY
```


First we try the Lasso on the imputed dataset where we imputed with our manual technique.

```{r Manually Imputed Lasso model}
# We need to standardize the matrix so that we can drop intercepts.
cv.out = cv.glmnet(x.train.imp, y.train.imp, family = "binomial", intercept = F, standardize=TRUE, alpha = 1)
plot(cv.out$glmnet.fit, "lambda", label=F)
plot(cv.out)
cv.out$lambda.min
cv.out$lambda.1se

lasso_mod = glmnet(x.train.imp, y.train.imp, family = "binomial", alpha = 1, intercept = F, standardize=TRUE,  lasso = cv.out$lambda.1se)

lasso_coef <- coef(lasso_mod,s=cv.out$lambda.1se)
lasso_coef[,1]

lasso_preds <- predict(lasso_mod, newx = x.test.imp, type = "response",  s = cv.out$lambda.1se)

sum(round(lasso_preds) == 1)

predicted_value <- factor(round(lasso_preds))
expected_value <- factor(d.test.imp$TenYearCHD)

confusionMatrix(data=predicted_value, reference = expected_value)$table

roc_obj <- roc(d.test.imp$TenYearCHD, lasso_preds, levels = c(0,1), direction = "<")
plot(roc_obj)
auc(roc_obj)
```

The performance of this model is quite similar to that of the complete case. The prediction is quite balanced, labeling more positives correctly than not. The model does however incorrectly many negatives as positives, like the lasso for the complete case. 

We try to do the same thing, using the 'MICE'-imputed data set. 

```{r Mice Imputed Lasso model, eval = F}
cv.out = cv.glmnet(x.train.imp.mice, y.train, family = "binomial", intercept = F, standardize=TRUE, alpha = 1)
plot(cv.out$glmnet.fit, "lambda", label=F)
plot(cv.out)
cv.out$lambda.min
cv.out$lambda.1se


lasso_mod = glmnet(x.train.imp.mice, y.train, family = "binomial", alpha = 1, intercept = F, standardize=TRUE, lasso = cv.out$lambda.1se)

lasso_coef <- coef(lasso_mod,s=cv.out$lambda.1se)
lasso_coef[,1]

lasso_preds <- predict(lasso_mod, newx = x.test.imp.mice, type = "response", s = cv.out$lambda.1se)

predicted_value <- factor(round(lasso_preds))
expected_value <- factor(y.test)

confusionMatrix(data=predicted_value, reference = expected_value)$table

roc_obj_lasso <- roc(y.test, lasso_preds)
auc(roc_obj_lasso)
```

The predictive performance has not changed a lot, in the eyes of AUC, but as it misclassifies almost every ill person, this is perhaps the worst model. 
This may indicate that the base imputation methods in 'mice' actually overfits to the training data.
Our manually imputed data set had a nuanced (although simple) take on which methods to use, such as linear models, log.reg. and kNN.

We check with methods 'mice' is using, in hope of finding the problem and improving the imputation procedure. 

### Specifying imputation methods

```{r Check what mice already uses, eval = F}
init = mice(data, maxit = 0)
meth = init$method
meth <- meth[-16] #Remove response
meth
```

It turns out that 'mice' uses predictive mean matching (pmm) for all variables, except 'education', which has multiple classes.
For 'education' it uses multiple logistic regression.
We first check which type each variable is. 

```{r Check data type, eval = F}
types = sapply(data, class)
types
```
Then we count the number of unique values each variable has, including NA's as its own level.

```{r Check number of unique, eval = F}
n_unique = sapply(data, function(x) length(unique(x)))
n_unique
```
From this, we may understand, that in the eyes of 'mice', 'BPMeds' actually has three levels, when it in reality has two. 
Therefore, we specify the methods we want mice to use before imputing again. 
Since 'BPMeds' is binary, we use logistic regression to classify, we continue to use 'polyreg' for 'education' (to compare with our manual imputation, where we used kNN), and we fit linear imputation models for the other variables.

```{r Specify new imputation methods, eval = F}
meth["BPMeds"] <- "logreg"
meth["totChol"] <- "norm.predict"
meth["BMI"] <- "norm.predict"
meth["heartRate"] <- "norm.predict"
meth["glucose"] <- "norm.predict"
meth["cigsPerDay"] <- "norm.predict"
meth
```

We try again to impute using 'mice', but with our newly specified imputation methods. 


```{r new mice imputation, eval = F}

set.seed(8701)
imp_train.mice.new = mice(d.train, m=1, method = meth)
d.train.imp.mice.new = complete(imp_train.mice.new)

imp_test.mice.new = mice.reuse(imp_train.mice.new, d.test)
d.test.imp.mice.new = imp_test.mice.new$"1"

x.train.imp.mice.new <- model.matrix(~.-1, data = d.train.imp.mice.new)
x.test.imp.mice.new <- model.matrix(~.-1, data = d.test.imp.mice.new)

# We scale the model matrix as before.

x.train.imp.mice.new= scale(model.matrix(~ . -1, data = d.train.imp.mice.new, family = binomial()))
train.imp.mean.mice.new = attr(x.train.imp.mice.new, "scaled:center")
train.imp.sd.mice.new = attr(x.train.imp.mice.new, "scaled:scale")
y.train.imp.mice.new = d.train.imp$TenYearCHD

# Same for test data, but depending on train imp
x.test.imp.mice.new = scale(model.matrix(~ . -1, data = d.test.imp.mice.new, family = binomial()), center = train.imp.mean.mice.new, scale = train.imp.sd.mice.new)

##### FIX DATA LEAKAGE COMMENT ON WHY WE SCALE TEST WEIRDLHY

```

The newly imputed data set found by specified methods in 'mice' is then used to fit a Lasso. 

```{r Lasso Imputed with new methods for mice, eval = F}
set.seed(8701)

# Use cross-validation to find lambda
cv.out = cv.glmnet(x.train.imp.mice.new, y.train, family = "binomial", intercept = F, standardize=TRUE, alpha = 1)
plot(cv.out$glmnet.fit, "lambda", label=F)
plot(cv.out)
cv.out$lambda.min
cv.out$lambda.1se



lasso_mod_imp_s = glmnet(x.train.imp.mice.new, y.train, family = "binomial", intercept = F, standardize=TRUE, alpha = 1,  lasso = cv.out$lambda.1se)

lasso_imp_coef <- coef(lasso_mod_imp_s, s=cv.out$lambda.1se)
lasso_imp_coef[,1]




lasso_imp_preds <- predict(lasso_mod_imp_s, newx = x.test.imp.mice.new, type = "response",  s = cv.out$lambda.1se)

predicted_value <- factor(round(lasso_imp_preds))
expected_value <- factor(y.test)

confusionMatrix(data=predicted_value, reference = expected_value)$table

roc_obj_lasso <- roc(y.test, lasso_imp_preds)
auc(roc_obj_lasso)
```

Surprisingly enough, we obtain a slightly better AUC, but without changing the number of true positives significantly. 
This is still terrible, as we would at least want to have a low amount of false negatives. 
Using different imputation methods seems to have some effect on the prediction, but the difference might be due to random effects, and it is not clear to us why 'mice' yields so bad results. 
We strongly suspect that 'mice' introduces extra correlation by running chained imputations.
That is, where our manual approach used the same complete data to predict each imputation value, 'mice' probably imputes one covariate, and then uses the newly imputed data set to impute further covariates. 
Since the Lasso is not that good at handling high correlation, this chained imputation by 'mice' may cause most of the problems.

### Bootstrapping

To obtain improved estimates for the coefficients in the Lasso, we use bootstrapping. 
The data set used will be our manually imputed data set, as this seems to give better results due to 'mice' introducing extra correlation.


```{r Bootstrap Lasso with imputed data, eval = F}
set.seed(8701)
B = 500
size = dim(x.train.imp)[1]

B_coef = matrix(NA, nrow = B, ncol = length(lasso_coef[,1]))
for (i in 1:B){
  boot_ind = sample(1:size, size = size, replace = TRUE)
  
  x = x.train.imp[boot_ind,]
  y = y.train[boot_ind]
  
  cv.out = cv.glmnet(x, y, family = "binomial", intercept = F, standardize=TRUE, alpha = 1)
  lasso_mod = glmnet(x, y, family = "binomial", intercept = F, standardize=TRUE, alpha = 1, lasso = cv.out$lambda.1se)

  B_coef[i,] <- coef(lasso_mod,s=cv.out$lambda.1se)[,1]
}

colnames(B_coef) = names(coef(lasso_mod,s=cv.out$lambda.1se)[,1])
boxplot.matrix(B_coef[,-1], las = 2)
#, ylim = c(-0.02,0.02)
```

```{r, eval = F}
B_coef_count = ifelse(B_coef == 0,0,1)
barplot(apply(B_coef_count, 2, sum)/B, las=2)
```
The result form the imputed data set is quite different from that of the non-imputed one. We might considder to use all coeficients which used in over 50% of the models, which is in this case 
```{r, eval = F}
names(apply(B_coef_count, 2, sum)/B)[apply(B_coef_count, 2, sum)/B > 0.5]
```

# Inference

In order to do inference we simply fit a logistic regression model using the `glm` function in `R`, and extract the inference from there. However, we will keep the coefficients chosen by the lasso-bootstrapping iteration in last section, and now use the test data to fit a logistic model to avoid overfitting. 

```{r complete case comparison, eval = F}
mod <- glm(TenYearCHD ~ 1 + age + male + sysBP, data = df_complete[-train,], family = binomial())

confint(mod)

confint(mod0)

```

```{r Inference for imputed data, eval = F}
data_test_imp["TenYearCHD"] <- y.test
mod <- glm(TenYearCHD ~ 1+age + male + sysBP + cigsPerDay + prevalentHyp + glucose, data = data_test_imp, family = binomial())

confint(mod)

summary(mod)
```



# Discussion

STILL MISSING TO DO: 

Endre så test-settene blir scaled med hensyn på colonne mean og sd fra training set, ikke seg selv. Kan fortsatt bruke scale, men må gi argumentene center = mean(train.data) og scale = sd(train.data). Bør være greit. Jeg kan evt. fikse senere, men prediksjonene nå er feil pga dette.  
Hvis vi fikser disse to tingene så bør det kun mangle model selection og inference. 
