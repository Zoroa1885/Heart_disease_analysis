---
subtitle: "MA8701 Advanced Statistical Learning V2023"
title: "Compulsory exercise: Team SuperGreat"
author: "Nora Aasen, Elias Angelsen, Jonas Nordstrom"
date: "`r format(Sys.time(), '%d %B, %Y')`"
bibliography: [references.bib]
nocite: '@*'
output: 
  # html_document
  pdf_document
---
  
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = FALSE, tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",
                      fig.width=6, fig.height=4.5,fig.align = "center")
```




```{r Loading Packages,eval=TRUE}
# Load libraries
library(naniar)
library(mice)
library(glmnet)
library(tidyr)
library(dplyr)
library(caret)
library(ggcorrplot)
library(ggplot2)
library(e1071)
library(pROC)
library(caret)
library(NADIA)
```

# Introduction

In this project we have studied the [Framingham Coronary Heart Disease Dataset](https://www.kaggle.com/datasets/dileep070/heart-disease-prediction-using-logistic-regression?fbclid=IwAR1LE3P3vM1SyHBifotrNdXoKGv7szGR07labEAQo6XUqV9Pi90vtAp4mS4). This dataset contains patient information for inhabitants in Framingham, Massachusetts, and is typically used to predict the chance of getting coronary heart disease (CHD) within the next 10 years. For this project, however, we intend to use lasso to find the most important risk factors.
A big part of the task is to handle missing data. We will do single regression imputation manually and through the `mice` package, and investigate a bit what the Lasso is doing on the imputed data sets, compared to the complete case.

## Exploratory Analysis

We start by examining the dataset. 

```{r Loading Data, echo = T, fig.width= 4, fig.height=4, cex = 0.7}
# Load and look at data
#data <- read.csv("C:\\Users\\noraa\\Student\\5thYear\\MA8701\\data_analysis_proj\\Heart_disease_analysis\\framingham.csv")
data <- read.csv("framingham.csv")

data_dim = dim(data)
pos_response = sum(data$TenYearCHD==1)

# We visualize the data
ggplot(gather(data), aes(value)) + 
    geom_histogram(bins = 16) + 
    facet_wrap(~key, scales = 'free_x')

# Code education as a factor variable instead of 1-2-3-4.
data$education = factor(data$education, labels = c("none","hs","college","post-grad"))

```

This data set contains `r data_dim[1]` observations, `r data_dim[2]-1` covariates and a binary response variable `TenYearCHD`. We will try to fit a logistic regression model. The response variable has `r pos_response` observations that are 1, which equals about `r round(pos_response/data_dim[1]*100,1)`\% of the total observations. Most of our covariates are either binary, or numeric. However, we note that the variable education is most likely a categorical covariate. We could not find any further elaboration for which four categories the numbers represent, so based on the frequency of each value and qualified guessing, we changed it to a factor variable and defined the four categories as `r names(summary(data$education))[1:4]`.

We plot the number of negative and positive cases in the dataset.

```{r}
response = data$TenYearCHD
response[response == 1] = "positive"
response[response == 0] = "negative"
response = data.frame(TenYearCHD = response)

ggplot(data = response, aes(TenYearCHD))+
  geom_bar()+
  ggtitle("Frequency of postive vs. negative cases")
```
There is a high number negative cases compared to positive one. This is important to take into account when choosing performance measure. In particular, we should refrain from using accuracy, as a model who predicts only negative would score high in this regard, but would be practice useless, since it would be unable to identify any positive cases. In practice, we would rather have a model that produces balanced predictions, at the sacrifice of some accuracy. ROC-AUC is a great measure to use for this purpose, since it takes into account both the specificity and sensitivity of the model; the true negative rate and the true positive rate, respectively. 

The next thing we looked at was the number of missing data in our data set.

```{r plot missing data, echo = T, fig.width= 10, fig.height=6, cex = 0.5}
# Look at the missing data
md_mat = md.pattern(data, rotate.names = T, plot = T)
```

```{r, echo = T, fig.width= 5, fig.height=3, cex = 0.5}
gg_miss_var(data)
```

As we can see there are six covariates that has missing data: `glucose`, `education`, `BPMeds`, `totChol`, `cigsPerDay`, and `BMI`. We cannot use the rows that contain missing values as is. The easiest solution is to remove all rows that contains `NA`'s. This is the \textit{complete case} solution.
We split the data into training and test sets, as well as keeping the complete case data set to later use it for comparison.

```{r train-test split, echo = T}
# Split into training and test first
set.seed(8701)
tr = 7/10 # train ratio
r = dim(data)[1]
size = round(r*tr)
train = sample(1:r,size = size)

d.train = data[train,]
d.test = data[-train,]

# Make a dataset containing only the complete cases
d.complete <- data[complete.cases(data), ]
d.train.complete <- d.train[complete.cases(d.train), ]
d.test.complete <- d.test[complete.cases(d.test), ]

pos_response_c = sum(data[complete.cases(data),]$TenYearCHD==1)
```

The complete data set contains `r sum(complete.cases(data))` observations and the response variable has `r pos_response_c` observations that are 1, which equals about `r round(pos_response_c/sum(complete.cases(data))*100,1)`\% of the total observations. As we can see, the proportion of positive observations in the response is the same, which is a good indicator that our data is missing at random (MAR), as we will discuss later.



```{r exp analysis, eval = F, fig.width= 5, fig.height=5, cex = 0.5}
# Is it of interest to consider a correlation plot in this case? 

# To end our exploratory analysis, we make a correlation plot of the complete data set, but note that we have removed the multi-class variable 'education'. 

# Make a correlation plot 
cor_mat = cor(subset(d.complete, select = -education)) # Cannot compute correlation with factor variables
ggcorrplot(cor_mat, hc.order = TRUE, type = "lower",
   outline.col = "white",
   colors = c("blue", "white", "red"))

# The above tells us that some variables are highly correlated. These are 'currentSmoker' and 'cigsPerDay', 'sysBP' and 'diaBP', 'prevalentHyp and 'diaBP'/'sysBP', and 'diabetes' and 'glucose'. Initially, we therefore expect at most one of 'currentSmoker' and 'cigsPerDay', one of 'sysBP' and 'diaBP', and one of 'diabetes' and 'glucose' to be picked out by the Lasso. The case for 'prevalentHyp' is slightly more complex. It's influence towards the response may be absorbed into 'sysBP' and/or 'diaBP', in the eyes of the Lasso. It could also dominate both and push both of these out of the model, or it could be included in the model, but with much lower significance than the other included variables. We don't know yet.
```


# Missing Data


We start by recalling that there are several types of mechanisms for missing data.
Let $Z = (X,y)$ denote the full collection of covariates and responses, respectively, and we let a subscript mis/obs indicate whether we are restricting $Z$ (or $X$) to the missing or observed parts, respectively.
We may form an indicator (0-1) matrix $R$ indicating missing (0) and observed (1) covariates. 
Assume $\psi$ is short for the parameters in the distribution of $R$.

The missing data may be characterized by the conditioning in the distribution of $R$.
We define the data to be:
\begin{itemize}
  \item missing completely at random (MCAR) if $P(R | Z, \psi) = P(R | \psi)$,
  \item missing at random (MAR) if $P(R | Z, \psi) = P(R | Z_{obs}, \psi)$,
  \item missing not at random (MNAR) if $P(R | Z, \psi) = P(R | Z, \psi)$ (i.e. we don't have MCAR or MAR).
\end{itemize}

By exploring the missing pattern of for example the variable `cigsPerDay`, we see that our missing mechanism is not MCAR.
No non-smoker has failed to answer the question ''How may cigarettes do you smoke a day?'', which is a question only aimed at smokers. 
The simple explanation may be that the survey automatically fills in $0$ for `cigsPerDay` if you claim to be a non-smoker. 
In more mathematical terms, the missingness of `cigsPerDay` depends on the observed answer to ''Do you smoke?'' (found in variable `currentSmoker`), indicating that we do not work with MCAR data. Luckily, most methods are applicable if our missingness is at least MAR. 

We will assume that the missing mechanism is MAR for all our missing observations, as there is no clear reason to suspect it to be MNAR. 

To treat the missing data, we will use single imputation, as multiple imputation may cause difficulties with the resulting inference, as Rubin's rules needs to be combined with the Lasso, bootstrap and concluding inference. Multiple imputation was therefore not considered because it is beyond the scope of this project.

To perform single imputation we will use regression imputation, where we adapt our regression technique depending on the type of variable imputed. For continuous variables, we use a linear regression model, for binary variables, we use logistic regression, and for the variable ''education'', which is a four-class variable, we have utilized kNN for multiclass imputation. 
This is implemented manually, but this could have been done using the `MICE` package and the function `mice`.

To avoid encountering observations with more than one missing value, and hence problems with regressing, we remove all rows containing more than one `NA`. 

Our data is split into training and test sets, with the test-to-training ratio being 3:7. In order to avoid data leakage in our imputation of the test set, we fit the imputation models on the training set.
The main idea is that the test set should be viewed as several independent observations. Using the test set to impute itself will use information not present at the time of training and will yield unintended correlation.
This is again done manually, but could also have been done using `mice.reuse`, as we will do later.

Note that we do not include the response (`TenYearCHD`) in the regression, and in order to avoid too much correlation between the imputed samples, we always base the regression models on the complete case data, instead of letting the imputed values for variable $n$ regress to impute variable $n+1$.  


```{r Remove rows with more than 1 NA, echo = T}
# Setting seed expressing our love for MA8701
set.seed(8701)

# Throwing out the samples with two or more NAs from the data
miss <- c()
for(i in 1:nrow(d.train)) {
  if(sum(is.na(d.train[i,])) > 1){
    miss <- append(miss,i)
    }  
}

d.train.missing <- d.train[-miss,]


miss <- c()
for(i in 1:nrow(d.test)) {
  if(sum(is.na(d.test[i,])) > 1){
    miss <- append(miss,i)
    }  
}

d.test.missing <- d.test[-miss,]
```


We further split the data into further training and test sets with and without the response `TenYearCHD`, following the training-test-ratio that we have previously set.

``` {r Missing data split, echo = T}
x.train_dat_m = subset(d.train.missing, select = -TenYearCHD)
y.train.miss = d.train.missing$TenYearCHD

x.test_dat_m = subset(d.test.missing, select = -TenYearCHD)
y.test.miss = d.test.missing$TenYearCHD
```

Using these data sets, we make regression models for each missing variable based on the data we have. 
These models automatically neglects NA's. For `glucose`, `cigsPerDay`, `BMI`, `totChol` and `heartRate`, we fit linear models, while for the binary variable `BPMeds`, a logistic model is fit. The multi-class variable `education` will be imputed using a kNN-model. 

``` {r Missing data regression models, echo = T}
# We fit linear models on the training set for glucose, cigsPerDay, BMI, totChol, heartRate.
fit_for_simp_glucose_m <- lm(glucose ~., data = x.train_dat_m)    # Linear model for glucose
fit_for_simp_cigs_m <- lm(cigsPerDay ~., data = x.train_dat_m)    # Linear model for cigsPerDay
fit_for_simp_BMI_m <- lm(BMI ~., data = x.train_dat_m)            # Linear model for BMI
fit_for_simp_totChol_m <- lm(totChol ~., data = x.train_dat_m)    # Linear model for totChol
fit_for_simp_heartRate_m <- lm(heartRate ~., data = x.train_dat_m)# Linear model for heartRate 

# We fit a logistic model on the training set for BPMeds, being a binary variable.
fit_for_simp_BPMeds_m <- glm(BPMeds ~., data = x.train_dat_m, family = "binomial") #Logistic model for BPMeds

# We fit a KNN model on the training set for education, being a multi-class variable.
fit_for_simp_edu_m <- gknn(education ~., data = x.train_dat_m) # kNN model for education
```

To predict the missing values for each variable, we pick out those samples with missing values of each variable from the training and test set.
Note that we are imputing for both the training and test set, even though the sampled (therefore "random") split into training and test set may have sorted all NA's of a variable (e.g. heartRate) into a single set (i.e. into either training or test set).
This is not a problem, as we are only picking out the incomplete cases. Therefore, predicting and filling in the missing values in the complete part is a vacuous procedure, not yielding any problems.

``` {r Missing data which-to-predict, echo = T}
# Pick out those variables with missing glucose from the training and test set.
train_data_to_pred_gluc_m = ic(x.train_dat_m)
train_data_to_pred_gluc_m = train_data_to_pred_gluc_m[ici(train_data_to_pred_gluc_m$glucose),] 
test_data_to_pred_gluc_m = ic(x.test_dat_m)
test_data_to_pred_gluc_m = test_data_to_pred_gluc_m[ici(test_data_to_pred_gluc_m$glucose),]

# Pick out those variables with missing cigsPerDay from the training and test set.
train_data_to_pred_cigs_m = ic(x.train_dat_m)
train_data_to_pred_cigs_m = train_data_to_pred_cigs_m[ici(train_data_to_pred_cigs_m$cigsPerDay),]
test_data_to_pred_cigs_m = ic(x.test_dat_m)
test_data_to_pred_cigs_m = test_data_to_pred_cigs_m[ici(test_data_to_pred_cigs_m$cigsPerDay),]

# Pick out those variables with missing BMI from the training and test set.
train_data_to_pred_BMI_m = ic(x.train_dat_m)
train_data_to_pred_BMI_m = train_data_to_pred_BMI_m[ici(train_data_to_pred_BMI_m$BMI),]
test_data_to_pred_BMI_m = ic(x.test_dat_m)
test_data_to_pred_BMI_m = test_data_to_pred_BMI_m[ici(test_data_to_pred_BMI_m$BMI),]

# Pick out those variables with missing totChol from the training and test set.
train_data_to_pred_totChol_m = ic(x.train_dat_m)
train_data_to_pred_totChol_m = train_data_to_pred_totChol_m[ici(train_data_to_pred_totChol_m$totChol),]
test_data_to_pred_totChol_m = ic(x.test_dat_m)
test_data_to_pred_totChol_m = test_data_to_pred_totChol_m[ici(test_data_to_pred_totChol_m$totChol),]

# Pick out those variables with missing heartRate from the training and test set.
train_data_to_pred_heartRate_m = ic(x.train_dat_m)
train_data_to_pred_heartRate_m = train_data_to_pred_heartRate_m[ici(train_data_to_pred_heartRate_m$heartRate),]
test_data_to_pred_heartRate_m = ic(x.test_dat_m)
test_data_to_pred_heartRate_m = test_data_to_pred_heartRate_m[ici(test_data_to_pred_heartRate_m$heartRate),]

# Pick out those variables with missing BPMeds from the training and test set.
train_data_to_pred_BPMeds_m = ic(x.train_dat_m)
train_data_to_pred_BPMeds_m = train_data_to_pred_BPMeds_m[ici(train_data_to_pred_BPMeds_m$BPMeds),]
test_data_to_pred_BPMeds_m = ic(x.test_dat_m)
test_data_to_pred_BPMeds_m = test_data_to_pred_BPMeds_m[ici(test_data_to_pred_BPMeds_m$BPMeds),]

# Pick out those variables with missing education from the training and test set.
train_data_to_pred_edu_m = ic(x.train_dat_m)
train_data_to_pred_edu_m = train_data_to_pred_edu_m[ici(train_data_to_pred_edu_m$education),]
test_data_to_pred_edu_m = ic(x.test_dat_m)
test_data_to_pred_edu_m = test_data_to_pred_edu_m[ici(test_data_to_pred_edu_m$education),]

```

We then predict the missing values for imputation. For the logistic model fit to predict `BPMeds`, we assign a prediction class $1$ or $0$ depending on whether the predicted value is above or below $0.5$.

``` {r Missing Data predictions, echo = T}
set.seed(8701)
# Predicting the values to impute for glucose on the training and test data.
pred_glucose_train_m <- predict(fit_for_simp_glucose_m,
                                newdata = train_data_to_pred_gluc_m) 
pred_glucose_test_m <- predict(fit_for_simp_glucose_m,
                               newdata = test_data_to_pred_gluc_m)

# Predicting the values to impute for cigsPerDay on the training and test data.
pred_cigs_train_m <- predict(fit_for_simp_cigs_m,
                             newdata = train_data_to_pred_cigs_m) 
pred_cigs_test_m <- predict(fit_for_simp_cigs_m,
                            newdata = test_data_to_pred_cigs_m)

# Predicting the values to impute for BMI on the training and test data.
pred_BMI_train_m <- predict(fit_for_simp_BMI_m,
                            newdata = train_data_to_pred_BMI_m) 
pred_BMI_test_m <- predict(fit_for_simp_BMI_m,
                           newdata = test_data_to_pred_BMI_m)

# Predicting the values to impute for totChol on the training and test data.
pred_totChol_train_m <- predict(fit_for_simp_totChol_m,
                                newdata = train_data_to_pred_totChol_m) 
pred_totChol_test_m <- predict(fit_for_simp_totChol_m,
                               newdata = test_data_to_pred_totChol_m)

# Predicting the values to impute for heartRate on the training and test data.
pred_heartRate_train_m <- predict(fit_for_simp_heartRate_m,
                                  newdata = train_data_to_pred_heartRate_m) 
pred_heartRate_test_m <- predict(fit_for_simp_heartRate_m,
                                 newdata = test_data_to_pred_heartRate_m)

# Predicting the classes to impute for BPMeds on the training and test data by first predicting numerical values in (0,1) and then classifying these to either 0 or 1. 
pred_BPMeds_train_m_probs <- predict(fit_for_simp_BPMeds_m,
                                     newdata = train_data_to_pred_BPMeds_m, type = "response") 
pred_BPMeds_test_m_probs <- predict(fit_for_simp_BPMeds_m,
                                    newdata = test_data_to_pred_BPMeds_m, type = "response")
pred_BPMeds_train_m <- ifelse(pred_BPMeds_train_m_probs >= 0.5, 1, 0)
pred_BPMeds_test_m <- ifelse(pred_BPMeds_test_m_probs >= 0.5, 1, 0)

# Predicting the classes to impute for education on the training and test data.
pred_edu_train_m <- predict(fit_for_simp_edu_m,
                            newdata = train_data_to_pred_edu_m, type = "class") 
pred_edu_test_m <- predict(fit_for_simp_edu_m,
                           newdata = test_data_to_pred_edu_m, type = "class")

```

To see how our predictions compare to the complete case, we plot (in different ways) the predicted values/classes.
For illustrating different types of plots useful for this, we plot glucose twice, as both a transparent histogram and a point plot. 
This is only for illustrational purposes and therefore only done for the training data with its imputed values. 

```{r Missing data train-pred-plots, echo = T, fig.width=6, fig.height=4.5}

# ---- Make transparent colors for plotting ----
c1 <- rgb(173,216,230,max = 255, alpha = 80, names = "lt.blue")
c2 <- rgb(255,192,203, max = 255, alpha = 80, names = "lt.pink")

# ---- Make histogram objects for glucose ----

# Histogram objects for glucose
hg_pred_gluc = hist(pred_glucose_train_m, plot = FALSE)
hg_old_gluc = hist(x.train_dat_m$glucose, plot = FALSE)


# ---- Make point plots over indexes for all except education ----

par(mfrow = c(2,3))

# Point plot for glucose
plot(x.train_dat_m$glucose, ylab = "Value of Glucose", main = "Glucose")
points(seq(1,length(x.train_dat_m[,1]),length.out = length(pred_glucose_train_m)),pred_glucose_train_m, col = "orange", pch = 16)
abline(h = mean(x.train_dat_m$glucose), col = "red")

# Point plot for cigsperday
plot(x.train_dat_m$cigs, ylab = "Value of cigsPerDay", main = "cigsPerDay")
points(seq(1,length(x.train_dat_m[,1]),
           length.out = length(pred_cigs_train_m)), 
       pred_cigs_train_m, col = "orange", pch = 16)
abline(h = mean(x.train_dat_m$cigsPerDay), col = "red")

# Point plot for BMI
plot(x.train_dat_m$BMI, ylab = "Value of BMI", main = "BMI")
points(seq(1,length(x.train_dat_m[,1]), 
           length.out = length(pred_BMI_train_m)),
       pred_BMI_train_m, col = "orange", pch = 16)
abline(h = mean(x.train_dat_m$BMI), col = "red")

# Point plot for totChol
plot(x.train_dat_m$totChol, ylab = "Value of totChol", main = "totChol")
points(seq(1,length(x.train_dat_m[,1]),
           length.out = length(pred_totChol_train_m)),
       pred_totChol_train_m, col = "orange", pch = 16)
abline(h = mean(x.train_dat_m$totChol), col = "red")

# Point plot for heartRate
plot(x.train_dat_m$heartRate, ylab = "Value of heartRate", main = "heartRate")
points(seq(1,length(x.train_dat_m[,1]),length.out = length(pred_heartRate_train_m)),pred_heartRate_train_m, col = "orange", pch = 16)
abline(h = mean(x.train_dat_m$heartRate), col = "red")

# Point plot for BPMeds
plot(x.train_dat_m$BPMeds, ylab = "Class of BPMeds", main = "BPMeds")
points(seq(1,length(x.train_dat_m[,1]),length.out = length(pred_BPMeds_train_m)),pred_BPMeds_train_m, col = "orange", pch = 16)
abline(h = mean(x.train_dat_m$BPMeds), col = "red")

```

```{r Missing data train-pred-plots-2, echo = T, fig.width=6.5, fig.height=3.5}

# ---- Make histogram plots for glucose and education ----

par(mfrow = c(1,1))

# Histogram plots for glucose
plot(hg_old_gluc, col = c1, xlab = "Value of Glucose", main = "Glucose") # Plot 1st histogram using a transparent color
plot(hg_pred_gluc, , col = c2, add = TRUE)
legend("topright", legend=c("Observed - blue", "Predictions - pink"), cex=0.6)

par(mfrow = c(1,2))

# Histograms for education
plot(x.train_dat_m$education, main = "Education - Observed", ylab = "Frequency")
plot(pred_edu_train_m, main = "Education - Predicted", ylab = "Frequency")

```
For most of the linear predictors, we are imputing using linear models. As we can see in these plots, we are close to the mean, but we are including more variance than what mean imputation would be able to. The binary variable `BPMeds` is often classified to zero, which is expected, as the variable explains whether or not the patient was taking blood pressure medicine at the time of the survey. The `education` variable is predicted using kNN, and we predict surprisingly many with higher education.
This can imply that the kNN-model we fit is not optimal for predicting `education`. Lastly, we update our data with the newly imputed values.

``` {r Missing Data imputation, echo = T}

# ---- Making new data sets based on the old ones ----
x.train_imp = x.train_dat_m
x.test_imp = x.test_dat_m

# ---- We add the predictions to this data set. ----

# Adding glucose
x.train_imp[ici(x.train_imp$glucose),]$glucose <- pred_glucose_train_m
x.test_imp[ici(x.test_imp$glucose),]$glucose <- pred_glucose_test_m

# Adding cigsPerDay
x.train_imp[ici(x.train_imp$cigsPerDay),]$cigsPerDay <- pred_cigs_train_m
x.test_imp[ici(x.test_imp$cigsPerDay),]$cigsPerDay <- pred_cigs_test_m

# Adding BMI
x.train_imp[ici(x.train_imp$BMI),]$BMI <- pred_BMI_train_m
x.test_imp[ici(x.test_imp$BMI),]$BMI <- pred_BMI_test_m

# Adding totChol
x.train_imp[ici(x.train_imp$totChol),]$totChol <- pred_totChol_train_m
x.test_imp[ici(x.test_imp$totChol),]$totChol <- pred_totChol_test_m

# Adding heartRate
x.train_imp[ici(x.train_imp$heartRate),]$heartRate <- pred_heartRate_train_m
x.test_imp[ici(x.test_imp$heartRate),]$heartRate <- pred_heartRate_test_m

# Adding BPMeds
x.train_imp[ici(x.train_imp$BPMeds),]$BPMeds <- pred_BPMeds_train_m
x.test_imp[ici(x.test_imp$BPMeds),]$BPMeds <- pred_BPMeds_test_m

# Adding education
x.train_imp[ici(x.train_imp$education),]$education <- pred_edu_train_m
x.test_imp[ici(x.test_imp$education),]$education <- pred_edu_test_m


# Construct final imputed data sets

d.train.imp = x.train_imp
d.test.imp = x.test_imp

d.train.imp["TenYearCHD"] = y.train.miss
d.test.imp["TenYearCHD"] = y.test.miss


```



``` {r check missing data fixed, echo = F, eval = F}
# We have obtained completely imputed data sets. To see that our procedure has worked, we could consider the missing data patterns of the newly constructed data sets.

# For training data
md1 = md.pattern(x.train_imp)
gg_miss_var(x.train_imp)

# For test data
md2 = md.pattern(x.test_imp)
gg_miss_var(x.test_imp)


```

We have managed to impute the missing values, but we have already some thoughts on how this procedure could be improved. 

First of all, we could have used more flexible models for imputation than linear imputation, and we could have fit several different models on the training set and done evaluation procedures within the training set (e.g. cross validated optimization of ROC-AUC) to pick the models before fixing a model to impute each variable. 

Some variables, such as `cigsPerDay`, are in some sense discrete, although we have treated them as continuous (as it is possible to smoke e.g. $2,73$ cigarettes per day). These could have been rounded off to integers, but we didn't see why this would be necessary. 
We did not include the response (`TenYearCHD`) in the regressions. 
It is not clear to us why it would be a better choice to include it, as we don't want the imputed data to be overfitted towards the response.
More reading and testing would be needed to figure out the optimal solution, if there is any canonical choice, and it would be interesting to see how our results changed if the response were included.

For less code, the `MICE` package could have been used, referring to the function `mice` for imputation of the training set and `mice.reuse` to reuse the imputation models on the test set. 


# Model

In the model section we will consider the two data sets; the complete case and imputed case. Both data sets are further divided into a train and test set. 
It is common to standardize the data when doing Lasso, which centeres the data about the origin, such that the least square estimate of the intercept is zero, $\hat{\beta}_0 = 0$. This effectively allows us the drop the intercept, such that we only meassure the effect of the covariates on the response. However, the problem with this is data leakage. If we want to standardize the test data, we should standardize it using the mean and the standard deviation of the training data. Most importantly, using the test data to scale the test data will introduce correlation between the independent observations of the test set.
Since the scaling information from the test set is "not available" to us at the time of training, we cannot expect the coefficients in the Lasso to be appropriately scaled compared to the test data.
We solve this by scaling the training data, and then using the attributes of the training data to scale the test data accordingly.


```{r complete train test, echo = T}
# Make the training data ready for lasso by scaling.

set.seed(8701)

# Scale complete data for training

x.train.complete = scale(model.matrix(TenYearCHD ~ . -1,
                                      data = d.train.complete,
                                      family = binomial())) 
train.complete.mean = attr(x.train.complete, "scaled:center")
train.complete.sd = attr(x.train.complete, "scaled:scale")

y.train.complete = d.train.complete$TenYearCHD

# Imputed data training set scaling

x.train.imp = scale(model.matrix(TenYearCHD ~ . -1,
                                 data = d.train.imp,
                                 family = binomial()))
train.imp.mean = attr(x.train.imp, "scaled:center")
train.imp.sd = attr(x.train.imp, "scaled:scale")
y.train.imp = d.train.imp$TenYearCHD

# Same for imputed test data, but depending on training set attributes.
x.test.imp = scale(model.matrix(TenYearCHD ~ . -1,
                                data = d.test.imp,
                                family = binomial()),
                   center = train.imp.mean, scale = train.imp.sd)

```

Given the binary response it is natural to consider fitting a logistic regression model to our data. Although we intend to use lasso, it is nice to start by fitting a regular logistic regression model on the complete case data to get an indication of which covariates that are most present, and for later comparison. 
We obtain the regression coefficients, a confusion matrix, a ROC-curve and the ROC-AUC. 

```{r Logistic Model, echo = T, fig.width= 4, fig.height=4}
# Fit a logistic model
set.seed(8701)
mod0 <- glm(TenYearCHD ~ ., data = d.train.complete, family = binomial())
round(summary(mod0)$coefficients,7)

mod0_preds <- predict(mod0, newdata = d.test.complete, type = "response")

predicted_value <- factor(round(mod0_preds))
expected_value <- factor(d.test.complete$TenYearCHD)

conf.mat.cc = confusionMatrix(data=predicted_value, reference = expected_value)$table
conf.mat.cc

roc_obj_cc <- roc(d.test.complete$TenYearCHD, mod0_preds, levels = c(0,1), direction = "<")
plot(roc_obj_cc, main = "ROC for Logistic Model - CC", cex = 0.5)
auc_cc = auc(roc_obj_cc)
auc_cc

```

The logistic regression model chooses `r names(which(summary(mod0)$coefficients[,4] < 0.01))` as the significant covariates, where the p-value cutoff is 0.01. It classifies very few positives correctly, which is very problematic if the model would be used to predict hearth disease. 



## Lasso on Complete Case 

We continue to do the Lasso on the complete case data. To do this, we use cross-validation to find $\lambda_{min}$ and use the highest $\lambda$ with deviance within one standard deviation of $\lambda_{min}$. We cross-validate for $\lambda$ and plot the shrinkage and binomial deviance.

```{r Lasso CV, echo = T, fig.height = 4.3 , fig.width=6}
#, fig.height = 4.3 , fig.width=6
set.seed(8701)

# Use cross-validation to find lambda
cv.out = cv.glmnet(x.train.complete, y.train.complete, family = "binomial",intercept = F, standardize=TRUE, alpha = 1)
plot(cv.out$glmnet.fit, "lambda", label=F, main = c("CV for lambda - shrinkage of coefficients",""))
plot(cv.out, main = c("Binomial deviance for lambda",""))

# Fit the Lasso model

lasso_mod_cc = glmnet(x.train.complete, y.train.complete, family = "binomial",intercept = F, standardize=TRUE, alpha = 1,  lasso = cv.out$lambda.1se)

lasso_coef_cc <- coef(lasso_mod_cc,s=cv.out$lambda.1se)
```

The confusion table, ROC and ROC-AUC is given below.
```{r Lasso CV2, echo = F, fig.height = 4 , fig.width=4}
# We predict to find the confusion matrix and the ROC-curve and the ROC-AUC.

x.test.complete = scale(model.matrix(TenYearCHD ~ . -1, data = d.test.complete, family = binomial()), center = train.complete.mean, scale = train.complete.sd)
lasso_preds_cc <- predict(lasso_mod_cc, newx = x.test.complete, type = "response",  s = cv.out$lambda.1se)

predicted_value_cc <- factor(round(lasso_preds_cc))
expected_value_cc <- factor(d.test.complete$TenYearCHD)

conf_mat_cc = confusionMatrix(data=predicted_value_cc, reference = expected_value_cc)$table
conf_mat_cc

roc_obj_lasso_cc <- roc(d.test.complete$TenYearCHD, lasso_preds_cc, levels = c(0,1), direction = "<")
plot(roc_obj_lasso_cc, main = "ROC for Lasso - CC", cex = 0.5)
auc_lasso_cc = auc(roc_obj_lasso_cc)
auc_lasso_cc

sens_cc = sensitivity(conf_mat_cc)
spes_cc = specificity(conf_mat_cc)
names_lasso_coef = names(which(lasso_coef_cc[,1] > 0))

```

The Lasso on the complete case data chooses `r names_lasso_coef` as the covariates to include in the model. 
Initially, this is the same lasso model is much better at classifying positives than the full logistic model and has only slightly worse AUC, at `r auc_lasso_cc` for the Lasso versus `r auc_cc` for the logistic model. We see that the Lasso includes `prevalentHyp`, while the logistic model finds it almost significant, with p-value 
`r summary(mod0)$coefficients[11,4]`.
This agrees with the reasoning we made earlier, saying it could be an important parameter, but not as important as (either) `sysBP` or `diaBP`.

To obtain a better understanding of these coefficients, we bootstrap from the training data to fit Lasso models and store their coefficients.

```{r Bootstrap Lasso, echo = T, fig.width=6, fig.height=4.2}
set.seed(8701)

B = 500
boot_size_cc = dim(d.train.complete)[1]

B_coef_cc = matrix(NA, nrow = B, ncol = length(lasso_coef_cc[,1]))
for (i in 1:B){
  data_b = sample(1:boot_size_cc, size = boot_size_cc, replace = TRUE)
  x = x.train.complete[data_b,]
  y = y.train.complete[data_b]
  cv.out = cv.glmnet(x, y, family = "binomial", alpha = 1)
  lasso_mod = glmnet(x, y, family = "binomial", alpha = 1, intercept = F, lasso = cv.out$lambda.1se)

  B_coef_cc[i,] <- coef(lasso_mod,s=cv.out$lambda.1se)[,1]
}

colnames(B_coef_cc) = names(coef(lasso_mod,s=cv.out$lambda.1se)[,1])
boxplot.matrix(B_coef_cc[,-1], ylim = c(-0.25,0.55), las = 2, main = "Boxplot of estimated coefficients")

B_coef_count_cc = ifelse(B_coef_cc == 0,0,1)
barplot(apply(B_coef_count_cc, 2, sum)/B, las = 2, main = "Percentage of times coefficient was nonzero")

names_lasso_cc_70 <- names(apply(B_coef_count_cc, 2, sum)/B)[apply(B_coef_count_cc, 2, sum)/B > 0.7]
names_lasso_cc_50 <- names(apply(B_coef_count_cc, 2, sum)/B)[apply(B_coef_count_cc, 2, sum)/B > 0.5]
```

We ran the bootstrap using `r B` date sets. The variables that have nonzero coefficients in the Lasso models at least 70\% of the time, are `r names_lasso_cc_70`.
Similarly, by those to those who are included at least $50$\% of the time, we obtain `r names_lasso_cc_50`.
This is indeed similar to the ones we picked out earlier. 

## Lasso on Imputed Data

We now do the same thing, just using the imputed data instead of the complete case. 
Even though the imputed data includes more samples, the data quality might be reduced when we impute. There are several possible reasons for this. Firstly imputing always involves some degree of uncertainty, which may introduce a larger degree of noise to the data. Depending on the imputation method, it is possible to introduce bias into the date set. This is especially the case when using naive imputation methods, like mean imputation. For this reason, we use more advanced techniques that predict rather than estimate the missing values. 

To get hands-on experience with the `MICE` package, we also construct an imputed data set using `mice` and `mice.reuse`, for comparison. 
`mice` can be used on the entire training data, without needing to remove those samples with two or more covariates missing. 
The imputation model from the training set is employed to impute the test set as well, to avoid data leakage. 

```{r Imputation with mice, echo = T}
set.seed(8701)

# Important that we don't use the response as predictor when imputing
y.train <- d.train$TenYearCHD
y.test <- d.test$TenYearCHD

d.train["TenYearCHD"] <- NULL
d.test["TenYearCHD"] <- NULL

imp_train = mice(d.train, m=1, printFlag = F)
d.train.imp.mice = complete(imp_train)

# Use the imputation model trained on the training set on the test set, to ensure no data leakage.
imp_test = mice.reuse(imp_train, d.test, printFlag = F)
d.test.imp.mice = imp_test$"1"

# We scale the model matrix as before.

x.train.imp.mice = scale(model.matrix(~ . -1, data = d.train.imp.mice, family = binomial()))
train.imp.mean.mice = attr(x.train.imp.mice, "scaled:center")
train.imp.sd.mice = attr(x.train.imp.mice, "scaled:scale")
y.train.imp.mice = d.train.imp$TenYearCHD

# Same for test data, but depending on attributes from the training data in order to avoid data leakage.
x.test.imp.mice = scale(model.matrix(~ . -1, data = d.test.imp.mice, family = binomial()), center = train.imp.mean.mice, scale = train.imp.sd.mice)

```

First we try the Lasso on the imputed dataset where we imputed with our manual technique.
We show the shrinkage and the binomial deviance over $\lambda$, which is a part of the cross-validation

```{r Manually Imputed Lasso model, echo = T, fig.height = 4.3 , fig.width=6}
# We need to standardize the matrix so that we can drop intercepts.
cv.out = cv.glmnet(x.train.imp, y.train.imp, family = "binomial", intercept = F, standardize=TRUE, alpha = 1)
plot(cv.out$glmnet.fit, "lambda", label=F, main = c("CV for lambda - shrinkage of coefficients",""))
plot(cv.out, main = c("Binomial deviance for lambda",""))

lasso_mod_imp = glmnet(x.train.imp, y.train.imp, family = "binomial", alpha = 1, intercept = F, standardize=TRUE,  lasso = cv.out$lambda.1se)

lasso_coef_imp <- coef(lasso_mod,s=cv.out$lambda.1se)
```

We also give the confusion matrix, the ROC and the ROC-AUC.
```{r Manually Imputed Lasso model 2, echo = T, fig.height = 4 , fig.width=4}

lasso_preds_imp <- predict(lasso_mod_imp, newx = x.test.imp, type = "response",  s = cv.out$lambda.1se)

predicted_value_imp <- factor(round(lasso_preds_imp))
expected_value_imp <- factor(d.test.imp$TenYearCHD)

conf_mat_imp = confusionMatrix(data=predicted_value_imp, reference = expected_value_imp)$table


roc_obj_imp <- roc(d.test.imp$TenYearCHD, lasso_preds_imp, levels = c(0,1), direction = "<")
plot(roc_obj_imp, main = "ROC for Lasso - Manual imp", cex = 0.5)
auc(roc_obj_imp)

conf_mat_imp

sens_imp = sensitivity(conf_mat_imp)
spes_imp = specificity(conf_mat_imp)
names_lasso_coef_imp = names(which(lasso_coef_imp[,1] > 0))

```

The performance of this model is quite similar to that of the complete case, which is to be expected.
The Lasso on the manually imputed data chooses `r names_lasso_coef_imp` as the covariates to include in the model. 

Recall that the sensitivity can be measured by the true positive rate (i.e. the number of true positives over all positives) and the specificity can be measured by the true negative rate (i.e. true negative over all negative).
Comparing the Lasso on the complete case data with the imputed data, we note that we have sensitivity `r sens_cc` in the complete case and sensitivity `r sens_imp` in the imputed case.
The specificity of the complete case Lasso is `r spes_cc`, while for the imputed case it is `r spes_imp`.

Although the difference is not huge, it may resemble that the data quality in the imputed data is slightly lower, although for selecting covariates, we obtained the same answer.
In our data rich situation, this is neither clear enough to be rendered true, nor actually a problem, but for data poor situations, this is something to keep in mind.

We try to do the same thing, using the `MICE`-imputed data set, first plotting the shrinkage and binomial deviance over $\lambda$, and then give the confusion matrix, ROC and ROC-AUC.

```{r Mice Imputed Lasso model, echo = T, fig.height = 4.5 , fig.width=6}
set.seed(8701)
# Cross-validation
cv.out = cv.glmnet(x.train.imp.mice, y.train, family = "binomial", intercept = F, standardize=TRUE, alpha = 1)

plot(cv.out$glmnet.fit, "lambda", label=F, main = c("CV for lambda - shrinkage of coefficients",""))
plot(cv.out, main = c("Binomial deviance for lambda", ""))
#cv.out$lambda.min
#cv.out$lambda.1se


lasso_mod_mice = glmnet(x.train.imp.mice, y.train, family = "binomial", alpha = 1, intercept = F, standardize=TRUE, lasso = cv.out$lambda.1se)

lasso_coef_mice <- coef(lasso_mod_mice,s=cv.out$lambda.1se)
```

```{r Mice Imputed Lasso model 2, echo = T, fig.height = 4.2 , fig.width=4.2}
lasso_preds_mice <- predict(lasso_mod_mice, newx = x.test.imp.mice, type = "response", s = cv.out$lambda.1se)

predicted_value_mice <- factor(round(lasso_preds_mice))
expected_value_mice <- factor(y.test)

conf_mat_mice = confusionMatrix(data=predicted_value_mice, reference = expected_value_mice)$table

roc_obj_mice <- roc(y.test, lasso_preds_mice, levels = c(0,1), direction = "<")
plot(roc_obj_mice, main = "ROC for Lasso - MICE-imp", cex = 0.5)
auc(roc_obj_mice)

conf_mat_mice

sens_mice = sensitivity(conf_mat_mice)
spes_mice = specificity(conf_mat_mice)
names_coef_mice = names(which(lasso_coef_mice[,1] > 0))
```

The predictive performance has not changed a lot, in the eyes of AUC. The performance of this model is quite similar to that of the complete case, which is to be expected.
The Lasso on the `MICE`-imputed data chooses `r names_coef_mice` as the covariates to include in the model. 

We suspect that the sensitivity and specificity is even lower for the `MICE`-imputed data, as it also imputes those with more than one missing value.

Comparing the Lasso on the complete case data with the imputed data, we note that we have sensitivity `r sens_imp` in the manually imputed case and `r sens_mice`.
The specificity of the manually imputed case Lasso is `r spes_imp`, while for the mice case it is `r spes_mice`. 

### Bootstrapping

To obtain improved estimates for the coefficients in the Lasso, we use bootstrapping. 
The data set used will be the `MICE`-imputed data set.


```{r Bootstrap Lasso with imputed data, echo = T, fig.width=6, fig.height=4.2}
set.seed(8701)
B = 500
size = dim(x.train.imp.mice)[1]

B_coef_mice = matrix(NA, nrow = B, ncol = length(lasso_coef_mice[,1]))
for (i in 1:B){
  boot_ind = sample(1:size, size = size, replace = TRUE)
  
  x = x.train.imp.mice[boot_ind,]
  y = y.train[boot_ind]
  
  cv.out = cv.glmnet(x, y, family = "binomial", intercept = F, alpha = 1)
  lasso_mod = glmnet(x, y, family = "binomial", intercept = F, alpha = 1, lasso = cv.out$lambda.1se)

  B_coef_mice[i,] <- coef(lasso_mod,s=cv.out$lambda.1se)[,1]
}

colnames(B_coef_mice) = names(coef(lasso_mod,s=cv.out$lambda.1se)[,1])
boxplot.matrix(B_coef_mice[,-1], ylim = c(-0.4,0.5), las = 2, main = "Boxplot of estimated coefficients")
#, ylim = c(-0.02,0.02)

B_coef_count_mice = ifelse(B_coef_mice == 0,0,1)
barplot(apply(B_coef_count_mice, 2, sum)/B, las = 2, main = "Percentage of times coefficient was nonzero")

names_lasso_mice_70 <- names(apply(B_coef_count_mice, 2, sum)/B)[apply(B_coef_count_mice, 2, sum)/B > 0.7]
names_lasso_mice_50 <- names(apply(B_coef_count_mice, 2, sum)/B)[apply(B_coef_count_mice, 2, sum)/B > 0.5]
```
We ran the bootstrap using `r B` re-sampled data sets. The variables that have nonzero coefficients in the Lasso models at least 70\% of the time, are `r names_lasso_mice_70`.
Similarly, by those to those who are included at least $50$\% of the time, we obtain `r names_lasso_mice_50`. This is indeed similar to the ones we picked out earlier. 

# Inference

In order to do inference we simply fit a logistic regression model using the `glm` function in `R`, and extract the inference from there. However, we will keep the coefficients chosen by the lasso-bootstrapping iterations in the earlier models, and now use the test data to fit a logistic model to avoid overfitting. Note that we have only used the test data to predict and observe different measures, such as ROC-AUC, sensitivity/specificity, and so on. 
The test data is therefore suitable for inference, as it has not been perturbed in the procedure of fitting the models.

We start out with the complete case models and fit a logistic model with the most important variables, namely `r names_lasso_cc_70`.
We first state the coefficients of the new regression model and their confidence intervals.

```{r complete case comparison, echo = T}
# Fit the model on the complete case training data.

mod <- glm(TenYearCHD ~ age + male + sysBP + glucose, data = d.complete[-train,], family = binomial())

# Confidence intervals.

CI_mod = confint(mod)

CI_mod0 = confint(mod0)

# Look at the coefficients

summary(mod)$coefficients

```
The confidence intervals of these variables for the new regression model (left) and naive logistic model fit (right) on the complete data set is given by the following.
```{r complete case comparison2, echo = F}

# Compare confidence intervals
index_CI_mod = c(1,3,2,14,18) #Swap indeces 2 and 3 for them to match up
cbind(as.matrix(round(CI_mod,5)),as.matrix(round(CI_mod0[index_CI_mod,],5)))
```
We can observe that all the coefficients in the new model are significant!
Comparing confidence intervals for the naive model and the new model with Lasso-selected variables, we see that the confidence intervals are shifted more towards zero, and some of them has even become slightly smaller. For example, the confidence interval of `sysBP` went from `r CI_mod0[14,2]-CI_mod0[14,1]` to `r CI_mod[4,2]-CI_mod[4,1]` after the subset selection.

Similarly, we may fit a logistic model on the imputed data.
We include the variables (`r names_lasso_mice_70`) that was nonzero more than 70\% of the times in the bootstrap, and we state the coefficients of the new regression model and their confidence intervals.

```{r Inference for imputed data, echo = T}
# Add the response values that we took out for the mice data.

d.test.imp.mice["TenYearCHD"] <- y.test

# Fit the model.
mod <- glm(TenYearCHD ~ age + male + sysBP + cigsPerDay + glucose, data = d.test.imp.mice, family = binomial())

# Consider confidence interval and coefficients
CI_mice = confint(mod)
cbind(as.matrix(round(summary(mod)$coefficients,5)), as.matrix(round(CI_mice,5)))

```

Again, we can observe that all the coefficients in the new model are significant!

We take a brief look at the complete case model with Lasso-selected variables to the model on the imputed data with Lasso-selected variables.
The intervals are for the model on the imputed data (left) and the model on the complete data (right).

```{r comparison, echo = T}

cbind(as.matrix(round(CI_mice[c(1,2,3,4,6,5),],4)), rbind(as.matrix(round(CI_mod,4)),c("*","*")))

diff1 = CI_mod[3,2] - CI_mod[3,1]
diff2 = CI_mice[3,2] - CI_mice[3,1]
```
We can see, for example by considering `sysBP`, see that the width of the interval has gone further down by working on the imputed data, as the confidence interval width for `sysBP` was `r diff1` for the complete case model and `r diff2`. 
This may simply be because we use more data to fit the model, but it may also be a nonsensical question, as we are in essence fitting two different models. 
The inclusion of `cigsPerDay` in the model on imputed data is probably a key reason why we see such a difference. 
More or less, we obtain the same results, as it is the same covariates that come back time and time again.

# Discussion

When we compare the chosen coefficients from the complete case data compared to the imputed data, we see that they correspond. Thus, in a data-rich situation imputation may only introduce unnecessary variance, without having an immediate effect on the quality of the model or inference.

Another interesting thing we discovered, was that even though we obtain good results with the imputed data, the data quality seems to go down, if only barely. 
The sensitivity and specificity of the model went down slightly when using the imputed data, but as the Lasso selects the same variables, it does not matter a lot for our purposes.
The decrease in data quality might have been more visible if the percentage of imputed values were higher, or if we were in a data-poor situation.

What we can probably conclude, is that the variables `r names_lasso_cc_70` are the most significant, and that other important variables are `cigsPerDay` and `prevalentHyp`.



# References 