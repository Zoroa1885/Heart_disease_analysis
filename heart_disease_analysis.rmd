---
subtitle: "MA8701 Advanced Statistical Learning V2023"
title: "Compulsory exercise: Team Supergreat"
author: "Nora Aasen, Elias Angelsen, Jonas Nordstr?m"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: 
  html_document
  # pdf_document
---
  
```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE,tidy=TRUE,message=FALSE,warning=FALSE,strip.white=TRUE,prompt=FALSE,
                      cache=TRUE, size="scriptsize",
                      fig.width=4, fig.height=3,fig.align = "center")
```


# Setup

```{r Loading Packages,eval=TRUE,echo=FALSE}
# Load libraries
library(naniar)
library(mice)
library(glmnet)
library(dplyr)
library(caret)
library(ggcorrplot)
library(ggplot2)
```

Plan for the project: 
\begin{itemize}
\item Start by fitting lasso in bootstrap using complete data
\item Then singular imputation in training set and use same in test set
\item Fit a lasso within bootstrap
\item New method: still fitting lasso but this time do single imputation within the bootstrap
\item Compare the three different methods
\item Fit logistic regression and do inference under glm assumptions
\end{itemize}

## Exploratory Analysis
```{r Loading Data}
# Load and look at data
data <- read.csv("framingham.csv")
dim(data)
sum(data$TenYearCHD==1)
head(data)
str(data)

# Code education as a factor variable instead
data$education = factor(data$education, labels = c("none","hs","college","post-grad"))

# Look at the missing data
md.pattern(data)
gg_miss_var(data)

# Make a dataset containing only the complete cases
df_complete <- data[complete.cases(data), ]
dim(df_complete)
sum(df_complete$TenYearCHD==1)
str(df_complete)
# View(df_complete)
# Should we code the binary responses to be categorical instead of integers? 


# Split data into training and test
r = dim(df_complete)[1]
size = r-round(r/5)
spl = sample(1:r,size = size)
X = model.matrix(TenYearCHD ~  ., data = df_complete)[,-1]
x.train = X[spl,]
x.test = X[-spl,]
y.train = df_complete$TenYearCHD[spl]
y.test = df_complete$TenYearCHD[-spl]
```

```{r exp analysis}
# Make a correlation plot 
cor_mat = cor(df_complete)
ggcorrplot(cor_mat, hc.order = TRUE, type = "lower",
   outline.col = "white",
   colors = c("blue", "white", "red"))
```
## Missing Data





# Model

## Naive Approach
```{r Logistic Model}
# Fit a logistic model
mod0 <- glm(TenYearCHD ~ ., data = df_complete, family = binomial())

summary(mod0)
```


## Lasso
```{r Lasso}
# Må vi fjerne intercept før lasso? 

# Use cross-validation to find lambda
cv.out = cv.glmnet(x.train, y.train, family = "binomial", alpha = 1)
plot(cv.out$glmnet.fit, "lambda", label=FALSE)
plot(cv.out)
cv.out$lambda.min
cv.out$lambda.1se
lasso_mod = glmnet(x.train, y.train, family = "binomial", alpha = 1, lasso = cv.out$lambda.1se)

lasso_coef <- coef(lasso_mod,s=cv.out$lambda.1se)
lasso_coef[,1]
``` 




```{r Bootstrap Lasso}
B = 1000

B_coef = matrix(NA, nrow = B, ncol = 16)
for (i in 1:B){
  print(i)
  spl_b = sample(1:size, size = size, replace = TRUE)
  x = x.train[spl_b,]
  y = y.train[spl_b]
  cv.out = cv.glmnet(x, y, family = "binomial", alpha = 1)
  lasso_mod = glmnet(x.train, y.train, family = "binomial", alpha = 1, lasso = cv.out$lambda.1se)

  B_coef[i,] <- coef(lasso_mod,s=cv.out$lambda.1se)[,1]
}

colnames(B_coef) = names(coef(lasso_mod,s=cv.out$lambda.1se)[,1])
B_coef = ifelse(B_coef == 0,0,1)
apply(B_coef, 2, sum)

```

## Ridge 
```{r Ridge Model}
response <- "TenYearCHD"
features <- names(df_complete)[!names(df_complete) %in% c("TenYearCHD")]

df_complete["education"] <- factor(df_complete$education, labels = c("None", "HighSchool", "Collage", "Grad"))

X <- model.matrix(~.-1, df_complete[features])
y <- df_complete[response]

train_frac = 0.8
train.ind <- sample(c(TRUE, FALSE), nrow(df_complete),
                    replace=TRUE, prob=c(train_frac, 1-train_frac))

X_train <- X[train.ind,]
X_test <- X[!train.ind,]

y_train <- y[train.ind,]
y_test <- y[!train.ind,]

ridge_cv <- cv.glmnet(X_train, y_train, family = "binomial", alpha = 0)

plot(ridge_cv)
lambda_min = ridge_cv$lambda.min
lambda_1se = ridge_cv$lambda.1se
cat("Lambda min:", lambda_min, "\n")
cat("Lambda 1se:", lambda_1se)


print(ridge_cv)
coef(ridge_cv, s = "lambda.1se")
```
Make a fit with the best lambda and measuring error
```{r}
ridge_fit = glmnet(X_train, y_train, lambda = lambda_min, alpha = 0, family = "binomial")
ridge_pred = predict(ridge_fit, X_test, type = "class")

ridge_pred_class = factor(round(ridge_pred))
y_test <- factor(y_test)

conf_mat <- confusionMatrix(ridge_pred_class, y_test)
conf_mat

mse_ridge = mean((ridge_pred - y_test)^2)
mse_ridge
```

